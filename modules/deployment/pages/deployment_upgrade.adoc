[[_cha.deploy.upgrade]]
= Upgrading {productname}
:doctype: book
:sectnums:
:toc: left
:icons: font
:experimental:
:sourcedir: .
:imagesdir: ./images
= Upgrading SUSE CaaS Platform
:doctype: book
:sectnums:
:toc: left
:icons: font
:experimental:
:imagesdir: ./images


As {productname}
is constantly developed and improved, new versions get released.
You are strongly advised to upgrade to a supported release.
These upgrades may involve manual intervention. 

.Service Window Required
[IMPORTANT]
====
Upgrades may take take some time, during which services may be degraded in performance or completely unavailable.
Please make sure to plan a service window. 
====

[[_pro.deploy.upgrade.procedure]]
.Procedure: General Upgrade Procedure
. Upgrade the {admin_node} . 
. Manually perform additional upgrade steps of the {admin_node} . These steps are version-specific and described in the following chapters. 
. Upgrade the cluster nodes through {dashboard} . 


[[_sec.deploy.upgrade.caasp1]]
== Upgrading from {productname} 1


The following sections contain the necessary steps to upgrade from {productname}
1 to 2 or later. 

[[_sec.deploy.upgrade.caasp1.users]]
=== Migrating Users

{productname}
2 comes with Role-Based Access Control (RBAC), which stores user information in [productname]##OpenLDAP##
.
Therefore, after upgrading from {productname}
 1 to version 2 or higher, you have to migrate existing {dashboard}
 users to [productname]##OpenLDAP##
 users. 

For more information about RBAC and user management, refer to <<_auth>>.

[[_pro.deploy.upgrade.caasp1.users]]
.Procedure: Migrate users from version 1 to 2
. Connect to the {admin_node} using SSH. 
. Open a shell in the {dashboard} container: 
+

----
{prompt.root}``docker`` _exec -it_ $(``docker`` _ps_ |``grep`` _dashboard_ |``awk`` _'{print $1}')_ ``bash`` 
----
. Inside the container, execute the following command: 
+

----
{prompt.bash}``entrypoint.sh`` _bundle exec rake velum:migrate_users_ 
----
+
Once the command successfully finishes, existing user accounts will be available for logging into {dashboard}
again. 
. Type [command]``exit`` or press kbd:[+D] to exit the {dashboard} container. 


[[_sec.deploy.upgrade.caasp1.etcd]]
=== Upgrading etcd

{productname}
2 comes with {kube}
1.7, which uses `etcd` version 3 as default storage backend.
Therefore, after upgrading from {productname}
 1 to version 2 or higher, you have to orchestrate the migration between `etcd` 2 and 3. 

.Service Window Required
[IMPORTANT]
====
This migration can take a several minutes, during which [service]``etcd``
 and [service]``kube-api``
 services are unavailable.
Please make sure to plan a service window. 
====

[[_pro.deploy.upgrade.caasp1.etcd]]
.Procedure: Migrate from etcd version 2 to 3
. Connect to the {admin_node} using SSH. 
. Open a shell in the {smaster} container: 
+

----
{prompt.root}``docker`` _exec -it_ $(``docker`` _ps_ |``grep`` _salt-master_ |``awk`` _'{print $1}')_ ``bash`` 
----
. Inside the container, execute the following command: 
+

----
{prompt.bash}``salt-run`` _state.orchestrate orch.etcd-migrate_ 
----
+
The orchestration will shutdown all [service]``etcd``
and [service]``kube-apiserver``
services, perform the [service]``etcd``
migration steps, set the "`etcd_version = etcd3`"
pillar value, and restart [service]``etcd``
and [service]``kube-api``
services. 
+ 
Once the command successfully finishes, all services will be available again. 
. Type [command]``exit`` or press kbd:[+D] to exit the {smaster} container. 


[[_sec.deploy.upgrade.caasp1.new_settings]]
=== Adding new settings


Run the following commands to ensure that default values are set correctly for some new options introduced in {productname}
2 which were not present in version 1. 

[[_pro.deploy.upgrade.caasp1.new_settings]]
.Procedure: Add new settings introduced in {productname}2
. Connect to the {admin_node} using SSH. 
. Open a shell in the {dashboard} container: 
+

----
{prompt.root}``docker`` _exec -it_ $(``docker`` _ps_ |``grep`` _dashboard_ |``awk`` _'{print $1}')_ ``bash`` 
----
. Set `dashboard_external_fqdn` to the Fully Qualified Domain Name (FQDN) of the {admin_node} : 
+

----
{prompt.bash}``entrypoint.sh`` _bundle exec rails runner \
    'Pillar.create(pillar: "dashboard_external_fqdn", value: "FQDN")'_ 
----
+
Replace `FQDN` with the Fully Qualified Domain Name of your {admin_node}
. 
. Create the LDAP related pillars: 
+

----
{prompt.bash}``entrypoint.sh`` _bundle exec rails runner \
    'Velum::LDAP.ldap_pillar_settings!({}).each \
    {|key, value| Pillar.create(pillar: Pillar.all_pillars[key.to_sym], \
    value: value)}'_ 
----
. If you intend to use Helm on your CaaSP Cluster, you also need to enable Tiller (Helm's server component). Execute the following command in the open shell: 
+

----
{prompt.bash}``entrypoint.sh`` _bundle exec rails runner \
    'Pillar.create(pillar: "addons:tiller", value: "true")'_ 
----
. Type [command]``exit`` or press kbd:[+D] to exit the {dashboard} container. 


[[_sec.deploy.upgrade.caasp1.service_account]]
=== Generating the Service Account Key File on the CA

{kube}
distinguishes between user and service accounts.
While user accounts  are for humans, service accounts are for processes, which run in pods. 

In order to use service acconunts, you have to generate the service account key file [path]``sa.key``
 on the Certificate Authority (CA). 

[[_pro.deploy.upgrade.caasp1.service_account]]
.Procedure: Generate the Service Account Key File [path]``sa.key``on the CA.
. Connect to the {admin_node} using SSH. 
. Open a shell in the {smaster} container with: 
+

----
{prompt.root}``docker`` _exec -it_ $(``docker`` _ps_ |``grep`` _salt-master_ |``awk`` _'{print $1}')_ ``bash`` 
----
. Inside the container, execute the following command: 
+

----
{prompt.bash}``salt`` _"ca" state.apply kubernetes-common.generate-serviceaccount-key_ 
----
. Type [command]``exit`` or press kbd:[+D] to exit the {smaster} container. 
