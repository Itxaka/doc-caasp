[[_integration]]
= {productname} Integration with SES
:doctype: book
:sectnums:
:toc: left
:icons: font
:experimental:
:sourcedir: .
:imagesdir: ./images
= SUSE CaaS Platform Integration with SES
:doctype: book
:sectnums:
:toc: left
:icons: font
:experimental:
:imagesdir: ./images

{productname}
can use {suse}
's another product as storage for containers{mdash}{suse}
Enterprise Storage (further called SES). This chapter gives details how to integrate these two products in several ways. 

[[_integration.prerequisites]]
== Prerequisites


Before you start the integration process, you need to ensure the following: 

* {productname} can access the SES cluster. 
* {productname} can communicate with the SES nodes{mdash} master, monitoring nodes, OSD nodes and the meta data server in case you need a shared file system. For more details regarding SES refer to https://www.suse.com/documentation/ses-4/book_storage_admin/data/book_storage_admin.html[SES documentation]. 


[[_integration.mounting.fixed.object]]
== Mounting a Named Object Storage to a Container


The procedure below describes steps to take when you need to mount a fixed name storage volume to a particular container. 

.Procedure: Mounting Storage to a Container
. On the {master_node} apply the configuration that includes Ceph secret by using the [command]``kubectl apply``
+

----
``kubectl apply -f - << *EOF*`` apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
type: "kubernetes.io/rbd" 
data:
  key: "`the Ceph secret key`"
*EOF*
----
+
The Ceph secret key is stored on the SES master node in the file [path]``/etc/ceph/ceph.client.admin.keyring``{ndash}
use the `key` value. 
. Create an image in the SES cluster. On the master node, run the following command: 
+

----
rbd create -s`1G``yourvolume`
----
+
Where [replaceable]``1G`` is the size of the image and [replaceable]``yourvolume`` is the name of the image. 
. Create a pod that uses the image. On the {master_node} run the following: 
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Pod
metadata:
  name: busybox-rbd
  labels:
    app: busybox-rbd
spec:
  containers:
  - name: busybox-rbd-container
    image: busybox
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
    volumeMounts:
    - mountPath: /mnt/rbdvol
      name: rbdvol
  volumes:
  - name: rbdvol
    rbd:
      monitors:
      - [monitor1 ip:port]
      - [monitor2 ip:port]
      - [monitor3 ip:port]
      - [monitor4 ip:port]
     pool: rbd
     image: yourvolume
     user: admin
     secretRef:
       name: ceph-secret
     fsType: ext4
     readOnly: false
*EOF*
----
. Verify that the pod exists and its status: 
+

----
kubectl get po
----
. Once the pod is running, check the mounted volume: 
+

----
``kubectl exec -it busybox-rbd -- df -k`` ...
/dev/rbd1               999320      1284    929224   0% /mnt/rbdvol
...
----


In case you need to delete the pod, run the following command on {master_node}
: 

----
kubectl delete pod`busybox-rbd`
----

[[_integration.pods.persistent.volumes]]
== Creating Pods with Persistent Volumes


The following procedure describes how to attach a pod to a persistent SES volume. 

.Procedure: Creating a Pod with Persistent Volume and Persistent Volume Claims
. Create a volume on the SES cluster: 
+

----
rbd create -s`1G``yourvolume`
----
+
Where [replaceable]``1G`` is the size of the image and [replaceable]``yourvolume`` is the name of the image. 
. Create the persistent volume on the {master_node} : 
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: PersistentVolume
metadata:
  name: yourpv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  rbd:
    monitors:
    - [monitor1 ip:port]
    - [monitor2 ip:port]
    - [monitor3 ip:port]
    - [monitor4 ip:port]
    pool: rbd
    image:`yourvolume`user: admin
    secretRef:
      name: ceph-secret
    fsType: ext4
    readOnly: false
*EOF*
----
. Create a persistent volume claim on the {master_node} : 
+

----
kubectl apply -f - << *EOF*
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: yourpvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
*EOF*
----
+
.Listing Volumes
NOTE: This persistent volume claim does not explicitly list the volume.
Persistent volume claims work by picking any volume that meets the criteria from a pool.
In this case we specified any volume with a size of 1G or larger.
When the claim is removed the recycling policy will be followed. 
+

. Create a pod that uses the persistent volume claim. On the {master_node} run the following: 
+

----
kubectl apply -f - <<*EOF*
apiVersion: v1
kind: Pod
metadata:
  name: busybox-rbd
  labels:
    app: busybox-rbd
spec:
  containers:
  - name: busybox-rbd-container
    image: busybox
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
    volumeMounts:
    - mountPath: /mnt/rbdvol
      name: rbdvol
  volumes:
  - name: rbdvol
    persistentVolumeClaim:
      claimName: yourpvc
*EOF*
----
. Verify that the pod exists and its status. On the {master_node} run: 
+

----
kubectl get po
----
. Once pod is running, check the volume by running on the {master_node} : 
+

----
``kubectl exec -it busybox-rbd -- df -k`` ...
/dev/rbd3               999320      1284    929224   0% /mnt/rbdvol
...
----


In case you need to delete the pod, run the following command on the {master_node}
: 

----
kubectl delete pod busybox-rbd
----


And when the command finishes, run 

----
kubectl delete persistentvolume yourpv
----

.Deleting a Pod
[NOTE]
====
When you delete the pod, the persistent volume claim is deleted as well. 
====