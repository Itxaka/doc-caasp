CaaSP is sourced from three types of repositories:

. RPM packages - MicroOS
.. Software updates on the operating system
.. Provided by SUSE package repositories
. Container images - Docker, CRI-O
.. Software updates for the deployments and the CaaSP infrastructure layer
.. Provided by Docker on dockerhub
.. Provided by Google on k8s.io
.. Provided by SUSE in default installation image
. Helm Charts - Kubernetes
.. Deployment information for applications and services
.. Provided by helm on GitHub
.. Provided by SUSE helm chart repository (does not exist yet)

In an airgapped environment we define three "layers" of these repositories:

. upstream, located on the internet and updated and maintained by SUSE or a software distributor
. external, located on the customer network but outside of the airgapped environment, updated and maintained by the customer
. internal, local in the air gapped network, updated and maintained by the customer

== RPM Packages Mirror

Customer has to configure a RPM repository mirror. This is needed to install
machines and deliver updates to the cluster.

This is a common problem (aka not specific to CaaSP/CAP) for our customers. The
SUSE solution to this problem was (with SLE <= 12) to deploy a
"Subscription Management Tool (SMT) server.

Starting with SLE15 the SMT product has been replaced by
"Repository Management Tool" (RMT).

Customer should use the existing documentation of SMT/RMT to deploy a dedicated
instance and mirror all the channels that are related with SUSE CaaSP.

=== Updating

RMT

RMT takes care of that automatically. RMT questions can be asked to the SUSE Customer Center team.

You can find them on irc.suse.de inside of the #happy-customer channel.

=== issues

PROBLEM: This is not possible in the current requirement. No internet/proxy at all.

== Container Images Mirror

Right now we don't have a proper solution for that. I've been working on that during latest hackweek. The reading might be interesting because it sheds some light on the most common issues.

https://trello.com/c/j5YbLk4y/118-describe-remote-registries-and-mirrors-feature

=== Air gapped registry - prerequisites

We are already supposed to have some documentation about that. I cannot find the trello card about that. The bug tracking it can be found here. The bugzilla entry has a drafted document attached to it.

As you can see the whole solution depends on having a proxy registry running somewhere outside of the air gapped network. The setup of a proxy registry should already be covered by our docs. Again this bug has the drafted doc attached to it.

=== The setup

We should care only to provide instructions about how to mirror contents from the SUSE registry. The same instructions can be used to mirror also other registries, like the Docker Hub, quay.io, ...

The customer has to operate two docker registries:

    pull-through registry: this is connected to the internet, hence it's outside of the air-gapped network. This is used to cache the SUSE images.
    read-only registry: this is running inside of the air-gapped network. The images of the pull-through registry are copied to a USB drive that is then attached to this instance.

=== Warming the cache

We don't have anything about how to do that in a disconnected environment. I have some ideas about that, I'll reach out to an engineer asking to put some information together.

I guess we can focus only on ensuring all the CAP pieces are mirrored. We don't actually want to mirror all the images on the SUSE registry yet.



=== From Bugzilla

----
This guide explains how to configure an air-gapped on-premise mirror of the SUSE container registry.

The final architecture of the deployment will look like the following figure.


The deployment consists of the following entities:
    • registry.suse.com: this is the official container registry of SUSE.
    • mirror.local.lan: this is a regular on-premise registry of the SUSE registry. The node running this service must be able to reach the official SUSE registry.
    • Air-gapped network: nodes inside of this network are completely isolated from the outside. They cannot reach neither the official SUSE registry nor the mirror.local.lan host.

The SUSE Container as a Service Platform cluster is deployed inside the air-gapped network. The same applies to the air-gapped mirror named “mirror.secure.lan”.
Prerequisites
Before proceeding with the next steps the user must deploy a traditional on-premise mirror. Please consult the “how to setup an on-premise mirror of SUSE registry” documentation.

The rest of the document will assume the traditional on-premise registry is running on a host named “mirror.local.lan”.
Air-gapped registry installation
On a regular SUSE Linux Enterprise Server machine install the “docker-distribution-registry” package. The package is available inside of the containers module:

$ sudo zypper in docker-distribution-registry
Configuration
The registry package provides a default configuration file: /etc/registry/config.yml

Identify the “storage” section and add the following lines:
maintenance:
    readonly:
      enabled: true

These lines will make the registry read-only, preventing its images from being overwritten by the clients of the air-gapped network.

The complete file will look like:

version: 0.1
log:
  level: info
storage:
  filesystem:
    rootdirectory: /var/lib/docker-registry
  maintenance:
    readonly:
      enabled: true
http:
  addr: 0.0.0.0:5000

Now start the registry service and enable it at boot time:
sudo systemctl enable --now registry.service

Note well: the default configuration is not using certificates to secure the communication between this registry and all the nodes communicating with it. Also there is no authentication and authorization in place.
For production usage it’s recommended to deploy the registry using tls certificates and implement authentication and authorization by deploying a Portus instance.

Note well: do not add the “proxy” section like in a traditional on-premise mirror. This is not needed and would cause the registry service to refuse to start. There would be an error at start time caused by the node not being able to connect to the remote registry.
Synchronizing contents
This procedure illustrates how to synchronize the air-gapped registry with the traditional on-premise one.

    1. Attach an external hard drive to “mirror.local.lan” and copy all the contents of “/var/lib/docker-registry” to it:

sudo rsync -aP --delete /var/lib/docker-registry /mnt/usbdisk

    2. Attach the external hard drive to “mirror.secure.lan” and import the contents into the registry:

sudo rsync -aP --delete /mnt/usbdisk/docker-registry/ /var/lib/docker-registry/

Additional security steps can be introduced between step #1 and #2; for example scanning the exported data using corporate tools before introducing them inside of the air-gapped network.

Note well: it’s not needed to stop the registry services while doing the backup and restore procedures.

Using the mirror
Using the air-gapped mirror works exactly like using a traditional on-premise mirror. Please refer to SUSE Container as Service Platform documentation to configure your cluster to take advantage of this air-gapped mirror.

Nothing has to be changed inside of Dockerfile(s), Kubernetes manifest files, Helm charts, custom scripts, etc. All the images with a prefix “registry.suse.com/” will be automatically pulled from this air-gapped mirror.

The mirror is read-only and can serve only the images that were previously cached by the regular on-premise mirror.
----

== Helm Charts Mirror

The customer has to run a simple web server (nginx, apache, whatever can serve static files) that serves a copy SUSE's helm charts.

All our helm charts can be downloaded in this way:

----
mkdir suse-charts && cd suse-charts
curl -O https://kubernetes-charts.suse.com/index.yaml
grep https index.yaml | awk {'print $2'} | xargs -d "\n" -i curl -O {}
----

Then add the packages to the webserver.
Add the webserver as a repo to helm on the admin node.

https://docs.helm.sh/helm/#helm-repo-add


== Deployment of CaaSP in an airgapped environment

=== Using the ISO

From YaST register the node against the RMT server. This will ensure the final node zypper repositories are pointed against RMT, moreover all the available updates are going to be installed (-> no need to run a transactional-update right after the installation -> way faster).

=== Using AutoYast

Ensure the admin node is registered against RMT, that will ensure the nodes are provisioned by AutoYaST are:

    Registered against RMT too
    Have all the updates applied

=== Using a prebuilt image (eg: KVM, Xen)

The node has to be registered against RMT. This should be done in the same way
as a regular SLE machine: via SUSEConnect.

=== Existing unregistered running node

Again, they should be using SUSEConnect.

.manual
----
clientSetup4SMT.sh --host <smt-server> --fingerprint "<fingerprint of the certificate>" --yes --regcert http://<smt-server>/smt.crt

The tool can be found at /srv/www/htdocs/repo/tools/clientSetup4SMT.sh on the SMT server.
The fingerprint can be found with "openssl x509 -noout -in /srv/www/htdocs/smt.crt -fingerprint -sha1" on the SMT server.
----

.cloud-init
----
runcmd:
    - /usr/bin/SUSEConnect -r "product_key"
----
