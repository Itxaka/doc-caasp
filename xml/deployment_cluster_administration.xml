<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="administration"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
  <info>
  <title>Cluster Administration</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 
 <sect1 xml:id="transactional.updates">
  <title>Handling Transactional Updates</title>
  <para>
   For security and stability reasons, the operating system and application should always be up-to-date. While with a single machine you can keep the system up-to-date quite easily by running several commands, in large-scaled cluster the update process become a real burden. Thus transactional automatic updates have been introduced. Transactional updates can be characterized as follows:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     they are atomic.
    </para>
   </listitem>
   <listitem>
    <para>
    They do not influence the running system.
    </para>
   </listitem>
   <listitem>
    <para>
     They can be rolled back.
    </para>
   </listitem>
   <listitem>
    <para>
     The system needs to be rebooted to activate the changes.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Transactional updates are managed by the <command>transactional-update</command> script, which is called once a day. The script checks if any update is available. If there is an update to be applied, a new snapshot of the root file system is created and the system is updated by using <command>zypper up</command>. All RPMS released to this point are applied. The snapshot is then marked as active and will be used after the next reboot of the system. The script can reboot the cluster itself or it can instruct the <literal>rebootmgr</literal> to schedule the reboot according to the configured policies (for details refer to <xref linkend="reboot.managment"/>).
  </para>
  
  <sect2 xml:id="transactional.updates.disabling">
   <title>Disabling Transactional Updates</title>
   <para>
    Even though we do not recommend that, you can disable transactional updates by issuing the command:    
   </para>
   <screen>systemctl --now disable transactional-update.timer</screen>
   <para>
    And also the automatic reboot can be disabled, just use any of the commands:
   </para>
   <screen>systemctl --now disable rebootmgr</screen>
   <para>
    or
   </para>
   <screen>rebootmgrctl off</screen>
  </sect2>
 </sect1>
 
 <sect1 xml:id="ptf.handling">
  <title>Handling Program Temporary Fixes</title>
  <para>
   Program temporary fixes (PTFs) are available in the &productname; environment. You install them by using the <command>transactional-update</command> script. Typically you invoke the installation of PTFs by running:
  </para>
  <screen>transactional_update reboot ptf install</screen>
  <para>
   The command installs PTF RPMs. The <literal>reboor<literal> option then schedules a reboot after the installation. PTFs are activate only after rebooting of your system.
  </para>
  <para>
   In case you need to remove the installed PTFs, use the following command:
  </para>
  <screen>transactional_update reboot ptf remove</screen>
 </sect1>
 
 <sect1 xml:id="reboot.managment">
   <title>Rebooting the Cluster</title>
   <para>
    In production you typically need to carefully shedule rebooting of your cluster as you may need to reboot during a specific time period and usually you do not want to reboot all nodes at the same time. &productname; involves a reboot manager that according to a passed configuration schedules rebooting of your cluster. The reboot manager is a daemon that is controlled by using the <command>rebootmgrctl</command> and configuration is stored in the <filename>/etc/rebootmgr.conf</filename>. The file is not overwritten after reboot. An example configuration follows:
   </para>
   <screen>[rebootmgr]
window-start=03:30
window-duration=1h20m
strategy=best-effort
lock-group=default</screen>

<para>
 The meaning of the options is the following:
</para>
<variablelist xml:id="reboot.managment.strategies">
 <varlistentry>
  <term><literal>window-start</literal></term>
  <listitem>
   <para>
   configures the beginning of the reboot window in the format: <emphasis>HH:MM</emphasis>
   </para>
  </listitem>
 </varlistentry>
 <varlistentry>
  <entry><literal>window-duration</literal></entry>
  <listitem>
   <para>
    configures duration of the maintenance window when the system can be rebooted. Use the format <emphasis>XXhYYm</emphasis>
   </para>
  </listitem>
 </varlistentry>
 <varlistentry>
  <term><literal>strategy</literal></term>
  <listitem>
   <para>
    defines a strategy of rebooting. You can use any of the following values:
    <itemizedlist>
     <listitem>
      <para>
       <emphasis role="bold"><literal>instantly</literal></emphasis> - when a signal is received, services will be rebooted without any lock or without waiting for a maintenance window.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold"><literal>maint-window</literal></emphasis> - the cluster reboots only within the set maintenance window. If you do not configure the maintenance window, the cluster reboots immediately.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold"><literal>etcd-lock</literal></emphasis> - before rebooting, machines specified in the <literal>lock-group</literal> are locked. If you configured the maintenance window, the lock is created within the configured window. For more details about lock group, refer to <xref linkend="reboot.managment.locks"/>.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold"><literal>best-effort</literal></emphasis> - the default value of the reboot strategy. If etcd is running, the <literal>etcd-lock</literal> reboot strategy is used. If etcd is not running, but you configured the maintenance window, the <literal>maint-window</literal> reboot strategy will be used. If there is no etcd and no maintenance window configured, the <literal>instantly</literal> reboot strategy will be used.
      </para>      
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold"><literal>off</literal></emphasis> - the <literal>rebootmg</literal> still runs, but all signals to reboot are ignored.
      </para>
     </listitem>
    </imtezedlist>
   </para>
  </listitem>
 </varlistentry>
</variablelist>

 <sect2 xml:id="reboot.managment.locks">
  <title>Locking by Using etcd</title>
  <para>
   Typically you need to avoid rebooting of all cluster nodes at the same. Thus you can use groups of nodes and configure the behaviour if a reboot signal arrives. For example, you can define two groups of nodes&mdash;one that groups nodes running etcd instances called <emphasis>etcd</emphasis> and one for other nodes called <emphasis>workers</emphasis>. In the <emphasis>etcd</emphasis> group you configure that only one node in this group can be rebooted at the same time. In the <emphasis>workers</emphasis> group you configure that a higher number of nodes can be rebooted at the same time.  
  </para>
  <para>
   In the directory <filename>rebootmg/locks/&lt;group name&gt;</filename> there are two variables:
  </para>
  <variablelist>
   <varlistentry>
    <term><literal>mutex</literal></term>
    <listitem>
     <para>
      defaults to 0.  You can set the value to 1 by using <literal>atomic_compare_and_swap</literal> to ensure that only one machine has write access at one time.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>data</literal></term>
    <listitem>
     <para>
      contains a JSON structure with the following data:
     </para>
     <screen>{
“max”:3,
“holders”:[
“3cb8c701b4d3474d99a7e88b31dd3439”,
“71c8efe539b280af2fe09b3b5771345e”
]
}</screen>
 <para>
  <literal>max</literal> defines the count of nodes within the group that can be rebooted at the same time.
 </para>
 <para>
  <literal>holders</literal> is a comma separated list of unique ID of machines. Values are obtained from the <filename>/etc/machine-id</filename> file.
 </para>
    </listitem>
   </varlistentry>
  </variablelist>
  
  <para>
   A typical workflow of a client that tries to reboot is the following:
  </para>
  <orderedlist>
   <listitem>
    <para>
     A check is performed if there are free locks. If not, the client checks the <literal>data</literal> variable until it changes.
    </para>
   </listitem>
   <listitem>
    <para>
     The client gets the <literal>mutex</literal>
    </para>
   </listitem>
   <listitem>
    <para>
     ID of the client is added to the <literal>holders</literal> list.
    </para>
   </listitem>
   <listitem>
    <para>
    The <literal>mutex</literal> is released. 
    </para>
   </listitem>
   <listitem>
    <para>
     The node reboots.
    </para>
   </listitem>
   <listitem>
    <para>
     On boot, the client checks if it holds the <literal>mutex</literal>. If yes, the client ID is removed from the <literal>holders</literal> list and the <literal>mutex</literal> is released. 
    </para>
   </listitem>
  </orderedlist>
 </sect2>
 </sect1>
    
  <sect1 xml:id="commands.cluster.managment">
  <title>Commands for Cluster Management</title>
  <para>
   TBD
  </para>  
  <sect2 xml:id="commands.cluster.managment.rebootmgctl">
   <title>The <command>rebootmgrctl</command> Command</title>
   <para>
    The <command>rebootmgrctl</command> is a tool that enables you to control and configure the behaviour of the reboot manager daemon. To schedule a reboot of your cluster, run the following command:
   </para>
   <screen>rebootmgrctl reboot</screen>
   <para>
    You can specify two options: <replaceable>fast</replaceable> and <replaceable>now</replaceable>. The <replaceable>fast</replaceable> option requests a lock from <literal>etcd</literal> but the configured maintenance window will be ignored. The <replaceable>now</replaceable> option forces immediate reboot without any lock from <literal>etcd</literal>.
   </para>
   <para>
    To view a status of the reboot manager daemon, run the command:
   </para>
   <screen>rebootmgrctl status</screen>
   <para>
   The command outputs the current status and current data structure stored in <literal>etcd</literal>. The status is a number from 0 to 3, where each has the following meaning:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis role="bold">0</emphasis> - no reboot has been requested.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">1</emphasis> - a reboot has been requested.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">2</emphasis> - a reboot has been requested, the manager is waiting for the maintenance window.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">3</emphasis> - a reboot has been requested, the manager is waiting for the <literal>etcd</literal> lock.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    To set or change particular reboot strategy, run the following:
   </para>
   <screen>rebootmgrctl set-strategy <replaceable></replaceable></screen>
   <para>
   For possible strategies refer to <xref linkend="reboot.managment.strategies"/>.
   </para>
   <para>
   If you need to view the currently used reboot strategy, run the command:
   </para>
   <screen>rebootmgrctl get-strategy</screen>
   <para>
   The <command>rebootmgrctl</command> enables you to set a group of machines that belongs to one group and are allowed to reboot at the same time. If you need your machine to a particular <literal>etcd</literal> lock group, use the following command:
   </para>
   <screen>rebootmgrctl set-group <replaceable>etcd-lock-group</replaceable></screen>
   <para>
    After you define a group, you can set the maximum count of machines in that group that are allowed to reboot at the same time. Use the command:
   </para>
   <screen>rebootmgrctl set-max <replaceable>-group group_name</replaceable> count</screen>
   <para>
    If you do not use the <literal>group</literal> option, the local default group will be used. 
   </para>
  </sect2>
 </sect1> 
 
 
 
 <sect1 xml:id="node.managment">
  <title>Node Managment</title>
  <para>
   TBD
  </para>
  
  <sect2 xml:id="node.managment.removing">
   <title>Removing Nodes</title>
   <para>
    As each node in the cluster runs also an instance of <literal>etcd</literal>, &productname; has to ensure that removing of several nodes does not break the <literal>etcd</literal> cluster. In case you you have for example three nodes in the <literal>etcd</literal> and you delete two of them, &productname; deletes one node, recovers the cluster and only if the recovery is successful, the next node can be removed. If a node runs just an <literal>etcd-proxy</literal>, there is nothing special that has to be done, as deleting any amount of <literal>etcd-proxy</literal> can not break the <literal>etcd</literal> cluster. 
   </para>
  </sect2>
 </sect1>
  
</chapter>
