<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.deployment.preparation"
 xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Preparing the Installation</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  This chapter prepares the installation of &productname; on physical
  machines, manually configured virtual machines or in private cloud
  environments.
 </para>

 <para>
  There are several ways to install &productname;:
 </para>
 <variablelist>
  <varlistentry>
   <term><xref linkend="sec.deploy.preparation.dvd" /></term>
   <listitem>
    <para>
     This method requires lots of manual interaction with the
     physical or virtual machines. This method is suitable for small to
     medium cluster sizes.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term><xref linkend="sec.deploy.preparation.pxe" /></term>
   <listitem>
    <para>
     With this method the boot into the installation is done with a
     network installation server. This reduces the need to interact
     with every single node. This method is suitable for medium cluster
     sizes.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term><xref linkend="sec.deploy.preparation.openstack" /></term>
   <listitem>
    <para>
     You can deploy &productname; on &soc;. This method is suitable
     for large cluster sizes.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term><xref linkend="sec.deploy.preparation.disk_images" /></term>
   <listitem>
    <para>
     You can use prepared disk images ready to deploy on virtual
     machines. This increases the deployment speed.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term><xref linkend="sec.deploy.preparation.public_cloud" /></term>
   <listitem>
    <para>
     This section is about installing &productname; in a public cloud,
     for example <emphasis>Microsoft Azure</emphasis>*,
     <emphasis>Amazon AWS</emphasis>* and <emphasis>Google Compute
     Engine</emphasis>*.
    </para>
   </listitem>
  </varlistentry>
 </variablelist>
 <para>
  To customize the setup of the nodes, use <command>cloud-init</command>.
  For details, see <xref linkend="sec.deploy.cloud-init" />.
 </para>

 <sect1 xml:id="sec.deploy.preparation.dvd">
  <title>Installing from USB, DVD or ISO Images</title>
  <para>
   This procedure provides an overview of the steps for the
   cluster deployment with classical boot devices like DVD drives. This
   method is suitable for small to medium cluster sizes.
  </para>
  <procedure>
   <step>
    <para>
     Choose an installation medium. You can install from DVD or USB
     sticks. On virtual machines, you can install from ISO images.
    </para>
   </step>
   <step>
    <para>
     Boot the machine designated for the &admin_node; from your
     selected medium. Then follow the installation and configuration
     instructions detailed in <xref
     linkend="sec.deploy.nodes.admin_install" /> and <xref
     linkend="sec.deploy.nodes.admin_configuration" />.
    </para>
   </step>
   <step>
    <para>
     Boot the machines designated to be the Master and Worker Nodes
     from your selected medium. Then follow the installation and
     configuration instructions detailed in <xref
     linkend="sec.deploy.nodes.worker_install" />. To automate the
     installation, you can use an &ay; file provided by the
     &admin_node;. We recommend to use &ay; to speed up the node
     deployment. For details, see <xref
     linkend="sec.deploy.nodes.worker_install.manual.autoyast" />.
    </para>
   </step>
   <step>
    <para>
     Finish the installation by bootstrapping the cluster. For details,
     see <xref linkend="sec.deploy.install.bootstrap" />.
    </para>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec.deploy.preparation.pxe">
  <title>Installing from Network Source</title>
  <para>
   This procedure provides an overview of the steps for the
   cluster deployment from an network installation server. A PXE
   environment sis used to provide the nodes with the data required for
   installation.
  </para>
  <procedure>
   <step>
    <para>
     Install an installation server that provides a DHCP, PXE and TFTP
     service. Additionally, you can provide the installation data on
     an HTTP or FTP server. For details, refer to the &sle; 12 Deployment Guide:
     <link xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_sle_deployment/book_sle_deployment.html#cha.deployment.prep_boot"/>.
    </para>
    <para>
     You can directly use the <filename>initrd</filename> and
     <filename>linux</filename> files from your installation media, or
     install the package
     <package>tftpboot-installation-CAASP-3.0</package> onto your TFTP
     server. The package provides the required
     <filename>initrd</filename> and <filename>linux</filename> files
     in the <filename>/srv/tftpboot/</filename> directory. You will
     need to modify the paths used in the &sle; 12 Deployment Guide to
     correctly point to the files provided by the package.
    </para>
   </step>
   <step>
    <para>
     PXE boot the machine designated for the &admin_node;. Then follow
     the installation and configuration instructions detailed in <xref
     linkend="sec.deploy.nodes.admin_install" /> and <xref
     linkend="sec.deploy.nodes.admin_configuration" />.
    </para>
   </step>
   <step>
    <para>
     Pxe boot the machines designated to be the Master and Worker
     Nodes. Then follow the installation and configuration instructions
     detailed in <xref linkend="sec.deploy.nodes.worker_install" />. To
     automate the installation, you can use an &ay; file provided by
     the &admin_node;. For details, see <xref
     linkend="sec.deploy.nodes.worker_install.manual.autoyast" />.
    </para>
   </step>
   <step>
    <para>
     Finish the installation by bootstrapping the cluster. For details,
     see <xref linkend="sec.deploy.install.bootstrap" />.
    </para>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec.deploy.preparation.openstack">
  <title>Installation in &soc;</title>
  <para>
   You can deploy a &productname; on &soc; using &ostack;.
   You will need a &productname; machine image and &ostack; Heat templates.
   Once you have created a stack, you will continue with the &productname; setup.
  </para>
  <note>
   <title>&productname; machine image for &soc;</title>
   <para>
    Download the latest &productname; for &ostack; image from
    <link xlink:href="https://download.suse.com">https://download.suse.com</link>
    (for example,
    <filename>SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2</filename>).
   </para>
  </note>
   <note>
    <title>
     &ostack; Heat Templates Repository
    </title>
    <para>
     &productname; Heat templates are available from <link xlink:href="https://github.com/SUSE/caasp-openstack-heat-templates">GitHub</link>.
    </para>
  </note>

  <sect2 xml:id="sec.deploy.preparation.openstack.horizon">
   <title>Using the Horizon Dashboard</title>
   <procedure>
   <step>
    <para>
     Go to <guimenu>Project &rarr; Compute &rarr; Images</guimenu> and click on
     <guimenu>Create Image</guimenu>.
    </para>
    <para>
     Give your image a name (for example: <literal>CaaSP-3</literal>); you will
     need to use this later to find the image.
    </para>
   </step>
    <step>
     <para>
      Go to <guimenu>Project &rarr; Orchestration</guimenu> and click on
      <guimenu>Stacks</guimenu>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="horizon_stacks.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="horizon_stacks.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Click on <guimenu>Launch Stack</guimenu> and provide the stack
      templates. Either upload the files, provide the URL to the raw files
      directly (only applies to stack template), or copy and paste the
      contents into the <guimenu>Direct Input</guimenu> fields.
     </para>
     <warning>
      <title>Replace the default <literal>root_password</literal></title>
       <para>
        Do not use the <filename>caasp-environment.yaml</filename> directly
        from the GitHub repository.
       </para>
        <para>
         You must make sure to replace the value for
         <literal>root_password</literal> with a secure password.
         This will become the password for the &rootuser; account on all
         nodes in the stack.
        </para>
    </warning>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="horizon_launch_stack.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="horizon_launch_stack.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
   <step>
    <para>
     Click <guimenu>Next</guimenu>.
    </para>
   </step>
    <step>
     <para>
      Now you need to define more information about the stack.
     </para>
     <variablelist>
      <varlistentry>
       <term><literal>Stack Name</literal></term>
       <listitem>
        <para>
         Give your stack a name
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>Password</literal></term>
       <listitem>
        <para>
         Your &soc; password
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>Image</literal></term>
       <listitem>
        <para>
         Select the image your machines will be created from
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>root_password</literal></term>
       <listitem>
        <para>
         Set the root password for your cluster machines
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>admin</literal>/master/worker_flavor</term>
       <listitem>
        <para>
         Select the machine flavor for your nodes
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>worker_count</literal></term>
       <listitem>
        <para>
         Number of worker nodes to be launched
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>external_net</literal></term>
       <listitem>
        <para>
         Select an external network that your cluster will be reachable from
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>internal_net_cidr</literal></term>
       <listitem>
        <para>
         The internal network range to be used inside the cluster
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>dns_nameserver</literal></term>
       <listitem>
        <para>
         Internal name server for the cluster
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="horizon_stack_options.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="horizon_stack_options.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
     Click <guimenu>Launch</guimenu>.
     </para>
    </step>
    <step>
     <para>
      After the cluster has been started and the cluster overview shows
      <guimenu>Create Complete</guimenu>, you need to find the external IP
      address for the admin node of your cluster (here:
      <literal>10.86.1.72</literal>). Now visit that IP address in your browser.
      You should see the &dashboard; login page and can continue with
      <xref linkend="sec.deploy.nodes.admin_configuration"/>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="horizon_stack_resources.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="horizon_stack_resources.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.deploy.preparation.openstack.cli">
   <title>Using the &ostack; CLI</title>
   <note>
    <para>
     You need to have access to the &ostack; command-line tools. You can
     either access those via <literal>ssh</literal> on your &soc; admin
     server or
     <link xlink:href="https://docs.openstack.org/newton/user-guide/common/cli-install-openstack-command-line-clients.html">
     install a local <command>openstack</command> client</link>.
    </para>
    <para>
     To use the local client, you need to access <guimenu>Project &rarr; Compute &rarr;
     Access &amp; Security</guimenu> in the Horizon Dashboard and click on the
     <guimenu>Download &ostack; RC File v3</guimenu>.
    </para>
    <para>
     The downloaded file is a script that you then need to load using the
     <command>source</command> command. The script will ask you for your
     &soc; password.
    </para>
     <screen>&prompt.user;<command>source container-openrc.sh</command></screen>
   </note>
   <procedure>
     <step>
      <para>
       Upload the container image to &ostack; Glance (Image service). This
       example uses the name <literal>CaaSP-3</literal> as the name of the
       image that is created in &soc;.
      </para>
     <screen>&prompt.user;<command>openstack image create --public --disk-format qcow2 \
--container-format bare \
--file SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2 \
<replaceable>CaaSP-3</replaceable></command></screen>
     </step>
     <step>
      <warning>
       <title>Replace the default <literal>root_password</literal></title>
        <para>
         Do not use the <filename>caasp-environment.yaml</filename> directly
         from the GitHub repository.
        </para>
        <para>
         You must make sure to replace the value for
         <literal>root_password</literal> with a secure password.
         This will become the password for the &rootuser; account on all
         nodes in the stack.
        </para>
     </warning>
      <para>
       Download the <filename>caasp-stack.yaml</filename> and
       <filename>caasp-environment.yaml</filename> Heat templates to your
       workstation and then run the <command>openstack stack create</command>
       command.
      </para>
      <screen>&prompt.user;<command>openstack stack create \
-t caasp-stack.yaml \
-e caasp-environment.yaml \
--parameter image=<replaceable>CaaSP-3</replaceable> <replaceable>caasp3-stack</replaceable></command></screen>
     </step>

    <step>
     <para>
      Find out which (external) IP address was assigned to the admin node
      of your &productname; cluster (here: <literal>10.81.1.51</literal>).
     </para>
     <screen>&prompt.user;<command>openstack server list --name "admin" | awk 'FNR > 3 {print $4 $5 $9}'</command>
caasp3-stack-admin|10.81.1.51</screen>
     </step>
     <step>
      <para>
       Visit the external IP address in your browser. You should see the &dashboard;
       login page and can continue with <xref linkend="sec.deploy.nodes.admin_configuration"/>.
      </para>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.deploy.preparation.disk_images">
  <title>Installing from Disk Images</title>
 <para>
  For building clusters from virtual machines on supported
  hypervisors, it is not necessary to individually install each
  node. &suse; offers pre-installed VM disk images in the following
  formats:
 </para>
 <variablelist>
  <varlistentry>
   <term>&kvm; and &xen;</term>
   <listitem>
    <para>
     In QCOW2 format, for &kvm; and for &xen; using full virtualization.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>&xen;</term>
   <listitem>
    <para>
     In QCOW2 format, for &xen; using paravirtualization.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>VMware</term>
   <listitem>
    <para>
     In VMDK format, for VMware ESXi.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>
    VHD
   </term>
   <listitem>
    <para>For Microsoft Hyper-V.</para>
   </listitem>
  </varlistentry>
 </variablelist>

  <sect2 xml:id="sec.deploy.preparation.disk_images.overview">
   <title>Overview</title>
   <para>
    When deploying a cluster node from pre-installed disk images, the setup
    program never runs. Thus, it is not possible to set values such as time
    servers, node names, the &rootuser; password for the &admin_node; and so
    on in the normal way.
   </para>
   <para>
    Instead, you need to set these values using <literal>cloud-init</literal> configuration
    files. You should place these into a specific subdirectory, then make an
    ISO image. Attach this to the VM as a secondary disk. The pre-installed
    operating system on the disk image looks for a medium with the
    volume label <literal>cidata</literal>, searches that volume for the
    configuration files, and applies the settings within them.
   </para>
   <para>
    Full details of the cloud-init metadata files can be found in the
    Configuration chapter of this manual; for more information, see
    <xref linkend="sec.deploy.cloud-init"/>.
   </para>
   <para>
    As with individual node installation, the &admin_node; needs to be
    configured separately and differently from the &worker_node;s.
   </para>
  </sect2>

  <sect2 xml:id="sec.deploy.preparation.disk_images.procedure">
   <title>Procedure for Hypervisors</title>
   <para>
    The process is very similar for all hypervisors. The examples in this
    manual use &kvm; running on &sle;.
   </para>
   <para>
    You need to build at least two ISO images with configuration information:
    one for the &admin_node; and one for the &worker_node;s. If you wish to
    assign specific host names or customize individual &worker_node;s, then
    you should create separate ISO images for each node.
   </para>
   <para>
    Additionally, you need to make a separate copy of the downloaded VM disk
    image for each VM. We suggest keeping the original download elsewhere and
    making a fresh copy for each node, naming it appropriately: for example,
    <literal>caas-admin</literal>, <literal>caas-master</literal>,
    <literal>caas-worker1</literal>, <literal>caas-worker2</literal> and so
    on.
   </para>
   <sect3 xml:id="sec.deploy.preparation.disk_images.configuration">
    <title>Configuration Files</title>
    <para>
     There are two separate configuration files: <filename>user-data</filename>
     and <filename>meta-data</filename>. Each node needs both. Thus, you need to
     prepare at a minimum one pair of files for the &admin_node;, and another
     pair of files for all the &worker_node;s.
    </para>
    <para>
     Place the files into subdirectories named <filename>cc-admin</filename>
     for the &admin_node; and <filename>cc-worker</filename> for the
     &worker_node;s.
    </para>
    <para>
     So, for instance, if your working directory is <filename>~/cloud-config</filename>,
     then for the admin node, you need these two files:
    </para>
    <screen>~/cloud-config/cc-admin/user-data
~/cloud-config/cc-admin/meta-data</screen>
    <para>
     For a &worker_node;, you need:
    </para>
    <screen>~/cloud-config/cc-worker/user-data
~/cloud-config/cc-worker/meta-data</screen>
    <para>
     The same <filename>meta-data</filename> file can be used for both node types.
     Here is an sample <filename>meta-data</filename> file:
     </para>
     <screen>#cloud-config
instance-id: <replaceable>iid-CAAS01</replaceable>
network-interfaces: |
   auto <replaceable>eth0</replaceable>
   iface <replaceable>eth0</replaceable> inet dhcp</screen>
    <para>
     The <filename>user-data</filename> file contains settings such as time
     servers, the &rootuser; password, and the node type.
    </para>
    <para>
     Here is an example <filename>cc-admin/user-data</filename> file for an
     &admin_node;:
    </para>
    <screen>#cloud-config
debug: True
disable_root: False
ssh_deletekeys: False
ssh_pwauth: True
chpasswd:
   list: |
     root:<replaceable>MY_PASSWORD</replaceable>
     expire: False
ntp:
   servers:
     - <replaceable>ntp1.example.com</replaceable>
     - <replaceable>ntp2.example.com</replaceable>
runcmd:
   - /usr/bin/systemctl enable --now ntpd
suse_caasp:
  role: <replaceable>admin</replaceable></screen>
    <para>
     Here is an example <filename>cc-worker/user-data</filename>  for a
     &worker_node;. Rather than providing the &rootuser; password in
     clear text, you can use a hash instead; this example is hashed with
     SHA-256.
    </para>
    <screen>#cloud-config
debug: True
disable_root: False
ssh_deletekeys: False
ssh_pwauth: True
chpasswd:
   list: |
   root:<replaceable>$5$eriogqzq$Dg7PxHsKGzziuEGkZgkLvacjuEFeljJ.rLf.hZqKQLA</replaceable>
     expire: False
suse_caasp:
    role: <replaceable>cluster</replaceable>
    admin_node: <replaceable>caas-admin.example.com</replaceable></screen>
   </sect3>
   <sect3 xml:id="sec.deploy.preparation.disk_images.iso-image">
    <title>Preparing an ISO image</title>
    <para>
     Once you have edited the configuration files with your desired settings,
     you need to create an ISO image with the volume label <literal>cidata</literal>
     containing only the subdirectory for that node type.
    </para>
    <para>
     On &sle; 12 or &opensuse; 42, use <command>genisoimage</command> to do this.
     On &sle; or &opensuse; 15, use <command>mkisofs</command>. The parameters
     are the same for both commands.
    </para>
    <para>
     For example, to create the ISO image for an admin node on a computer
     running &opensuse; 42:
    </para>
    <screen>&prompt.user;sudo genisoimage -output <replaceable>cc-admin.img</replaceable> -volid <replaceable>cidata</replaceable> -joliet -rock <replaceable>cc-admin</replaceable></screen>
    <para>
     To create the ISO image for a worker node on a computer running &opensuse;
     15, substituting the name of the folder containing the configuration
     files for a &worker_node; and titling the volume <literal>cc-worker</literal>:
    </para>
    <screen>&prompt.user;sudo mkisofs -output <replaceable>cc-worker.img</replaceable> -volid <replaceable>cidata</replaceable> -joliet -rock <replaceable>cc-worker</replaceable></screen>
   </sect3>

   <sect3 xml:id="sec.deploy.preparation.qcow2.admin">
    <title>Procedure to Bring Up an &Admin_Node;</title>
    <procedure>
     <step>
      <para>
       Make a folder called <filename>cc-admin</filename>.
      </para>
     </step>
     <step>
      <para>
       In that folder, prepare <filename>user-data</filename> and <filename>meta-data</filename>
       files to set the &rootuser; password, node name and so on for the
       &admin_node;.
      </para>
     </step>
     <step>
      <para>
       Create an ISO file called <filename>cc-admin.img</filename> containing
       these two files:
      </para>
      <screen>&prompt.user;sudo genisoimage -output cc-admin.img -volid cidata -joliet -rock cc-admin</screen>
     </step>
     <step>
      <para>
       Create a new VM for the &admin_node;.
      </para>
     </step>
     <step>
      <para>
       Attach a copy of the downloaded disk image as its main hard disk.
      </para>
     </step>
     <step>
      <para>
       Attach the <filename>cc-admin.img</filename> disk image as a secondary disk.
     </para>
     </step>
     <step>
      <para>
       Start the VM.
      </para>
     </step>
     <step>
      <para>
       Configure the new &admin_node; as in step <xref linkend="sec.deploy.nodes.admin_configuration"/>.
      </para>
     </step>
    </procedure>
   </sect3>

   <sect3 xml:id="sec.deploy.preparation.disk_images.worker">
    <title>Procedure to Bring Up a &Worker_Node;</title>
    <para>
     The first procedure is to generate the configuration files. Unless
     you wish to customize the individual &worker_node;s, then you need only
     do this part once.
    </para>
    <procedure>
     <step>
      <para>
       Make a folder called <filename>cc-worker</filename>.
      </para>
     </step>
     <step>
      <para>
       In that folder, prepare a <filename>user-data</filename> file to set the &rootuser;
       password, network settings and so on for the &worker_node;. If no
       changes are necessary, you can copy the <filename>meta-data</filename>
       file from that used to configure the &admin_node;.
      </para>
     </step>
     <step>
      <para>
       Create an ISO file called <filename>cc-worker.img</filename> containing
       these two files:
      </para>
      <screen>&prompt.user; sudo genisoimage -output cc-worker.img -volid cidata -joliet -rock cc-worker</screen>
     </step>
    </procedure>
    <para>
     Once you have a disk image containing the configuration files, it can be
     reused for multiple &worker_node;s.
    </para>
    <para>
     Then, repeat the following steps for each &worker_node;:
    </para>
    <procedure>
     <step>
      <para>
       Create a new VM for the &worker_node;.
      </para>
     </step>
     <step>
      <para>
       Attach a copy of the downloaded disk image as its main hard disk.
      </para>
     </step>
     <step>
      <para>
       Attach the <filename>cc-worker.img</filename> disk image as a
       secondary disk.
      </para>
     </step>
     <step>
      <para>
       Start the VM.
      </para>
     </step>
    </procedure>
    <para>
     Once you have brought up as many &worker_node;s as you need, proceed to
     bootstap the cluster using the &dashboard; dashboard.
    </para>
   </sect3>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.deploy.preparation.public_cloud">
  <title>Installing in Public Cloud</title>
  <para>
   The &productname; images published by &suse; in selected Public Cloud
   environments are provided as <literal>Bring Your Own Subscription
   (BYOS)</literal> images. &productname; instances need to be registered with
   the &scc; in order to receive bugfix and security
   updates. Images labeled with the <literal>cluster</literal> designation in
   the name are not intended to be started directly; they are deployed by the
   Administrative node. Administrative node images contain the
   <literal>admin</literal> designation in the image name.
  </para>

  <sect2 xml:id="sec.deploy.preparation.public_cloud.installation">
   <title>Installation</title>

   <para>
    &productname; requires a chain of trust and therefore none of the
    administrative services are running when an instance is launched. The
    cluster administrator needs to ssh into the instance and run the
    <command>caasp-admin-setup</command> executable as the
    <literal>root</literal>user.
   </para>

   <para>
    By default the <command>caasp-admin-setup</command> executable operates in
    <literal>wizard</literal> mode, walking you through the necessary steps.
    During this process your &scc; credentials will be requested. Registration
    with &scc; can be skipped. If this step is skipped during setup the admin node
    and the cluster nodes will not receive any updates. While registration to
    &scc; can be performed after the initial setup with
    <literal>SUSEConnect</literal>, performing the registration during setup has
    the advantage that cluster nodes will automatically be registered with &scc;
    as well. If you prefer not to run the <literal>wizard</literal>, use
    <command>caasp-admin-setup --help</command> to obtain a list of the
    available command line arguments.
   </para>

   <para>
    Once the <literal>caasp-admin-setup</literal> process is complete all
    &productname; containers will be launched on the admin node instance. Use
    your web browser to access the Velum dashboard via <literal>https</literal>.
    If you did not provide your own certificate, a certificate was generated for
    you and the fingerprint was written to the terminal in which
    <command>caasp-admin-setup</command> was executed. You can compare this
    fingerprint in your browser to establish the chain of trust.
   </para>

   <para>
    Microsoft Azure does not provide a time protocol service. Please refer to
    <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html">
    SUSE Linux Enterprise Server documentation</link> for more information about
    NTP configuration. No manual NTP configuration is required on the cluster
    nodes, they synchronize time with the admininstration node.
   </para>

   <sect3>
    <title>caasp-admin-setup detail</title>
    <para>
     The general purpose of <command>caasp-admin-setup</command> is to collect
     all information needed to successfully start the &productname; containers.
    </para>
    <para>
     When <command>caasp-admin-setup</command> is executed it determines which
     cluster node image to use according to the cloud framework. For this
     operation to succeed outgoing traffic on port <literal>443</literal> to the
     Internet must be permitted. The code will access the <literal>Public Cloud
     Information Tracker</literal> service operated by SUSE. This service
     provides information about all images ever released to the Public Cloud by
     SUSE. The latest available cluster node image for this version of
     &productname; will be used. This initial outreach and image filtering
     introduces a small startup delay before the command line options are
     processed or the wizard mode starts.
    </para>
    <sect4>
     <title>Providing SSL certificate and key</title>
     <para>
      You may choose to supply your own SSL certificate and key for initial
      access the dashboard, with the <literal>--ssl-crt</literal> and
      <literal>--ssl-key</literal> options or by answering the question
      <quote>Would you like to use your own certificate from a known (public
      or self signed) Certificate Authority?</quote> with
      <literal>y</literal>.
     </para>
     <para>
      In order to use your own SSL certificate and key you must upload the files
      to the admin node into a location of your choice. This location is then
      provided to the setup code. For example, if your certificate is called
      <filename>my-velum.crt</filename> and you uploaded it to
      <filename>/tmp</filename> then the <command>caasp-admin-setup</command>
      code expects <filename>/tmp/my-velum.crt</filename> as the location for
      the SSL certificate. The same concept applies to the SSL key. The
      certificate and key will be placed in the appropriate locations on the
      admin node.
     </para>
    </sect4>
    <sect4>
     <title>Velum Administrator credentials</title>
     <para>
      Velum is the name of the administrative dashboard web interface. The setup
      code will ask for an e-mail address and a password if not supplied with
      the <literal>--admin-email</literal> and
      <literal>--admin-password</literal> arguments. These are the
      administrative credentials to log into the Velum dashboard. The e-mail
      used does not have to be an e-mail associated with your &scc; account.
      Please do not forget the values you enter, as they cannot be recovered.
     </para>
    </sect4>
    <sect4>
     <title>Registering with &scc;</title>
     <para>
      The setup code will ask for an e-mail address and the registration code.
      Use your &scc; credentials that provide access to the &productname; product
      in &scc;. The admin node and all cluster nodes will get registered to &scc;.
      The registration process requires access to the Internet on port 443.
      Alternatively you may use the <literal>--reg-email</literal> and
      <literal>--reg-code</literal> arguments. Registration with &scc; is
      optional. However, without registration the system will not receive any
      updates unless specifically setup to receive updates via a different route
      such as a private &smt; server. Registration after the initial setup also
      requires an explicit registration of each node in the cluster.
     </para>
     <para>
      When all information is collected, accept your selections/input with
      <literal>y</literal> to complete the initial setup.
     </para>
    </sect4>
   </sect3>
  </sect2>
  <sect2 xml:id="sec.deploy.preparation.public_cloud.bootstrapping">
   <title>Bootstrapping a Cluster</title>

   <para>
    To finalize the configuration and bootstrap the cluster, log into Velum
    using your admininstrative e-mail address and password. This will take you
    to the <guimenu>Initial CaaS Platform Configuration</guimenu> page. The
    field <guimenu>Internal Dashboard FQDN/IP</guimenu>should be preset with the
    internal IP address of your administrative host. You may also choose to
    install Helm or change the configuration of the overlay network. Clicking
    <guimenu>Next</guimenu> will take you to the cloud framework specific
    configuration page.
   </para>

   <sect3>
    <title>Configuration</title>
    <para>
     Within a Public Cloud environment, bootstrapping a &productname; cluster is
     performed in an automated manner. You must simply choose the instance type
     best suited to your intended workloads, size the cluster, and specify a few
     configuration details. The instances will be created, and automatically
     joined to the cluster, skipping the <literal>pending</literal> state. At
     this point you can continue with the standard setup procedure.
    </para>
    <para>
     In &productname; version 2.1, cluster size has a minimum of three nodes and
     a maximum of 50 nodes.
    </para>
    <sect4>
     <title>Amazon Web Services EC2</title>
     <para>
      You may select from one of the predefined instance types, hand selected
      for general container workloads, or choose <guimenu>Other
      types...</guimenu> and enter any <guimenu>instance type</guimenu>, as
      defined at <link xlink:href="https://aws.amazon.com/ec2/instance-types/">
      https://aws.amazon.com/ec2/instance-types/</link>
     </para>
     <para>
      Two configuration options are required in EC2:
     </para>
     <variablelist>
      <varlistentry>
       <term>Subnet ID</term>
       <listitem>
        <para>
         The <literal>subnet</literal> within which cluster nodes will be
         attached to the network, in the form
         <literal>subnet-xxxxxxxx</literal>.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Security Group ID</term>
       <listitem>
        <para>
         The <literal>security group</literal> defining network access rules for
         the cluster nodes, in the form <literal>sg-xxxxxxxx</literal>.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      The defaults used for those two options are preset to the subnet ID of the
      administration host and the security group ID that was automatically
      created by <command>caasp-admin-setup</command>. You may choose to place
      the cluster nodes in a different subnet and you can also use a custom
      security group, but please bear in mind that traffic must be allowed
      between the individual cluster nodes and also between the admininstration
      node and the cluster nodes.
     </para>
     <para>
      See the <link xlink:href="https://aws.amazon.com/documentation/vpc/">
      Amazon Virtual Private Cloud Documentation</link>for more information.
     </para>
    </sect4>
    <sect4>
     <title>Microsoft Azure</title>
     <para>
      You need to configure credentials for access to the Azure framework so
      instances can be created, as well as parameters for the cluster node
      instances themselves. The credentials refer to authentication via a
      service principal. See
      <link xlink:href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal">
      https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal</link>
      for more information on how you can create a service principal.
     </para>
     <variablelist>
      <varlistentry>
       <term>Subscription ID</term>
       <listitem>
        <para>
         The subscription ID of your Azure account.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Tenant ID</term>
       <listitem>
        <para>
         The tenant ID of your service principal, also known as the Active
         Directory ID.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Application ID</term>
       <listitem>
        <para>
         The application ID of your service principal.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Client Secret</term>
       <listitem>
        <para>
         The key value or password of your service principal.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      Below the <guimenu>Service Principal Authentication</guimenu> box you will
      find the <guimenu>Instance Type</guimenu> configuration. You may select
      from one of the predefined instance types, hand selected for general
      container workloads, or choose <guimenu>Other types...</guimenu> and enter
      any <literal>size</literal>, as defined at
      <link xlink:href="https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes/">
      https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes/</link>Set
      the <guimenu>Cluster size</guimenu> using the slider.
     </para>
     <para>
      The parameters in <guimenu>Resource Scopes</guimenu> define attributes of
      the cluster instances, as required for Azure Resource Manager:
     </para>
     <variablelist>
      <varlistentry>
       <term>Resource Group</term>
       <listitem>
        <para>
         The Resource Group in which all cluster nodes will be created.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Storage Account</term>
       <listitem>
        <para>
         The Storage Account that will be used for storing the cluster node OS
         disks. See
         <link xlink:href="https://docs.microsoft.com/en-us/azure/storage/common/storage-create-storage-account">
         https://docs.microsoft.com/en-us/azure/storage/common/storage-create-storage-account</link>for
         more information about Azure Storage Accounts.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Network</term>
       <listitem>
        <para>
         The virtual network the cluster nodes will be connected to.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Subnet</term>
       <listitem>
        <para>
         A subnet in the previously defined virtual network. See
         <link xlink:href="https://docs.microsoft.com/en-us/azure/virtual-network/">
         https://docs.microsoft.com/en-us/azure/virtual-network/</link> for more
         information about Azure Virtual Networks.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </sect4>
    <sect4>
     <title>Google Compute Engine</title>
     <para>
      You may select from one of the predefined instance types, hand selected
      for general container workloads, or choose <guimenu>Other
      types...</guimenu> and enter any <literal>machine type</literal>, as
      defined at
      <link xlink:href="https://cloud.google.com/compute/docs/machine-types">
      https://cloud.google.com/compute/docs/machine-types</link>
     </para>
     <para>
      Two configuration options are required in GCE:
     </para>
     <variablelist>
      <varlistentry>
       <term>Network</term>
       <listitem>
        <para>
         The name of the virtual network the cluster nodes will run within.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Subnet</term>
       <listitem>
        <para>
         If you created a custom network, you must specify the name of the
         subnet within which the cluster nodes will run.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      See the <link xlink:href="https://cloud.google.com/vpc/docs/vpc"> GCE
      Network Documentation</link> for more information.
     </para>
    </sect4>
   </sect3>

   <sect3>
    <title>Bootstrapping the Cluster</title>
    <para>
     Clicking <guimenu>Next</guimenu> on the framework specific page will start
     creating the cluster node instances and take you to the discovery page.
     Once the instances are up and running, they will appear in the list of
     <guimenu>Nodes Found</guimenu>. Assign each node a role (worker or master)
     and click <guimenu>Next</guimenu> to get to the bootstrap page.
    </para>
    <para>
     On the bootstrap page, there are two more configuration fields. One is
     labeled <guimenu>External Kubernetes API FQDN</guimenu>. This is the FQDN
     of one of the master nodes, and it must be resolvable via DNS and reachable
     from your network. This name will be used in the configuration for
     <command>kubectl</command>. Kubectl is the command line tool that allows
     you to control the kubernetes cluster.
    </para>
    <para>
     The other configuration field is labeled <guimenu>External Dashboard
     FQDN</guimenu>. This is the FQDN of your administration host, and it must
     be resolvable via DNS and reachable from your network. When done
     configuring, click <guimenu>Bootstrap cluster</guimenu>. The bootstrap
     process will take a few minutes. When finished, your cluster is be ready.
    </para>
   </sect3>
  </sect2>
  <sect2 xml:id="sec.deploy.preparation.public_cloud.operating">
   <title>Operating the Cluster</title>
   <para>
    Kubectl uses a configuration file, <filename>kubeconfig</filename>, that
    defines parameters to access a kubernetes cluster. This configuration file
    can be generated on the admin node. Click <guimenu>kubectl config</guimenu>
    to see instructions on how to generate the <literal>kubeconfig</literal>
    file.
   </para>
  </sect2>
 </sect1>
</chapter>
