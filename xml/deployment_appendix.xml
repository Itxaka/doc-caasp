<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE appendix
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<appendix xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="app.kvm">
 <title>Appendix</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:bugtracker>
          </dm:bugtracker>
      </dm:docmanager>
    </info>
    <para/>
 <sect1 xml:id="deployment.appendix.admin_node">
  <title>Installing an &Admin_Node; using &ay;</title>

  <para/>

  <para>
   To assist with automating the installation of &productname; clusters as much
   as possible, it is possible to automatically install the &admin_node; with
   &ay;, similarly to the process used for &worker_node;s.
  </para>
  <para>
   Be aware, though, that this requires considerable customisation of the
   <literal>autoyast.xml</literal> file.
  </para>
  <para>
   Here is a sample file to create an &admin_node;.
  </para>
  <screen>
<xi:include href="autoyast_example_adminnode.xml" parse="text"/>
  </screen>

  <para/>

  <para>
   Copy the above and paste it into a file named <literal>autoyast.xml</literal>,
   then edit it as appropriate for your configuration. After you have prepared
   the file, you will need to put it on a Web server that is accessible to the
   &productname; cluster.
  </para>
  <para>
   After this, install the admin node by following the same procedure as for a
   &worker_node; in <xref linkend="sec.deploy.nodes.worker_install.manual.autoyast"/>.
  </para>
  <para>
   For more information about using and customizing &ay;, refer to
   <link xlink:href="https://www.suse.com/documentation/sles-12/book_autoyast/data/invoking_autoinst.html#commandline_ay"/>.
  </para>
  <para>
   For more information about using pre-hashed passwords, refer to
   <xref linkend="sec.deploy.cloud-init.user-data.password"/>.
  </para>
 </sect1>
 <sect1 xml:id="deployment.appendix.cilium">
  <title>Installing &cilium;</title>

  <warning>
   <title>Tech Preview</title>
   <para>
    Please note &cilium; is currently only a tech preview. It will work but
    is not officially supported.
   </para>
  </warning>

  <para>
   &kube; allows to choose one of many available Network Plugins, which usually
   come in form of &cni; plugins.
  </para>
  <para>
   &kube; provides a specification called &net_pols; which define how groups of
   pods are allowed to communicate with each other and with other network
   endpoints. &net_pols; are clustered firewall rules defined with &kube;
   concepts. But &kube; does not provide &net_pols; implementation on its own.
  </para>
  <para>
   &cni; plugins are supposed to implement &net_pols; for &kube;. By default,
   &productname; ships &flannel; as a default &cni; plugin. However, &flannel;
   does not provide &net_pols; support.
  </para>
  <para>
   &productname; provides another &cni; plugin as a tech preview feature -
   &cilium; - which supports &net_pols;.
  </para>
  <para>
   The main characteristic of &cilium; is using &ebpf;
   (<link xlink:href="https://opensource.com/article/17/9/intro-ebpf">extended Berkeley Packet Filter</link>)
   as a mechanism for filtering incoming and outgoing packets in containers.
   &ebpf; is a bytecode interpreter inside Linux kernel which allows to write
   programs to analyze, filter network packets and to monitor system function
   calls. &cilium; translates &net_pols; into &ebpf; programs which are loaded
   into the kernel.
  </para>
  <para>
   This section shows how to set up &cilium; on &productname; and check whether
   it is working.
  </para>
  <sect2 xml:id="deployment.appendix.cilium.disclaimers">
   <title>Disclaimers</title>
   <important>
    <para>
     This document describes a temporary way of &cilium; deployment. Before
     releasing &productname; v4, &cilium; deployment will be supported by
     &dashboard;.
    </para>
   </important>
   <para>
    All steps need to be done <emphasis role="strong">before</emphasis> the
    cluster is deployed with &dashboard;.
   </para>
   <para>
    The current implementation of &cilium; deployment in &productname; does not
    allow yet to use L7 network policies (only L3 and L4).
    <!-- The support of L7
    policies will be enabled when &envoy; will be packaged and shipped in
    &opensuse; and &sle;. &envoy; will also bring a possibility of creating a
    &svc_mesh; with &istio;. -->
   </para>
   <para>
    &productname; provides &cilium; in version 1.2.
   </para>
  </sect2>
  <sect2 xml:id="deployment.appendix.cilium.salt-conf">
   <title>Configuring of &smaster; to deploy &cilium;</title>
   <procedure>
    <title>Change &smaster; configuration to choose &cilium; as a &cni; plugin</title>
    <step>
     <para>
      Connect to the &Admin_Node; and execute the following command:
     </para>
     <screen>&prompt.root;<command>transactional-update shell</command></screen>
     <para>
      This will open a <command>transactional-update</command> shell.
     </para>
    </step>
    <step>
     <para>
      From this shell, edit the
      <filename>/usr/share/salt/kubernetes/pillar/cni.sls</filename> file.
      Inside <literal>cni</literal> section, change the <literal>plugin</literal>
      from <literal>flannel</literal> to <literal>cilium</literal>.
     </para>
     <para>
      The last section of the file should look like this:
     </para>
<screen>
cni:
  plugin: 'cilium'
  dirs:
    bin:  '/var/lib/kubelet/cni/bin'
    conf: '/etc/cni/net.d'
   </screen>
    </step>
    <step>
     <para>
      Once the file contents are changed, exit from the shell and reboot the
      system:
     </para>
<screen>&prompt.root;<command>exit</command>
&prompt.root;<command>reboot</command></screen>
    </step>
   </procedure>
  </sect2>
  <sect2 xml:id="deployment.appendix.cilium.deploying">
   <title>Deploying the cluster</title>
   <para>
    After the successful reboot, go to &dashboard; web interface and
    proceed with the deployment of the cluster. Feel free to change the
    networking settings on the bootstrap page (i.e. <literal>CIDR</literal>);
    all these settings are going to be used by &cilium;.
   </para>
  </sect2>
  <sect2 xml:id="deployment.appendix.cilium.checking">
   <title>Checking whether &cilium; works</title>
   <para>
    This section is based on the official &cilium; documentation which provides
    <link xlink:href="http://docs.cilium.io/en/v1.4/gettingstarted/http/">an example for checking &net_pols; in &cilium;</link>.
   </para>
   <para>
    It comes with three microservices applications inspired by Star Wars:
   </para>
   <variablelist>
    <varlistentry>
     <term>&deathstar;</term>
     <listitem>
      <para>
       An HTTP webservice, which is exposed as a Kubernetes Service. It provides
       landing services to the Empire's spaceships which can request a landing.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&tiefighter;</term>
     <listitem>
      <para>
       A pod which serves as a landing-request client. It's Empire's ship.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&xwing;</term>
     <listitem>
      <para>
       A pod which serves as a landing-request client. It's Alliance's ship.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <procedure>
    <title>Checking whether &cilium; works with Star Wars microservices</title>
    <step>
     <para>
      Deploy Star Wars applications with the following command:
     </para>
<screen>&prompt.root;<command>kubectl create -f https://raw.githubusercontent.com/cilium/cilium/v1.2/examples/minikube/http-sw-app.yaml</command>
service/deathstar created
deployment.extensions/deathstar created
pod/tiefighter created
pod/xwing created</screen>
    </step>
    <step>
     <para>
      Check the status of pods and services, wait until they are all ready.
     </para>
<screen>&prompt.root;<command>kubectl create -f https://raw.githubusercontent.com/cilium/cilium/v1.2/examples/minikube/http-sw-app.yaml</command>
NAME                             READY   STATUS    RESTARTS   AGE
pod/deathstar-6fb5694d48-5hmds   1/1     Running   0          107s
pod/deathstar-6fb5694d48-fhf65   1/1     Running   0          107s
pod/tiefighter                   1/1     Running   0          107s
pod/xwing                        1/1     Running   0          107s

NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
service/deathstar    ClusterIP   10.96.110.8   &lt;none&gt;  80/TCP    107s
service/kubernetes   ClusterIP   10.96.0.1     &lt;none&gt;  443/TCP   3m53s</screen>
    </step>
    <step>
     <para>
      Check whether both tiefighter and xwing are allowed to connect to deathstar and acces the API. They should, because there was no &net_pol; applied yet.
     </para>
     <screen>&prompt.root;<command>kubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing</command>
Ship landed
&prompt.root;<command>kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing</command>
Ship landed</screen>
    </step>
    <step>
     <para>
      Apply the &net_pol; which allows to connect to deathstar only from
      endpoints which have label <literal>org: empire</literal>.
     </para>
     <screen>&prompt.root;<command>kubectl create -f https://raw.githubusercontent.com/cilium/cilium/v1.2examples/minikube/sw_l3_l4_policy.yaml</command>
ciliumnetworkpolicy.cilium.io/rule1 created</screen>
    </step>
    <step>
     <para>
      Now try to connect to deathstar from tiefighter. It should work.
     </para>
     <screen>&prompt.root;<command>kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing</command>
Ship landed</screen>
    </step>
    <step>
     <para>
      And then try to connect to deathstar from xwing. The request shold hang
      forever and pressing Control-C will be needed to kill the curl request.
     </para>
     <screen>&prompt.root;<command>kubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing</command></screen>
    </step>
   </procedure>
   <para>
    &cilium; is up and running properly when the previous steps all completed
    except the last one; which should be forbidden by the policy.
   </para>
  </sect2>
 </sect1>
</appendix>
