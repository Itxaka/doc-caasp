<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.deployment.scenarios"
 xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Deployment Scenarios</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sec.deploy.scenarios.default">
  <title>Default Scenario</title>

  <para>
   In the default scenario &productname; is deployed in such a way, that its
   components have access (either direct or via proxy) to resources on the
   internet.
  </para>

  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="direct_connection.png" width="100%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="direct_connection.png" width="100%"/>
    </imageobject>
   </mediaobject>
  </informalfigure>
 </sect1>
 <sect1 xml:id="sec.deploy.scenarios.airgap">
  <title>Airgapped Installation</title>
  <para>
   An airgapped deployment can not have any direct connection to the Internet
   or external networks.
  </para>
  <para>
   All data must be transferred into the airgapped network in a secure fashion.
  </para>

  <screen>
Current:
* Deploy RMT external, warm it
* Deploy RMT internal
* Transfer RMT cache to internal
* Deploy external container registry mirror
* Deploy internal container registry mirror
* Set up CaaSP on single node to get access to patched docker
* Put the patched docker daemon config on the caasp node to talk to external registry mirror
* Deploy internal helm chart repository mirror
* Manually warm helm chart repo mirror
* Manually define the images you need by trial and error (or magic)
* Manually warm the external container cache through patched docker with your list of images
* Transfer the registry cache to the internal mirror
* Deploy CaaSP cluster in internal network

Future:
* Deploy RMT external, warm it
* Deploy RMT internal
* Transfer RMT cache to internal
* Deploy external container registry mirror
* Deploy internal container registry mirror
* Deploy internal helm chart repository mirror
* Have cache warming tool that downloads the helm charts, extracts the images, downloads and caches the images on external registry mirror
* Yet undiscussed method to run helm chart mirror (update)
* Transfer the registry cache to the internal mirror
* Deploy CaaSP1 cluster in internal network
  </screen>

  <important>
   <title>Scope Of This Document</title>
   <para>
    This document focuses on providing mirrors for the resources provided by
    &suse; and required for basic &productname; functionality. If you require
    additional functionality, you can use these instructions as an example on
    how to provide additional mirrors.
   </para>
   <para>
    Providing a full set of mirroring instructions, for all usage scenarios, is
    beyond the scope of this document.
   </para>
  </important>

  <sect2 xml:id="sec.deploy.scenarios.airgap.concepts">
   <title>Concepts</title>
   <para>
    Requirement: Strictly NO connection (not even proxy) between internal and
    external/upstream layers
   </para>
   <para>
    In order to disconnect &productname; from the external network, we must
    provide ways for the components to retrieve data from alternative sources
    inside the trusted network.
   </para>
   <para>
    The three main sources that must be replaced are:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &suse; &mos; RPM packages
     </para>
     <para>
      Provided by the &suse; package repositories
     </para>
    </listitem>
    <listitem>
     <para>
      Docker/CRI-O Container images
     </para>
     <para>
      Provided by the &suse; container registry (https://registry.suse.com)
     </para>
    </listitem>
    <listitem>
     <para>
      Helm installation charts
     </para>
     <para>
      Provided by the &suse; helm chart repository
      (https://kubernetes-charts.suse.com/)
     </para>
    </listitem>
   </itemizedlist>
   <para>
    You will need to create mirror servers inside the air gapped network which
    act as the replacement for the default sources.
   </para>
   <para>
    These internal mirrors must be updated with data retrieved from the
    original upstream sources in a trusted and secure fashion. To achieve this,
    you will need an additional set of mirroring servers outside of the
    airgapped network which act as first stage mirrors.
   </para>
   <para>
    Updating of mirrors happens in three stages.
   </para>
   <orderedlist>
    <listitem>
     <para>
      Update the external mirror from upstream.
     </para>
    </listitem>
    <listitem>
     <para>
      Transfer the updated data onto a trusted storage device.
     </para>
    </listitem>
    <listitem>
     <para>
      Update the internal mirror from the trusted storage device.
     </para>
    </listitem>
   </orderedlist>
   <informalfigure>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="airgap.png" width="100%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="airgap.png" width="100%"/>
     </imageobject>
    </mediaobject>
   </informalfigure>
   <variablelist>
    <varlistentry>
     <term>upstream</term>
     <listitem>
      <para>
       Outside the controlled network.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>external</term>
     <listitem>
      <para>
       Inside the controlled network, outside the airgapped network.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>internal</term>
     <listitem>
      <para>
       Inside the airgapped network.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Once the replacement sources are in place, the key components are
    reconfigured to use the mirrors as their main sources.
   </para>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.machines">
   <title>Machines Needed In Addition To The Cluster</title>
   <para>
    All of these can be hosted on one shared machine for each location.
   </para>
   <para>
    External:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>1</literal> (virtual) machine for the RMT server (SLES 15)
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>1</literal> (virtual) machine for every container registry
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Internal:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>1</literal> (virtual) machine for the RMT server (SLES 15)
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>1</literal> (virtual) machine for every container registry
     </para>
     <para>
      You can host multiple container registries on a single machine. They must
      be run on different ports and use different storage paths.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>1</literal> (virtual) machine for the helm chart server
     </para>
    </listitem>
   </itemizedlist>
   <note>
    <title>Adjust Number Of Mirror Servers</title>
    <para>
     Multiply all these numbers by the amount of fallback/failover you require.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.storage">
   <title>Trusted Storage</title>
   <para>
    Transferring data from the external network mirror to the internal mirror
    can be performed in many ways. The most common way is portable storage (USB
    keys or external hard drives).
   </para>
   <para>
    Sizing of the storage is dependent on the number of data sources that need
    to be stored. Container images can easily measure several Gigabytes per
    item although they are generally smaller for Kubernetes related
    applications. The overall size of any given RPM repository is at least tens
    of Gigabytes. At the time of writing the package repository for &sls; alone
    contains <literal>36GB</literal> of data. It is highly recommended to have
    enough storage for multiple snapshots of the repositories.
   </para>
   <para>
    We recommend external storage with at least <literal>500GB</literal>.
   </para>
   <note>
    <title>Handling Of Secure Storage</title>
    <para>
     Data integrity checks, duplication, backup, and secure handling procedures
     of secure storage are beyond the scope of this document.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.rpm-repository">
   <title>RPM Repository Mirror</title>
   <para>
    Providing RPM packages is be handled by the
    <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/book_rmt.html">&rmtool;
    (RMT)</link>. RMT will provide a repository server that holds the packages
    and related metadata for &mos; to install them like from the upstream
    repository. Data is synchronized once a day to the external mirror
    automatically. You can then copy this data to your trusted storage at any
    point and update the internal mirror.
   </para>
   <sect3>
    <title>Mirror Configuration</title>
    <note>
     <title>If Possible, Deploy The Mirror Before &productname; Cluster Deployment</title>
     <para>
      The mirror on the internal network should be up and running before
      deploying &productname;, this makes rolling out much easier since you can
      configure the nodes during installation to use the correct internal
      mirror. If the cluster is deployed before the mirror is running, you must
      reconfigure all nodes manually.
     </para>
    </note>
    <para>
     Set up two
     <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/cha_rmt_installation.html">RMT
     mirrors</link>. One in the external network and one in the airgapped
     network.
    </para>
    <para>
     The internal RMT mirror must be available on <literal>HTTP/HTTPS</literal>
     ports <literal>80/443</literal> for the &productname; cluster nodes.
    </para>
   </sect3>
   <sect3>
    <title>Client Configuration</title>
    <para>
     Configure all &productname; nodes to
     <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/sec_rmt_client_clientsetupscript.html">get
     updates from the internal RMT server</link>.
    </para>
    </sect3>
   <sect3>
    <title>Updating The Mirror</title>
    <para>
     Follow the procedure in
     <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/sec_rmt_mirroring_export_import.html" />.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.container-registry">
   <title>Container Registry Mirror</title>
   <para>
    Container image registries are provided by SUSE, Docker, and other sources.
    The &suse; container registry is used to update the &productname;
    components.
   </para>
   <sect3>
    <title>Mirror Configuration</title>
    <important>
     <title>This example does not provide any security measures</title>
     <para>
      The default configuration is not using certificates to secure the
      communication between this registry and all the nodes communicating with
      it. There is also no authentication and authorization in place.
     </para>
     <para>
      For production usage, we highly recommend deploying the registry using
      TLS certificates and implementing authentication and authorization by
      deploying a Portus instance.
     </para>
     <para>
      In addition: access to the container image registries is not restricted
      in any way. If you need to secure the registry specifically please refer
      to: <link xlink:href="https://docs.docker.com/registry/deploying/#restricting-access">Docker Registry: Restricting Access</link>.
      </para>
    </important>
    <note>
     <title>Registry Mirrors Are Read Only</title>
     <para>
      For security reasons, both the external and internal registry mirrors will
      be configured in <literal>read-only</literal> mode. Therefore, pushing
      container images to these mirrors will not be possible.
      </para>
      <para>
       If you need the ability to store modified container images, you will
       have to create another registry that will hold these images. The steps
       needed to run your own full container image registry are not part of this
       document. For more information you can refer to:
       <link xlink:href="https://docs.docker.com/registry/">Docker Registry</link>.
       </para>
      </note>
    <para>
     Internal and airgapped mirrors have different configurations.
    </para>
     <para>
      The external mirror is configured as a <link xlink:href="https://docs.docker.com/registry/recipes/mirror/">"pull through"-cache</link>.
      Which means, it connects to the upstream registry and any attempt to
      pull an image from this registry is routed through this mirror. The images
      pulled through this mirror will be stored in the mirror. This is the
      basis for the data you need to transfer to the internal mirror.
     </para>
     <para>
      The internal airgapped registry is configured in <literal>read only</literal>
      mode.
    </para>
    <para>
     You can host multiple registries on one internal mirror the need to run on
     different ports. The respective container registry mirror must be
     available to the &productname; cluster nodes at the configured port
     (Default: <literal>5000</literal>).
    </para>
    <procedure>
     <title>Set Up The External Mirror</title>
     <step>
      <para>
       <link xlink:href="https://www.suse.com/documentation/sles-15/book_quickstarts/data/sec_sle_installquick.html">Set
       up a &sls; 15 machine</link> on the external network.
      </para>
      <para>
       Make sure you have
       <link xlink:href="https://www.suse.com/documentation/sles-15/book_quickstarts/data/sec_modules_installing.html">enabled
       the <filename>containers</filename> module</link>.
      </para>
     </step>
     <step>
      <para>
       Install the <filename>docker-distribution-registry</filename> package:
      </para>
<screen>&prompt.sudo;<command>zypper in docker-distribution-registry</command>
</screen>
     </step>
     <step>
      <para>
       The registry package provides a default configuration file:
       <filename>/etc/registry/config.yml</filename>. Fill in the necessary
       configuration details. For example: Add the <literal>remoteurl</literal>
       of the registry you wish to connect to.
      </para>
      <screen>
proxy:
  remoteurl: https://registry.suse.de
</screen>
     <para>
      For details on the configuration, refer to: <link xlink:href="https://docs.docker.com/registry/configuration/">Docker Registry: Configuration</link>
     </para>
     </step>
     <step>
      <para>
       Now start the registry service and enable it at boot time:
      </para>
<screen>
&prompt.sudo;<command>systemctl enable --now registry.service</command>
         </screen>
     </step>
    </procedure>
    <procedure>
     <title>Set Up Internal Mirror</title>
     <step>
      <para>
       <link xlink:href="https://www.suse.com/documentation/sles-15/book_quickstarts/data/sec_sle_installquick.html">Set
       up a &sls; 15 machine</link> on the airgapped network.
      </para>
      <para>
       Make sure you have
       <link xlink:href="https://www.suse.com/documentation/sles-15/book_quickstarts/data/sec_modules_installing.html">enabled
       the <filename>containers</filename> module</link>.
      </para>
     </step>
     <step>
      <para>
       Install the <filename>docker-distribution-registry</filename> package:
      </para>
<screen>&prompt.sudo;<command>zypper in docker-distribution-registry</command>
</screen>
     </step>
     <step>
      <para>
       In the default configuration file
       <filename>/etc/registry/config.yml</filename>, identify the
       <literal>storage</literal> section and add the following lines:
      </para>
<screen>
maintenance:
    readonly:
      enabled: true
       </screen>
       <para>
        For details on the configuration, refer to: <link xlink:href="https://docs.docker.com/registry/configuration/">Docker Registry: Configuration</link>
       </para>
      <para>
       These lines will make the registry read-only, preventing its images from
       being overwritten by the clients of the air-gapped network. The complete
       file will look like:
      </para>
<screen>
version: 0.1
log:
  level: info
storage:
  filesystem:
    rootdirectory: /var/lib/docker-registry
  maintenance:
    readonly:
      enabled: true
http:
  addr: 0.0.0.0:5000
         </screen>
      <important>
       <title>Do Not Configure The <literal>proxy</literal> Section For The Internal Mirror</title>
       <para>
        Do not add the “proxy” section like you must do on the external
        on-premise mirror. This is not needed and would cause the registry
        service to refuse to start since the airgapped network can not reach
        the specified proxy.
       </para>
      </important>
     </step>
    </procedure>
<!-- FIXME mnapp 18.9.18 Shouldn't we explain how to provide this default auth/security? -->
   </sect3>
   <sect3>
    <title>Updating The Mirror</title>
    <note>
     <para>
      It’s not needed to stop the registry services while doing the backup
      and restore procedures.
     </para>
    </note>
    <para>
     Configure &kube; to use this internal registry. <guimenu>Velum &rarr;
     Settings &rarr; Remote Registries</guimenu>.
    </para>
    <para>
     Configure &productname; to rewrite external registry requests to that
     mirror <guimenu>Velum &rarr; Settings &rarr; Mirrors</guimenu>.
    </para>
    <procedure>
     <title>Update Container Registry Mirror</title>
     <step>
      <para>
       <link xlink:href="https://docs.docker.com/registry/deploying/#copy-an-image-from-docker-hub-to-your-registry">Copy
       the docker images to your external registry mirror</link>
      </para>
<screen>
Example for docker image pull here
     </screen>
     </step>
     <step>
      <para>
       Tag the image with your internal mirror name
      </para>
<screen>
Example for docker tag with hostname:port here
     </screen>
     </step>
    </procedure>
<screen>
     Container images:

     . Update external container registry mirrors from upstream
     . Copy registry data onto data storage
     . Connect storage to internal instance and upload data
     </screen>
<screen>
     Synchronizing contents
     This procedure illustrates how to synchronize the air-gapped registry with the traditional on-premise one.

         1. Attach an external hard drive to “mirror.local.lan” and copy all the contents of “/var/lib/docker-registry” to it:

     sudo rsync -aP --delete /var/lib/docker-registry /mnt/usbdisk

         2. Attach the external hard drive to “mirror.secure.lan” and import the contents into the registry:

     sudo rsync -aP --delete /mnt/usbdisk/docker-registry/ /var/lib/docker-registry/

     Additional security steps can be introduced between step #1 and #2; for example scanning the exported data using corporate tools before introducing them inside of the air-gapped network.

     Using the mirror
     Using the air-gapped mirror works exactly like using a traditional on-premise mirror. Please refer to SUSE Container as Service Platform documentation to configure your cluster to take advantage of this air-gapped mirror.

     Nothing has to be changed inside of Dockerfile(s), Kubernetes manifest files, Helm charts, custom scripts, etc. All the images with a prefix “registry.suse.com/” will be automatically pulled from this air-gapped mirror.

     The mirror is read-only and can serve only the images that were previously cached by the regular on-premise mirror.
         </screen>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.helm-charts">
   <title>Helm Chart Repository Mirror</title>
   <sect3>
    <title>Mirror Configuration</title>
    <para>
     Set up an internal Helm chart mirror. This can be any webserver capable of
     hosting static files. Add the internal mirror as a helm repo to the Admin
     node.
    </para>
   </sect3>
   <sect3>
    <title>Client Configuration</title>
    <screen>
     Add the webserver as a repo to helm on the admin node.
     https://docs.helm.sh/helm/#helm-repo-add
    </screen>
   </sect3>

   <sect3 xml:id="sec.deploy.scenarios.airgap.helm-charts.update">
    <title>Updating The Mirror</title>
    <para>
     Helm charts:

     . Download helm charts from upstream repository
     . Transfer charts to internal charts repo webserver
     . Run `helm repo update` on admin node
    </para>
    <procedure>
     <title>Download Helm Charts From Suse Repository</title>
     <step>
     <para>
      On the external network, log in to your external mirror or any Linux
      workstation with internet access.
     </para>
     </step>
     <step>
      <para>
      Move to your desired working directory and create a new sub-directory
      <filename>suse-charts</filename>. Enter this directory.
     </para>
     <screen>
    &prompt.user;<command>mkdir suse-charts</command>
    &prompt.user;<command>cd suse-charts</command>
     </screen>
     </step>
     <step>
    <para>
     Download the <filename>index.yaml</filename> file from the &suse; Helm
     Chart Repository. Then use the file to download the Helm charts.
    </para>
     <screen>
    &prompt.user;<command>curl -O https://kubernetes-charts.suse.com/index.yaml</command>
    &prompt.user;<command>grep 'https' index.yaml | awk {'print $2'} | xargs -d "\n" -i curl -O {}</command>
    </screen>
    </step>
    <step>
    <para>
     Package the index file and the charts in an archive.
     </para>
     <screen>
    &prompt.user;<command>tar czvf suse-charts-`date +%F`.tar.gz *</command>
    </screen>
    </step>
    <step>
     <para>
      Transfer <filename>suse-charts-*</filename> to the webserver and unpack
      it to the desired location. The <filename>index.yaml</filename> file must
      be in the root directory for your webserver.
      </para>
     </step>
      <step>
      <para>
       Update the repository information on the &productname; admin node.
      </para>
       <screen>
        helm repo update
       </screen>
      </step>
    </procedure>


   </sect3>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.caasp-deployment">
   <title>Considerations for Deployment Of &productname; In An Airgapped Environment</title>
   <sect3>
    <title>Using the ISO</title>
    <para>
     From YaST register the node against the RMT server. This will ensure the
     final node zypper repositories are pointed against RMT, moreover all the
     available updates are going to be installed (-> no need to run a
     transactional-update right after the installation -> way faster).
    </para>
   </sect3>
   <sect3>
    <title>Using AutoYast</title>
    <para>
     Ensure the admin node is registered against RMT, that will ensure the
     nodes are provisioned by AutoYaST are: Registered against RMT too Have all
     the updates applied
    </para>
   </sect3>
   <sect3>
    <title>Using a prebuilt image (eg: KVM, Xen)</title>
    <para>
     The node has to be registered against RMT. This should be done in the same
     way as a regular SLE machine: via SUSEConnect.
    </para>
   </sect3>
   <sect3>
    <title>Existing unregistered running node</title>
    <para>
     Again, they should be using SUSEConnect.
    </para>
   </sect3>
  </sect2>
 </sect1>
</chapter>
