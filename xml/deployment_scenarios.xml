<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.deployment.scenarios"
 xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Deployment Scenarios</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sec.deploy.scenarios.default">
  <title>Default Scenario</title>

  <para>
   In the default scenario &productname; is deployed in such a way, that its
   components have access (either direct or via proxy) to resources on the
   internet.
  </para>

  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="direct_connection.png" width="100%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="direct_connection.png" width="100%"/>
    </imageobject>
   </mediaobject>
  </informalfigure>
 </sect1>
 <sect1 xml:id="sec.deploy.scenarios.airgap">
  <title>Airgapped Installation</title>

  <para>
   An airgapped deployment can not have any direct connection to the Internet
   or external networks.
  </para>

  <para>
   All data must be transferred into the airgapped network in a secure fashion.
  </para>

<screen>
Current:
* Deploy RMT external, warm it
* Deploy RMT internal
* Transfer RMT cache to internal
* Deploy external container registry mirror
* Deploy internal container registry mirror
* Set up CaaSP on single node to get access to patched docker
* Put the patched docker daemon config on the caasp node to talk to external registry mirror
* Deploy internal helm chart repository mirror
* Manually warm helm chart repo mirror
* Manually define the images you need by trial and error (or magic)
* Manually warm the external container cache through patched docker with your list of images
* Transfer the registry cache to the internal mirror
* Deploy CaaSP cluster in internal network

Future:
* Deploy RMT external, warm it
* Deploy RMT internal
* Transfer RMT cache to internal
* Deploy external container registry mirror
* Deploy internal container registry mirror
* Deploy internal helm chart repository mirror
* Have cache warming tool that downloads the helm charts, extracts the images, downloads and caches the images on external registry mirror
* Yet undiscussed method to run helm chart mirror (update)
* Transfer the registry cache to the internal mirror
* Deploy CaaSP1 cluster in internal network
  </screen>

  <important>
   <title>Scope Of This Document</title>
   <para>
    This document focuses on providing mirrors for the resources provided by
    &suse; and required for basic &productname; functionality. If you require
    additional functionality, you can use these instructions as an example on
    how to provide additional mirrors.
   </para>
   <para>
    Providing a full set of mirroring instructions, for all usage scenarios, is
    beyond the scope of this document.
   </para>
  </important>

  <sect2 xml:id="sec.deploy.scenarios.airgap.concepts">
   <title>Concepts</title>
   <para>
    In order to disconnect &productname; from the external network, we must
    provide ways for the components to retrieve data from alternative sources
    inside the trusted network.
   </para>
   <para>
    The three main sources that must be replaced are:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &suse; &mos; RPM packages
     </para>
     <para>
      Provided by the &suse; package repositories
     </para>
    </listitem>
    <listitem>
     <para>
      Container images
     </para>
     <para>
      Provided by the &suse; container registry (https://registry.suse.com)
     </para>
    </listitem>
    <listitem>
     <para>
      Helm installation charts
     </para>
     <para>
      Provided by the &suse; helm chart repository
      (https://kubernetes-charts.suse.com/)
     </para>
    </listitem>
   </itemizedlist>
   <para>
    You will need to create mirror servers inside the airgapped network which
    act as the replacement for the default sources.
   </para>
   <para>
    These internal mirrors must be updated with data retrieved from the
    original upstream sources; in a trusted and secure fashion. To achieve
    this, you will need an additional set of mirroring servers outside of the
    airgapped network which act as first stage mirrors.
   </para>
   <para>
    Updating of mirrors happens in three stages.
   </para>
   <orderedlist>
    <listitem>
     <para>
      Update the external mirror from upstream.
     </para>
    </listitem>
    <listitem>
     <para>
      Transfer the updated data onto a trusted storage device.
     </para>
    </listitem>
    <listitem>
     <para>
      Update the internal mirror from the trusted storage device.
     </para>
    </listitem>
   </orderedlist>
   <informalfigure>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="airgap.png" width="100%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="airgap.png" width="100%"/>
     </imageobject>
    </mediaobject>
   </informalfigure>
   <variablelist>
    <varlistentry>
     <term>Upstream</term>
     <listitem>
      <para>
       Outside the controlled network.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>External</term>
     <listitem>
      <para>
       Inside the controlled network, outside the airgapped network.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Internal</term>
     <listitem>
      <para>
       Inside the airgapped network.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Once the replacement sources are in place, the key components are
    reconfigured to use the mirrors as their main sources.
   </para>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.machines">
   <title>Mirror Servers</title>
   <para>
    You will need to provide and maintain at least two machines in addition to
    your &productname; cluster. These mirror servers will reside on the
    external part of your network and the internal (airgapped) network
    respectively.
   </para>
   <note>
    <title>Shared Mirror Server</title>
    <para>
     You can have multiple mirror services share a single machine. The
     specifications mentioned here are only basic minimum requirements.
    </para>
    <para>
     If you have multiple clusters or a very large amount of nodes accessing
     these mirrors, you should increase CPU/RAM.
    </para>
    <para>
     Storage sizing depends on your intended update frequency and data
     retention model. If you want to keep snapshots or images of repository
     states at various points, you must increase storage size accordingly.
    </para>
   </note>
   <variablelist>
    <varlistentry>
     <term>External</term>
     <listitem>
      <para>
       This machine will host the <literal>&rmtool;</literal> for RPM packages
       and the <literal>Docker Pull-Trough Cache</literal> for container
       images.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>1</literal> Host machine for the mirror servers.
        </para>
        <itemizedlist>
         <listitem>
          <para>
           SLES 15
          </para>
         </listitem>
         <listitem>
          <para>
           2 (v)CPU
          </para>
         </listitem>
         <listitem>
          <para>
           8 GB RAM
          </para>
         </listitem>
         <listitem>
          <para>
           500 GB Storage
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Internal</term>
     <listitem>
      <para>
       This machine will host the <literal>&rmtool;</literal> for RPM packages,
       the <literal>Docker Container Registry</literal> for container images
       and a webserver hosting the <literal>Helm Chart Repository</literal>
       files.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>1</literal> Host machine for the mirror servers.
        </para>
        <itemizedlist>
         <listitem>
          <para>
           SLES 15
          </para>
         </listitem>
         <listitem>
          <para>
           2 (v)CPU
          </para>
         </listitem>
         <listitem>
          <para>
           8 GB RAM
          </para>
         </listitem>
         <listitem>
          <para>
           500 GB Storage
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
   </variablelist>
   <important>
    <title>Adjust Number Of Mirror Servers</title>
    <para>
     This scenario description does not contain any fallback contingencies for
     the mirror servers. Add additional mirror servers behind a loadbalancer if
     you require additional reliability/availability.
    </para>
   </important>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.network">
   <title>Networking</title>
   <para>
    All members of the &productname; cluster must be able to communicate with
    the internal mirror server(s) within the airgapped network.
   </para>
   <para>
    Ports:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      80 HTTP - RMT Server and Helm Chart Repository Mirror
     </para>
    </listitem>
    <listitem>
     <para>
      443 HTTPS - RMT Server and Helm Chart Repository Mirror
     </para>
    </listitem>
    <listitem>
     <para>
      5000 HTTP - Docker Registry
     </para>
    </listitem>
   </itemizedlist>
   <note>
    <title>Additional Port Configuration</title>
    <para>
     If you choose to add more Docker registries to your internal network,
     these must run on different ports than the standard
     <literal>5000</literal>.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.storage">
   <title>Trusted Storage</title>
   <para>
    Transferring data from the external network mirror to the internal mirror
    can be performed in many ways. The most common way is portable storage (USB
    keys or external hard drives).
   </para>
   <para>
    Sizing of the storage is dependent on the number of data sources that need
    to be stored. Container images can easily measure several Gigabytes per
    item; although they are generally smaller for Kubernetes related
    applications. The overall size of any given RPM repository is at least tens
    of Gigabytes.
   </para>
   <para>
    At the time of writing, the package repository for &sls; alone contains
    <literal>36GB</literal> of data.
   </para>
   <para>
    We recommend external storage with at least <literal>128GB</literal>.
   </para>
   <note>
    <title>Handling Of Trusted Storage</title>
    <para>
     Data integrity checks, duplication, backup, and secure handling procedures
     of trusted storage are beyond the scope of this document.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.rpm-repository">
   <title>RPM Repository Mirror</title>
   <para>
    Providing RPM packages is handled by the
    <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/book_rmt.html">&rmtool;
    (RMT)</link>. RMT will provide a repository server that holds the packages
    and related metadata for &mos;; to install them like from the upstream
    repository. Data is synchronized once a day to the external mirror
    automatically or can be forced via the CLI. You can then copy this data to
    your trusted storage at any point and update the internal mirror.
   </para>
   <sect3>
    <title>Mirror Configuration</title>
    <note>
     <title>If Possible, Deploy The Mirror Before &productname; Cluster Deployment</title>
     <para>
      The mirror on the internal network should be running and populated before
      deploying &productname;, this makes rolling out much easier; since you
      can configure the nodes during installation to use the correct internal
      mirror.
     </para>
     <para>
      If the cluster is deployed before the mirror is running, you must
      reconfigure all nodes manually.
     </para>
    </note>
    <para>
     Set up two
     <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/cha_rmt_installation.html">RMT
     mirrors</link>. One in the external network and one in the airgapped
     network.
    </para>
   </sect3>
   <sect3>
    <title>Client Configuration</title>
    <para>
     Configure all &productname; nodes to
     <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/sec_rmt_client_clientsetupscript.html">get
     updates from the internal RMT server</link>.
    </para>
   </sect3>
   <sect3>
    <title>Updating The Mirror</title>
    <para>
     Follow the procedure in
     <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/sec_rmt_mirroring_export_import.html" />.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.container-registry">
   <title>Container Registry Mirror</title>
   <para>
    Container image registries are provided by SUSE, Docker, and other sources.
    The &suse; container registry is used to update the &productname;
    components.
   </para>
   <para>
    Once configured, nothing has to be changed inside of Dockerfile(s),
    Kubernetes manifest files, Helm charts or custom scripts, etc. All images
    with a prefix <literal>registry.suse.com/</literal> will be automatically
    pulled from the internal mirror.
   </para>
   <para>
    Images that are located on other registries must be mirrored separately on
    the airgapped network.
   </para>

   <sect3 xml:id="sec.deploy.scenarios.airgap.container-registry.configuration">
    <title>Mirror Configuration</title>
    <important>
     <title>This example does not provide any security measures</title>
     <para>
      The default configuration is not using certificates to secure the
      communication between this registry and all the nodes communicating with
      it. There is also no authentication and authorization in place.
     </para>
     <para>
      For production usage, we highly recommend deploying the registry using
      TLS certificates and implementing authentication and authorization by
      deploying a Portus instance.
     </para>
     <para>
      In addition: access to the container image registries is not restricted
      in any way. If you need to secure the registry specifically please refer
      to:
      <link xlink:href="https://docs.docker.com/registry/deploying/#restricting-access">Docker
      Registry: Restricting Access</link>.
     </para>
    </important>

    <para>
     The external and internal mirrors have slightly different configurations.
    </para>
    <para>
     The external mirror is configured and acts as a
     <link xlink:href="https://docs.docker.com/registry/recipes/mirror/">"pull
     through"-cache</link>. It connects to the upstream registry and any attempt
     to pull an image from this registry is routed through this mirror. All
     images pulled through this mirror will be stored in the cache. This cache
     is the the data you need to transfer to the internal mirror.
    </para>
    <para>
     The internal mirror acts as a container registry mirror that provides the
     images to the &productname; cluster nodes. You can host multiple registries
     on one internal mirror but they need to run on different ports. Each
     respective container registry mirror must be available to the &productname;
     cluster nodes at the configured port (Default: <literal>5000</literal>).
    </para>
    <para>
     Nothing has to be changed inside of Dockerfile(s), Kubernetes manifest
     files, Helm charts, custom scripts, etc. All images using a prefix that
     was configured as a remote registry/mirror (for example <literal>registry.suse.com/</literal>)
     will be automatically pulled from the air-gapped mirror(s).
     </para>
     <note>
      <title>Registry Mirrors Are Read Only</title>
      <para>
       For security reasons, both the registry mirrors are configured in
       <literal>read-only</literal> mode. Therefore, pushing container images to
       these mirrors will not be possible. They can only serve images that were
       previously pulled and cached by the external mirror and then uploaded to
       the internal mirror.
      </para>
      <para>
       If you need the ability to store modified container images, you will have
       to create a new registry that will hold these images. The steps needed
       to run your own full container image registry are not part of this
       document.
      </para>
       <para>
       For more information you can refer to:
       <link xlink:href="https://docs.docker.com/registry/">Docker
       Registry</link>.
      </para>
     </note>

    <procedure>
     <title>Set Up The External Mirror</title>
     <step>
      <para>
       <link xlink:href="https://www.suse.com/documentation/sles-15/book_quickstarts/data/sec_sle_installquick.html">Set
       up a &sls; 15 machine</link> on the external network.
      </para>
      <para>
       Make sure you have
       <link xlink:href="https://www.suse.com/documentation/sles-15/book_quickstarts/data/sec_modules_installing.html">enabled
       the <filename>containers</filename> module</link>.
      </para>
     </step>
     <step>
      <para>
       Install the <filename>docker-distribution-registry</filename> package:
      </para>
<screen>&prompt.sudo;<command>zypper in docker-distribution-registry</command>
</screen>
     </step>
     <step>
      <para>
       The registry package provides a default configuration file:
       <filename>/etc/registry/config.yml</filename>. Fill in the necessary
       configuration details. For example: Add the <literal>remoteurl</literal>
       of the registry you wish to connect to.
      </para>
<screen>
proxy:
  remoteurl: https://registry.suse.de
      </screen>
     </step>
     <step>
      <para>
       In <filename>/etc/registry/config.yml</filename>, identify the
       <literal>storage</literal> section and add the following lines:
      </para>
<screen>
maintenance:
  readonly:
    enabled: true
       </screen>
      <para>
       These lines will make the registry read-only, preventing its images from
       being overwritten by the clients. The complete file should look like:
      </para>
<screen>
version: 0.1
log:
  level: info
storage:
  filesystem:
    rootdirectory: /var/lib/docker-registry
  maintenance:
    readonly:
      enabled: true
http:
  addr: 0.0.0.0:5000
proxy:
  remoteurl: https://registry.suse.de
          </screen>
      <para>
       For more details on the configuration, refer to:
       <link xlink:href="https://docs.docker.com/registry/configuration/">Docker
       Registry: Configuration</link>
      </para>
     </step>
     <step>
      <para>
       Now start the registry service and enable it at boot time:
      </para>
<screen>
&prompt.sudo;<command>systemctl enable --now registry.service</command>
         </screen>
     </step>
    </procedure>
    <procedure>
     <title>Set Up Internal Mirror</title>
     <step>
      <para>
       <link xlink:href="https://www.suse.com/documentation/sles-15/book_quickstarts/data/sec_sle_installquick.html">Set
       up a &sls; 15 machine</link> on the airgapped network.
      </para>
      <para>
       Make sure you have
       <link xlink:href="https://www.suse.com/documentation/sles-15/book_quickstarts/data/sec_modules_installing.html">enabled
       the <filename>containers</filename> module</link>.
      </para>
     </step>
     <step>
      <para>
       Install the <filename>docker-distribution-registry</filename> package:
      </para>
<screen>&prompt.sudo;<command>zypper in docker-distribution-registry</command>
</screen>
     </step>
     <step>
      <para>
       In the default configuration file
       <filename>/etc/registry/config.yml</filename>, identify the
       <literal>storage</literal> section and add the following lines:
      </para>
<screen>
maintenance:
  readonly:
    enabled: true
       </screen>
      <para>
       For more details on the configuration, refer to:
       <link xlink:href="https://docs.docker.com/registry/configuration/">Docker
       Registry: Configuration</link>
      </para>
      <para>
       These lines will make the registry read-only, preventing its images from
       being overwritten by the clients. The complete file should look like:
      </para>
<screen>
version: 0.1
log:
  level: info
storage:
  filesystem:
    rootdirectory: /var/lib/docker-registry
  maintenance:
    readonly:
      enabled: true
http:
  addr: 0.0.0.0:5000
         </screen>
      <important>
       <title>Do Not Configure The <literal>proxy</literal> Section For The Internal Mirror</title>
       <para>
        Do not add the “proxy” section like you must do on the external
        on-premise mirror. This is not needed and would cause the registry
        service to refuse to start since the airgapped network can not reach
        the specified proxy.
       </para>
      </important>
     </step>
     <step>
      <para>
       Now start the registry service and enable it at boot time:
      </para>
<screen>
&prompt.sudo;<command>systemctl enable --now registry.service</command>
         </screen>
     </step>
    </procedure>
    <para>
     Now you should have the registries set up and listening on port
     <literal>5000</literal>.
    </para>
<!-- FIXME mnapp 18.9.18 Shouldn't we explain how to provide this default auth/security? -->
   </sect3>

   <sect3 xml:id="sec.deploy.scenarios.airgap.container-registry.clients">
    <title>Client Configuration</title>
    <para>
     Using the air-gapped mirror works exactly like using a traditional
     on-premise mirror. You must configure the internal registry mirror in
     &dashboard;.
    </para>
    <procedure>
     <title>Configuring Registry Mirror</title>
     <step>
      <para>
       Log in to &dashboard; on the &productname; admin node.
      </para>
     </step>
     <step>
      <para>
       Navigate to <guimenu>Settings &rarr; Mirrors</guimenu> and create a
       definition for your internal registry mirror as described in
       <xref linkend="sec.admin.velum.mirror"/>.
      </para>
     </step>
    </procedure>
   </sect3>

   <sect3 xml:id="sec.deploy.scenarios.airgap.container-registry.update">
    <title>Updating The Mirror</title>
    <note>
     <title>Live Update Of Registry</title>
     <para>
      There is no need to stop the registry services while doing the backup and
      restore procedures.
     </para>
    </note>
    <procedure>
     <title>Update Container Registry Mirror</title>
     <step>
      <para>
       Pull all the desired images
      </para>
     </step>
     <step>
      <para>
       Attach your trusted storage to the external mirror as <filename>/mnt/usbdisk</filename>
       and copy all the contents of <filename>/var/lib/docker-registry</filename>
       onto the drive.
      </para>
<screen>&prompt.sudo;<command>rsync -aP --delete /var/lib/docker-registry <replaceable>/mnt/usbdisk</replaceable></command>
        </screen>
      </step>
      <step>
       <para>
        Transport the trusted storage to the airgapped network and attach it
        to the internal container registry mirror in the same way.
        </para>
        </step>
        <step>
         <para>
          Copy the data from the trusted storage onto the internal mirror.
          </para>
<screen>&prompt.sudo;<command>rsync -aP --delete <replaceable>/mnt/usbdisk</replaceable>/docker-registry/ /var/lib/docker-registry/</command>
                  </screen>
          </step>
    </procedure>
<screen>
     Container images:

     . Update external container registry mirrors from upstream
     . Copy registry data onto data storage
     . Connect storage to internal instance and upload data
     </screen>
<screen>
     Synchronizing contents
     This procedure illustrates how to synchronize the air-gapped registry with the traditional on-premise one.

         1. Attach an external hard drive to “mirror.local.lan” and copy all the contents of “/var/lib/docker-registry” to it:

     sudo rsync -aP --delete /var/lib/docker-registry /mnt/usbdisk

         2. Attach the external hard drive to “mirror.secure.lan” and import the contents into the registry:

     sudo rsync -aP --delete /mnt/usbdisk/docker-registry/ /var/lib/docker-registry/

     Additional security steps can be introduced between step #1 and #2; for example scanning the exported data using corporate tools before introducing them inside of the air-gapped network.

     Using the mirror



         </screen>
    <note>
     <title>Configuring Additional External Registries</title>
     <para>
      Configure &kube; to use this internal registry. <guimenu>Velum &rarr;
      Settings &rarr; Remote Registries</guimenu>.
     </para>
     <para>
      Configure &productname; to rewrite external registry requests to that
      mirror <guimenu>Velum &rarr; Settings &rarr; Mirrors</guimenu>.
     </para>
    </note>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.helm-charts">
   <title>Helm Chart Repository Mirror</title>
   <para>
    Helm Charts provide a simple way to deploy entire application stacks
    including all their dependencies and configurations. The repository consists
    of chart packages that contain the necessary information and an index file.
   </para>
   <sect3>
    <title>Mirror Configuration</title>
    <para>
     Set up an internal Helm chart mirror. This can be any webserver capable of
     hosting static files.
    </para>
    <para>
     You do not need a Helm chart repository mirror on the external
     network.
    </para>
   </sect3>
   <sect3>
    <title>Client Configuration</title>
    <para>
     Add the webserver as a repo to <command>helm</command> on the admin node.
     </para>
<screen>&prompt.user;<command>helm repo add SUSE-MIRROR <replaceable>&lt;internal-mirror.hostname&gt;</replaceable></command>
   </screen>
   </sect3>
   <sect3 xml:id="sec.deploy.scenarios.airgap.helm-charts.update">
    <title>Updating The Mirror</title>
    <para>
     To update the repository, you simply download the helm chart repository
     data from the upstream source and transfer it to your trusted storage.
     Then you transfer the data onto the webserver in your airgapped network.
    </para>
    <procedure>
     <title>Download Helm Charts From Suse Repository</title>
     <step>
      <para>
       On the external network, log in to your external mirror or any Linux
       workstation with internet access.
      </para>
     </step>
     <step>
      <para>
       Move to your desired working directory and create a new sub-directory
       <filename>suse-charts</filename>. Enter this directory.
      </para>
<screen>
&prompt.user;<command>mkdir suse-charts</command>
&prompt.user;<command>cd suse-charts</command>
     </screen>
     </step>
     <step>
      <para>
       Download the <filename>index.yaml</filename> file from the &suse; Helm
       Chart Repository. Then use the file to download the Helm charts.
      </para>
<screen>
&prompt.user;<command>curl -O https://kubernetes-charts.suse.com/index.yaml</command>
&prompt.user;<command>grep 'https' index.yaml | awk {'print $2'} | xargs -d "\n" -i curl -O {}</command>
    </screen>
     </step>
     <step>
      <para>
       Package the index file and the charts in an archive.
      </para>
<screen>
&prompt.user;<command>tar czvf suse-charts-`date +%F`.tar.gz *</command>
    </screen>
     </step>
     <step>
      <para>
       Transfer <filename>suse-charts-*</filename> to the webserver and unpack
       it to the desired location. The <filename>index.yaml</filename> file
       must be in the root directory for your webserver.
      </para>
     </step>
     <step>
      <para>
       Update the repository information on the &productname; admin node.
      </para>
<screen>&prompt.user;<command>helm repo update</command>
       </screen>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.caasp-deployment">
   <title>Airgapped &productname; Deployment</title>
   <sect3>
    <title>Using the ISO</title>
    <para>
     From YaST register the node against the RMT server. This will ensure the
     final node zypper repositories are pointed against RMT, moreover all the
     available updates are going to be installed (-> no need to run a
     transactional-update right after the installation -> way faster).
    </para>
   </sect3>
   <sect3>
    <title>Using AutoYast</title>
    <para>
     Ensure the admin node is registered against RMT, that will ensure the
     nodes that are provisioned by AutoYaST are registered against RMT to have all
     the updates applied
    </para>
   </sect3>
   <sect3>
    <title>Using a prebuilt image (eg: KVM, Xen)</title>
    <para>
     The node has to be registered against RMT. This should be done in the same
     way as a regular SLE machine: via SUSEConnect.
    </para>
   </sect3>
   <sect3>
    <title>Existing unregistered running node</title>
    <para>
     Again, they should be using SUSEConnect.
    </para>
   </sect3>
  </sect2>
 </sect1>
</chapter>
