[[_administration]]
= Cluster Administration
:doctype: book
:sectnums:
:toc: left
:icons: font
:experimental:
:sourcedir: .
:imagesdir: ./images
= Cluster Administration
:doctype: book
:sectnums:
:toc: left
:icons: font
:experimental:
:imagesdir: ./images

[[_auth]]
== Authentication and Authorization


Role-based access control (RBAC) adds the ability to perform authentication and authorization of activities performed against a {kube}
cluster.
Authentication is concerned with the "`who`"
 and authorization is concerned with the "`what`"
. 

[[_auth.kubeconfig]]
=== Authentication


Starting in {productname}
2, {kubectl}
needs to authenticate against the {kube}{master_node}
.
The necessary authentication information is included in the {kubeconfig}
file available from {dashboard}
.
Click the `kubectl config` button and authenticate with your user name and password.
Download the file from {dashboard}
 and save it as [path]``$HOME/.kube/config``
. 

.The KUBECONFIG variable
[TIP]
====
{kubectl}
uses an environment variable named [var]``KUBECONFIG`` to locate your {kubeconfig}
 file.
If this variable is not specified, it defaults to [path]``$HOME/.kube/config``
.
To use a different location, run 

----
{prompt.user}``export`` _KUBECONFIG=/path/to/kube/config/file_ 
----
====

.Obtaining the root CA certificate
[NOTE]
====
You can obtain the root CA certificate from any node in your cluster via SCP: 

----
{prompt.user}``scp`` _NODE:/etc/pki/trust/anchors/SUSE_CaaSP_CA.crt ._ 
----

To trust this root CA certificate on your machine, place it in [path]``/etc/pki/trust/anchors/``
 and call the [command]``update-ca-certificates`` script. 
====

[[_auth.users]]
=== Managing Users and Groups


User information is stored in [productname]##OpenLDAP##
 running in a container on your {productname}{admin_node}
.
You can use standard LDAP administration tools for managing these users remotely.
To do so, install the [package]#openldap2#
 package on a computer in your network and make sure that computer can connect to the {admin_node}
 on port 389.
For further information, refer to <<_sec.caasp.installquick.netreqs>>. 

[[_auth.user.ldap_rootpw]]
==== Obtaining the OpenLDAP Password


Before performing any administrative tasks on the [productname]##OpenLDAP##
 instance, you will need to retrieve your [productname]##OpenLDAP##
 administrator account password.
To do this, run: 

----
{prompt.user}``ssh`` _root@caasp-admin.example.com_ \``cat`` _/var/lib/misc/infra-secrets/openldap-password_ 
----


Make sure to replace [replaceable]``caasp-admin.example.com`` with the FQDN or IP of your {admin_node}
. 

[[_auth.users.import_cert]]
==== Importing the LDAP Certificate Locally


To be able to perform any LDAP queries, you first need to import the generated LDAP certificate to your local trusted certificate storage.
On the {admin_node}
, run: 

----
{prompt.root}``docker`` exec -it $(docker ps | grep openldap | awk '{print $1}') cat /etc/openldap/pki/ca.crt > ~/ca.pem
----

----
{prompt.root}``scp`` ~/ca.pem`root@workstation`:/usr/share/pki/trust/anchors/ca-caasp.crt.pem
----


Replace [replaceable]``root@workstation`` with the appropriate username and hostname for the workstation where you wish to run the LDAP queries.
 

Then, on that workstation, run: 

----
{prompt.root}``update-ca-certificates`` 
----

[[_auth.users.add]]
==== Adding New Users


By default, when you create the first user in {dashboard}
during bootstrap of your cluster, that user is granted `Cluster Administrator` privileges within {kube}
.
You can add additional users with these rights by adding new entries into the LDAP directory. 

To add a new user, create a LDIF file like this: 

----
dn: uid=`userid` <1>,ou=People,dc=infra,dc=caasp,dc=local
objectClass: person
objectClass: inetOrgPerson
objectClass: top
uid:`userid`<<_co.auth.users.add.uid>>userPassword:`password hash` <2>givenname:`first name` <3>sn:`surname` <4>cn:`full name` <5>mail:`email address` <6>
----


Make sure to replace all the parameters indicated [replaceable]``like
      this`` in the template above as follows: 
<1>
       User ID (UID) of the new user. Needs to be unique.
      
<2>
       The user's hashed password. Use [command]``/usr/sbin/slappasswd``

       to generate the hash.
      
<3>
       The user's first name
      
<4>
       The user's last name
      
<5>
       The user's full name
      
<6>The user's e-mail address. It is used as the login name to
       {dashboard}
 and {kube}
.


Populate your OpenLDAP server with this LDIF file: 

----
{prompt.root}``ldapadd`` -H ldap://`ADMIN NODE IP`;:389 -ZZ -D cn=admin,dc=infra,dc=caasp,dc=local -w`LDAP ADMIN PASSWORD`-f`LDIF FILE`
----


To add this new user to the existing Administrators group, create a new LDIF file like this: 

----
dn: cn=Administrators,ou=Groups,dc=infra,dc=caasp,dc=local
changetype: modify
add: uniqueMember
uniqueMember: uid=`userid` <7>,ou=People,dc=infra,dc=caasp,dc=local
----


Make sure to replace all the parameters indicated [replaceable]``like
      this`` in the template above as follows: 
<7>The user ID (UID) of the user


Populate your [productname]##OpenLDAP##
 server with the LDIF file: 

----
{prompt.root}``ldapadd`` -H ldap://`ADMIN NODE IP`:389 -ZZ -D cn=admin,dc=infra,dc=caasp,dc=local -w`LDAP ADMIN PASSWORD`-f`LDIF FILE`
----

[[_auth.users.change_pw]]
==== Changing a User's Password


To change a user's password, create a LDIF file like this: 

----
dn: uid=`userid` <8>,ou=People,dc=infra,dc=caasp,dc=local
changetype: modify
modify: userPassword
userPassword:`password hash` <9>
----


Make sure to replace all the parameters indicated [replaceable]``like
      this`` in the template above as follows: 
<8>
       User ID (UID) of the user.
      
<9>
       The user's new hashed password. Use
       [command]``/usr/sbin/slappasswd``
 to generate the hash.
      


Populate your OpenLDAP server with this LDIF file: 

----
{prompt.root}``ldapadd`` -H ldap://`ADMIN NODE IP`;:389 -ZZ -D cn=admin,dc=infra,dc=caasp,dc=local -w`LDAP ADMIN PASSWORD`-f`LDIF FILE`
----

[[_auth.group]]
==== Adding New Groups


Say you have users that you want to grant access to manage a single namespace in Kubernetes.
To do this, first create your users as mentioned in <<_auth.users.add>>.
Then create a new group: 

----
dn: cn=`group name` <10>,ou=Groups,dc=infra,dc=caasp,dc=local
objectclass: top
objectclass: groupOfUniqueNames
cn:`group name`<<_co.auth.group.cn>>uniqueMember: uid=`member1`, <11>ou=People,dc=infra,dc=caasp,dc=local
uniqueMember: uid=`member2`,<<_co.auth.group.member>>ou=People,dc=infra,dc=caasp,dc=local
uniqueMember: uid=`member3`,<<_co.auth.group.member>>ou=People,dc=infra,dc=caasp,dc=local
----


Make sure to replace all the parameters indicated [replaceable]``like
      this`` in the template above as follows: 
<10>
       The group's name.
      
<11>
       Members of the group. Repeat the `uniqueMember`

       attribute for every member of this group.
      


Populate your [productname]##OpenLDAP##
 server with the LDIF file: 

----
{prompt.root}``ldapadd`` -H ldap://`ADMIN NODE IP`:389 -ZZ -D cn=admin,dc=infra,dc=caasp,dc=local -w`LDAP ADMIN PASSWORD`-f`LDIF FILE`
----


Next, create a role binding to allow this new LDAP group access in {kube}
.
Create a Kubernetes deployment descriptor like this: 

----
# Define a Role and its permissions in{kube}kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name:`role name` <12>namespace:`applicable namespace` <13># This set of rules amounts to "allow all"
rules:
- apiGroups: [""]
  resources: [""]
  resourceNames: [""]
  verbs: [""]
---
# Map an LDAP group to this{kube}role
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name:`role binding name` <14>namespace:`applicable namespace`<<_co.auth.group.role_namespace>>subjects:
- kind: Group
  name:`LDAP group name` <15>apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name:`role name`<<_co.auth.group.role_name>>apiGroup: rbac.authorization.k8s.io
----
<12>
       Name of the new role in {kube}

      
<13>
       Namespace the new group should be allowed to access. Use
        `
default`
 for {kube}
' default namespace.
      
<14>
       Name of the role binding in {kube}

      
<15>
       Name of the corresponding group in LDAP
      


Add this role and binding to {kube}
: 

----
{prompt.root}``kubectl`` _apply -f DEPLOYMENT DESCRIPTOR FILE_ 
----

==== Further information


For more details on authorization in {kube}
, refer to https://kubernetes.io/docs/admin/authorization/rbac/

[[_transactional.updates]]
== Handling Transactional Updates


For security and stability reasons, the operating system and application should always be up-to-date.
While with a single machine you can keep the system up-to-date quite easily by running several commands, in a large-scaled cluster the update process can become a real burden.
Thus transactional automatic updates have been introduced.
Transactional updates can be characterized as follows: 

* They are atomic. 
* They do not influence the running system. 
* They can be rolled back. 
* The system needs to be rebooted to activate the changes. 


Transactional updates are managed by the [command]``transactional-update`` script, which is called once a day.
The script checks if any update is available.
If there is an update to be applied, a new snapshot of the root file system is created and the system is updated by using [command]``zypper dup``.
All updates released to this point are applied.
The snapshot is then marked as active and will be used after the next reboot of the system.
Ensure that the cluster is rebooted as soon as possible after the update installation is complete, otherwise all changes will be lost. 

.General Notes to the Updates Installation
[NOTE]
====
Only packages that are part of the snapshot of the root file system can be updated.
If packages contain files that are not part of the snapshot, the update could fail or break the system. 

RPMs that require a license to be accepted cannot be updated. 
====

[[_transactional.updates.installation]]
=== Manual Installation of Updates


After the [command]``transactional-update`` script has run on all nodes, {dashboard}
 displays any nodes in your cluster running outdated software. {dashboard}
 then enables you to update your cluster directly.
Follow the next procedure to update your cluster. 

.Procedure: Updating the Cluster with {dashboard}
. Login to {dashboard} . 
. If required, click menu:UPDATE ADMIN NODE[] to start the update. 
+


image::velum_updating.png[]
. Confirm the update by clicking menu:Reboot to update[] . 
+


image::velum_reboot_and_update.png[]
. Now you have to wait until the {admin_node} reboots and {dashboard} is available again. 
. Click menu:update all nodes[] to update {master_node} and {worker_node} s. 
+


image::velum_update_nodes.png[]


[[_transactional.updates.disabling]]
=== Disabling Transactional Updates


Even though it is not recommended, you can disable transactional updates by issuing the command: 

----
``systemctl`` _--now disable transactional-update.timer_ 
----

[[_ptf.handling]]
== Handling Program Temporary Fixes


Program temporary fixes (PTFs) are available in the {productname}
environment.
You install them by using the [command]``transactional-update`` script.
Typically you invoke the installation of PTFs by running: 

----
``transactional-update`` _reboot ptf install rpm … rpm_ 
----


The command installs PTF RPMs.
The `reboot` option then schedules a reboot after the installation.
PTFs are activate only after rebooting of your system. 

.Reboot Required
[NOTE]
====
If you install or remove PTFs and you call the [command]``transactional-update`` to update the system before reboot, the applied changes by PTFs are lost and need to be done again after reboot. 
====


In case you need to remove the installed PTFs, use the following command: 

----
``transactional-update`` _reboot ptf remove rpm … rpm_ 
----

[[_commands.node.managment]]
== Commands for Single Node Management

{productname}
comes with several built-in commands that enable you to manage your {cluster_node}
s. 

[[_commands.node.managment.issue_generator]]
=== The [command]``issue-generator`` Command


The [command]``issue-generator`` creates a volatile temporary [path]``/run/issue``
 file.
The file [path]``/etc/issue``
 should be a symbolic link to the temporary [path]``/run/issue``
. 

You can use the command to prefix all directories and files with a specified prefix (path in this case): 

----
issue-generator --prefix`path`
----


By using the command you can also create or delete files in the network configuration, for example: 

----
issue-generator network`remove``interface`
----


The command removes file [path]``/run/issue.d/70-interface.conf``
.
The file contains the name of the [replaceable]``interface`` and escape codes for [command]``agentty``. 

You can use the command to add or delete [path]``/run/issue.d/60-ssh_host_keys.conf``
 that contains fingerprints of the public SSH keys of the host: 

----
issue-generator ssh`add|remove`
----

.The Command without Arguments
[NOTE]
====
If you run the command without any argument, all input files will be applied. 
====

[[_commands.node.managment.transactional_update]]
=== The [command]``transactional-update`` Command


The [command]``transactional-update`` enables you to install or remove updates of your system in an atomic way.
The updates are applied all or none of them if any package cannot be installed.
Before the update is applied, a snapshot of the system is created in order to restore the previous state in case of a failure. 

If the current root file system is identical to the active root file system (after applying updates and reboot), run cleanup of all old snapshots: 

----
transactional-update cleanup
----


Other options of the command are the following: 

`up`::
If there are new updates available, a new snapshot is created and [command]``zypper dup`` is used to update the snapshot.
The snapshot is activated afterwards and is used as the new root file system after reboot. 
+

----
transactional-update up
----
`dup`::
If there are new updates available, a new snapshot is created and [command]``zypper dup –no-allow-vendor-change`` is used to update the snapshot.
The snapshot is activated afterwards and is used as the new root file system after reboot. 
+

----
transactional-update dup
----
`patch`::
If there are new updates available, a new snapshot is created and [command]``zypper patch`` is used to update the snapshot.
The snapshot is activated afterwards and is used as the new root file system after reboot. 
+

----
transactional-update patch
----
`ptf install`::
The command installs the specified RPMs: 
+

----
transactional-update ptf install`rpm ... rpm`
----
`ptf remove`::
The command removes the specified RPMs from the system: 
+

----
transactional-update ptf remove`rpm ... rpm`
----
`rollback`::
The command sets the default sub volume.
On systems with read-write file system [command]``snapper rollback`` is called.
On a read-only file system and without any argument, the current system is set to a new default root file system.
If you specify a number, that snapshot is used as the default root file system.
On a read-only file system, no additional snapshots are created. 
+

----
transactional-update rollback`snapshot_number`
----
`--help`::
The option outputs possible options and subcommands. 
+

----
transactional-update --help
----

[[_commands.node.managment.create_autoyast_profile]]
=== The [command]``create_autoyast_profile`` Command


The [command]``create_autoyast_profile`` command creates an autoyast profile for fully automatic installation of {productname}
.
You can use the following options when invoking the command: 

`-o|--output`::
Specify to which file the command should save the created profile. 
+

----
create_autoyast_profile -o`filename`
----
`--salt-master`::
Specify the host name of the {smaster}
. 
+

----
create_autoyast_profile --salt-master`saltmaster`
----
`--smt-url`::
Specify the URL of the SMT server. 
+

----
create_autoyast_profile --smt-url`saltmaster`
----
`--regcode`::
Specify the registration code for {productname}
. 
+

----
create_autoyast_profile --regcode`405XAbs593`
----
`--reg-email`::
Specify an e-mail address for registration. 
+

----
create_autoyast_profile --reg-email`address@exampl.com`
----

[[_node.managment]]
== Node Management


After you complete the deployment and you bootstrap the cluster, you may need to perform additional changes to the cluster.
By using {dashboard}
you can add additional nodes to the cluster.
You can also delete some nodes, but in that case make sure that you do not break the cluster. 

[[_node.managment.adding]]
=== Adding Nodes


You may need to add additional {worker_node}
s to your cluster.
The following steps guides you through that procedure: 

.Procedure: Adding Nodes to Existing Cluster
. Prepare the node as described in <<_sec.caasp.installquick.node>>
. Open {dashboard} in your browser and login. 
. You should see the newly added node as a node to be accepted in menu:Pending Nodes[] . Accept the node. 
+


image::velum_pending_nodes.png[]
. In the menu:Summary[] you can see the menu:New[] that appears next to menu:New nodes[] . Click the menu:New[] button. 
+


image::velum_unassigned_nodes.png[scaledwidth=60%]
. Select the node to be added and click menu:Add nodes[] . 
. The node has been added to your cluster. 


[[_node.managment.removing]]
=== Removing Nodes


As each node in the cluster runs also an instance of ``etcd``, {productname}
 has to ensure that removing of several nodes does not break the `etcd` cluster.
In case you have for example three nodes in the `etcd` and you delete two of them, {productname}
 deletes one node, recovers the cluster and only if the recovery is successful, the next node can be removed.
If a node runs just an ``etcd-proxy``, there is nothing special that has to be done, as deleting any amount of `etcd-proxy` can not break the `etcd` cluster. 

[[_cluster.monitoring]]
== Cluster Monitoring


There are three basic ways how you can monitor your cluster: 

* by directly accessing the _cAdvisor_ on ``http://[replaceable]``WORKER NODE ADDRESS``;:4194/containers/``. The _cAdvisor_ runs on worker nodes by default. 
* By using __Heapster__, for details refer to <<_cluster.monitoring.heapster>>. 
* By using __Grafana__, for details refer to <<_cluster.monitoring.grafana>>. 


[[_cluster.monitoring.heapster]]
=== Monitoring with Heapster

_Heapster_ is a tool that collects and interprets various signals from your cluster. _Heapster_ communicates directly with the __cAdvisor__.
The signals from the cluster are then exported using REST endpoints. 

To deploy __Heapster__, run the following command: 

----
kubectl apply -f https://raw.githubusercontent.com/SUSE/caasp-services/master/contrib/addons/heapster/heapster.yaml
----

_Heapster_ can store data in __InfluxDB__, which can be then used by other tools. 

[[_cluster.monitoring.grafana]]
=== Monitoring with Grafana

_Grafana_ is an analytics platform that processes data stored in _InfluxDB_ and displays the data graphically.
You can deploy _Grafana_ by running the following commands: 

----
kubectl apply -f https://raw.githubusercontent.com/SUSE/caasp-services/master/contrib/addons/heapster/heapster.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/release-1.3/deploy/kube-config/influxdb/grafana-deployment.yaml
wget https://raw.githubusercontent.com/kubernetes/heapster/release-1.3/deploy/kube-config/influxdb/grafana-service.yaml
----


Then open the file [path]``grafana-service.yaml``
: 

----
vi grafana-service.yaml
----


In the file uncomment the line with the `NodePort` type. 

To finish the _Grafana_ installation, apply the configuration by running: 

----
kubectl apply -f grafana-service.yaml
----