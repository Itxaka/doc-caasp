== Deployment on Amazon AWS

.Preparation Required
[NOTE]
You must have completed <<deployment.preparations>> to proceed.

You will use {tf} to deploy the whole infrastructure described in
<<architecture.aws>>. Then you will use the `skuba` tool to bootstrap the
{kube} cluster on top of those.

The {tf} link:https://www.terraform.io/docs/providers/aws/index.html[AWS provider]
requires your credentials. These can be obtained by following these steps:

* Log in to the AWS console.
* Click on your username in the upper right hand corner to reveal the drop-down menu.
* Click on My Security Credentials.
* Click Create Access Key on the Security Credentials tab.
* Note down the newly created _Access_ and _Secret_ keys.

=== Deploying the infrastructure

On the management machine, find the {tf} template files for AWS in
`/usr/share/caasp/terraform/aws`. These files have been installed as part of
the management pattern (`sudo zypper in -t pattern SUSE-CaaSP-Management`).

Copy this folder to a location of your choice as the files need adjustment.

----
mkdir -p ~/caasp/deployment/
cp -r /usr/share/caasp/terraform/aws/ ~/caasp/deployment/
cd ~/caasp/deployment/aws/
----

Once the files are copied, rename the `terraform.tfvars.example` file to
`terraform.tfvars`:

----
cp terraform.tfvars.example terraform.tfvars
----

Edit the `terraform.tfvars` file and add/modify the following variables:

include::deployment-terraform-example.adoc[tags=tf_aws]

[TIP]
====
You can set timezone and other parameters before deploying the nodes
by modifying the cloud-init template:

* `~/caasp/deployment/aws/cloud-init/cloud-init.yaml.tpl`
====

You can enter the registration code for your nodes in
`~/caasp/deployment/aws/registration.auto.tfvars` instead of the
`terraform.tfvars` file.

Substitute `CAASP_REGISTRATION_CODE` for the code from <<registration_code>>.

[source,json]
----
# SUSE CaaSP Product Key
caasp_registry_code = "<CAASP_REGISTRATION_CODE>"
----

This is required so all the deployed nodes can automatically register
with {scc} and retrieve packages.

Now you can deploy the nodes by running:

----
terraform init
terraform plan
terraform apply
----

Check the output for the actions to be taken. Type "yes" and confirm with
`Enter` when ready.
Terraform will now provision all the cluster infrastructure.

.Public IPs for nodes
[IMPORTANT]
====
`skuba` cannot currently access nodes through a bastion host, so all
the nodes in the cluster must be directly reachable from the machine where
`skuba` is being run.
`skuba` could be run from one of the master nodes or from a pre-existing bastion
host located inside of a joined VPC as described in
<<architecture.aws.vpc_peering>>.
====

.Note down IP/FQDN for nodes
[IMPORTANT]
====
The IP addresses and FQDN of the generated machines will be displayed in the
terraform output during the cluster node deployment. You need these information
later to deploy {productname}.

These information can be obtained at any time by executing the
`terraform output` command within the directory from which you executed
terraform.
====

=== Logging in to the Cluster Nodes

Connecting to the cluster nodes can be accomplished only via SSH key-based
authentication thanks to the ssh-public key injection done earlier via
cloud-init. You can use the predefined `ec2-user` user to log in.

If the ssh-agent is running in the background, run:

----
ssh ec2-user@<node-ip-address>
----

Without the ssh-agent running, run:

----
ssh ec2-user@<node-ip-address> -i <path-to-your-ssh-private-key>
----

Once connected, you can execute commands using password-less `sudo`.
