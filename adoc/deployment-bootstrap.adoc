[[bootstrap]]
== Bootstrapping the cluster

Bootstrapping the cluster is the initial process of starting up the cluster
and defining which of the nodes are masters and which workers. For maximum automation of this process
{productname} uses the `skuba` package.

=== Preparation

==== Install `skuba`

First you need to install `skuba` on your local workstation:

. Add the SLE15 SP1 extension containing `skuba`. This also requires the "containers" module.
+
----
SUSEConnect -p sle-module-containers/15.1/x86_64
SUSEConnect -p caasp/4.0/x86_64 -r <Registration Code>
----
. Install `skuba`:
+
----
sudo zypper in skuba
----

=== Cluster Deployment

Make sure you have added the SSH identity (corresponding to the SSH key distributed above) to the ssh-agent on your workstation.
This is a requirement for `skuba` (https://github.com/SUSE/skuba#prerequisites).

[TIP]
====
The `ssh-agent` process is usually started automatically by most UNIX
environments. If that's not the case, invoke the `ssh-agent` command
and follow the guidance given by the tool's output.

You can load as many keys as you want into your agent using the
`ssh-add <path to key>` command. Keys can also be password protected.

You can list all the identities loaded into the agent by using the
`ssh-add -L` command.

`skuba` will try all the identities loaded into the ssh-agent until one of
them grants access to the node.

It is also possible to forward the authentication agent connection from a
host to another one. This can be useful if you intend to run `skuba` on
a "jump host" and don't want to copy your private key to this node.

This can be achieved using the `ssh -A` command. Please refer to the man page
of ssh to learn about the security implications of using this feature.
====


. Now you can initialize the cluster on the deployed machines.
As `--control-plane` enter the IP/FQDN of your load balancer.
If you do not use a load balancer use your first master node.
+
----
skuba cluster init --control-plane <LB IP/FQDN> my-cluster
----
`cluster init` generates the folder named `my-cluster` and initializes the directory that will hold the configuration (`kubeconfig`) for the cluster.

[IMPORTANT]
====
The IP/FQDN must be reachable by every node of the cluster and therefore 127.0.0.1/localhost can't be used.
====

==== Cluster configuration

At this point you can perform some configurations before bootstrapping the cluster.

===== Integrate External LDAP TLS

. Open the `Dex` `ConfigMap` in `my-cluster/addons/dex/dex.yaml`
. Adapt the `ConfigMap` by adding LDAP configuration to the connector section of the `config.yaml` file. For detailed configurations for the LDAP connector, refer to https://github.com/dexidp/dex/blob/v2.16.0/Documentation/connectors/ldap.md.
====
# Example LDAP connector

    connectors:
    - type: ldap
      id: 389ds
      name: 389ds
      config:
        host: ldap.example.org:636 // <1> <2>
        rootCAData: <base64 encoded PEM file> // <3>
        bindDN: cn=user-admin,ou=Users,dc=example,dc=org // <4>
        bindPW: <Password of Bind DN> // <5>
        usernamePrompt: Email Address // <6>
        userSearch:
          baseDN: ou=Users,dc=example,dc=org // <7>
          filter: "(objectClass=person)" // <8>
          username: mail // <9>
          idAttr: DN // <10>
          emailAttr: mail // <11>
          nameAttr: cn // <12>
====
<1> Host name of LDAP server reachable from the cluster.
<2> The port on which to connect to the host (e.g. StartTLS: `389`, TLS: `636`).
<3> LDAP server base64 encoded root CA certificate file (e.g. `cat <root-ca-pem-file> | base64 | awk '{print}' ORS='' && echo`)
<4> Bind DN of user that can do user searches.
<5> Password of the user.
<6> Label of LDAP attribute users will enter to identify themselves (e.g. `username`).
<7> BaseDN where users are located (e.g. `ou=Users,dc=example,dc=org`).
<8> Filter to specify type of user objects (e.g. "(objectClass=person)").
<9> Attribute users will enter to identify themselves (e.g. mail).
<10> Attribute used to identify user within the system (e.g. DN).
<11> Attribute containing email of users.
<12> Attribute used as username used within OIDC tokens.

Besides the LDAP connector you can also setup other connectors. For additional connectors, refer to the available connector configurations in the Dex repository: https://github.com/dexidp/dex/tree/v2.16.0/Documentation/connectors.

===== Prevent Nodes Running Special Workloads From Being Rebooted

. Open the `kured` deployment in `my-cluster/addons/kured/kured.yaml`
. Adapt the `DaemonSet` by adding one of the following flags to the `command` section of the `kured` container:
+
----
---
apiVersion: apps/v1
kind: DaemonSet
...
spec:
  ...
    ...
      ...
      containers:
        ...
          command:
            - /usr/bin/kured
            - --blocking-pod-selector=name=<NAME OF POD>
----

You can add any key/value labels to this selector:
----
--blocking-pod-selector=<LABEL KEY 1>=<LABLE VALUE 1>,<LABEL KEY 2>=<LABEL VALUE 2>
----

Alternatively you can adapt the `kured` DaemonSet also later during runtime (after bootstrap) by editing `my-cluster/addons/kured/kured.yaml` and executing:
----
kubectl apply -f my-cluster/addons/kured/kured.yaml
----

This will restart all `kured` pods with the additional configuration flags.

==== Prevent Nodes With Any Prometheus Alerts From Being Rebooted

[NOTE]
====
By default, **any** prometheus alert blocks a node from reboot. However you can filter specific alerts to be ignored via the `--alert-filter-regexp` flag.
====

. Open the `kured` deployment in `my-cluster/addons/kured/kured.yaml`
. Adapt the `DaemonSet` by adding one of the following flags to the `command` section of the `kured` container:
+
----
---
apiVersion: apps/v1
kind: DaemonSet
...
spec:
  ...
    ...
      ...
      containers:
        ...
          command:
            - /usr/bin/kured
            - --prometheus-url=<PROMETHEUS SERVER URL>
            - --alert-filter-regexp=^(RebootRequired|AnotherBenignAlert|...$
----

[IMPORTANT]
====
The <PROMETHEUS SERVER URL> needs to contain the protocol (`http://` or `https://`)
====

Alternatively you can adapt the `kured` DaemonSet also later during runtime (after bootstrap) by editing `my-cluster/addons/kured/kured.yaml` and executing:
----
kubectl apply -f my-cluster/addons/kured/kured.yaml
----

This will restart all `kured` pods with the additional configuration flags.

==== Cluster bootstrap
. Switch to the new directory.
. Now bootstrap a master node.
For `--target` enter the IP address of your first master node.
Replace `<NODE NAME>` with a unique identifier for example "master-one".
+
.Secure configuration files access
[WARNING]
====
The directory created during this step contains configuration files
that allow full administrator access to your cluster.
Apply best practices for access control to this folder.
====
+
----
cd my-cluster
skuba node bootstrap --user sles --sudo --target <IP/FQDN> <NODE NAME>
----
This will bootstrap the specified node as the first master in the cluster.
The process will generate authentication certificates and the `admin.conf`
file that is used for authentication against the cluster.
The files will be stored in the `my-cluster` directory specified in step one.
. Add additional master nodes to the cluster.
+
Replace the `<IP/FQDN>` with the IP for the machine.
Replace `<NODE NAME>` with a unique identifier for example "master-two".
+
----
skuba node join --role master --user sles --sudo --target <IP/FQDN> <NODE NAME>
----
. Add a worker to the cluster.
+
Replace the `<IP/FQDN>` with the IP for the machine.
Replace `<NODE NAME>` with a unique identifier for example "worker-one".
+
----
skuba node join --role worker --user sles --sudo --target <IP/FQDN> <NODE NAME>
----
. Verify the nodes that you added
+
----
skuba cluster status
----
+
The output should look like this:
+# instance user name
username = "sles"
----
NAME         OS-IMAGE                              KERNEL-VERSION        CONTAINER-RUNTIME   HAS-UPDATES   HAS-DISRUPTIVE-UPDATES
master-one   SUSE Linux Enterprise Server 15 SP1   4.12.14-110-default   cri-o://1.13.3      <none>        <none>
worker-one   SUSE Linux Enterprise Server 15 SP1   4.12.14-110-default   cri-o://1.13.3      <none>        <none>
----

[IMPORTANT]
====
The IP/FQDN must be reachable by every node of the cluster and therefore 127.0.0.1/localhost can't be used.
====

=== Using kubectl

You can install and use kubectl by installing the kubernetes-client package from the {productname} extension.

----
sudo zypper in kubernetes-client
----

[TIP]
====
Alternatively you can install from upstream: https://kubernetes.io/docs/tasks/tools/install-kubectl/.
====

To talk to your cluster, simply symlink the generated configuration file to `~/.kube/config`.

[source,bash]
----
ln -s ~/clusters/my-cluster/admin.conf ~/.kube/config
----

Then you can perform all cluster operations as usual. For example checking cluster status with either:

* `skuba cluster status`
+
or
* `kubectl get nodes -o wide`
+
or
* `kubectl get pods --all-namespaces`
+
[source,bash]
----
# kubectl get pods --all-namespaces

NAMESPACE     NAME                                READY     STATUS    RESTARTS   AGE
kube-system   coredns-86c58d9df4-5zftb            1/1       Running   0          2m
kube-system   coredns-86c58d9df4-fct4m            1/1       Running   0          2m
kube-system   etcd-my-master                      1/1       Running   0          1m
kube-system   kube-apiserver-my-master            1/1       Running   0          1m
kube-system   kube-controller-manager-my-master   1/1       Running   0          1m
kube-system   kube-flannel-ds-amd64-b6krs         1/1       Running   0          53s
kube-system   kube-flannel-ds-amd64-v7kt7         1/1       Running   0          2m
kube-system   kube-proxy-5qxnt                    1/1       Running   0          2m
kube-system   kube-proxy-746ws                    1/1       Running   0          53s
kube-system   kube-scheduler-my-master            1/1       Running   0          1m
----
