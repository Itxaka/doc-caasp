[[bootstrap]]
== Bootstrapping the cluster

=== Cluster Deployment

==== Cluster initialization
. Initialize the cluster on the deployed machines.
As `--control-plane` enter the IP/FQDN of your load balancer. If you do not use a load balancer use your first master node.
+
----
skuba cluster init --control-plane <LB IP/FQDN> my-cluster
----
`cluster init` generates the folder named `my-cluster` and initializes the directory that will hold the configuration (`kubeconfig`) for the cluster.

[IMPORTANT]
====
The IP/FQDN must be reachable by every node of the cluster and therefore 127.0.0.1/localhost can't be used.
====

==== Cluster configuration

At this point you can perform some configurations before bootstrapping the cluster.

===== Integrate External LDAP

. Open the `Dex` `ConfigMap` in `my-cluster/addons/dex/dex.yaml`
. Adapt the `ConfigMap` by adding LDAP configuration to the connector section of the `config.yaml` file. For detailed configurations for the LDAP connector, refer to https://github.com/dexidp/dex/blob/master/Documentation/connectors/ldap.md.
====
# Example LDAP connector

    connectors:
    - type: ldap
      id: ldap
      name: openLDAP
      config:
        host: dirsrv-389ds.kube-system.svc.cluster.local:389 // <1> <2>
        insecureNoSSL: true
        insecureSkipVerify: true
        bindDN: cn=admin,dc=example,dc=org // <3>
        bindPW: 'admin' // <4>
        usernamePrompt: Email Address // <5>
        userSearch:
          baseDN: cn=People,dc=example,dc=org // <6>
          filter: "(objectClass=person)" // <7>
          username: mail // <8>
          idAttr: DN // <9>
          emailAttr: mail // <10>
          nameAttr: uid // <11>
====
<1> Host name of LDAP server reachable from the cluster.
<2> The port on which to connect to the host (e.g. StartTLS: `389`, TLS: `636`).
<3> Bind DN of user that can do user searches.
<4> Password of the user.
<5> Label of LDAP attribute users will enter to identify themselves (e.g. `username`).
<6> BaseDN where users are located (e.g. `cn=users,dc=example,dc=com`).
<7> Filter to specify type of user objects (e.g. "(objectClass=person)").
<8> Attribute users will enter to identify themselves (here: `mail`).
<9> Attribute used to identify user within the system (e.g. uid).
<10> Attribute containing email of users.
<11> Attribute used as username used within OIDC tokens.

Besides the LDAP connector you can also setup other connectors. For additional connectors, refer to the available connector configurations in the Dex repository: https://github.com/dexidp/dex/tree/master/Documentation/connectors.

===== Prevent Nodes Running Special Workloads From Being Rebooted

. Open the `kured` deployment in `my-cluster/addons/kured/kured.yaml`
. Adapt the `DaemonSet` by adding one of the following flags to the `command` section of the `kured` container:
+
----
---
apiVersion: apps/v1
kind: DaemonSet
...
spec:
  ...
    ...
      ...
      containers:
        ...
          command:
            - /usr/bin/kured
            - --blocking-pod-selector=name=<NAME OF POD>
----

You can add any key/value labels to this selector:
----
--blocking-pod-selector=<LABEL KEY 1>=<LABLE VALUE 1>,<LABEL KEY 2>=<LABEL VALUE 2>
----

Alternatively you can adapt the `kured` DaemonSet also later during runtime (after bootstrap) by editing `my-cluster/addons/kured/kured.yaml` and executing:
----
kubectl apply -f my-cluster/addons/kured/kured.yaml
----

This will restart all `kured` pods with the additional configuration flags.

==== Prevent Nodes With Any Prometheus Alerts From Being Rebooted

[NOTE]
====
By default, **any** prometheus alert blocks a node from reboot. However you can filter specific alerts to be ignored via the `--alert-filter-regexp` flag.
====

. Open the `kured` deployment in `my-cluster/addons/kured/kured.yaml`
. Adapt the `DaemonSet` by adding one of the following flags to the `command` section of the `kured` container:
+
----
---
apiVersion: apps/v1
kind: DaemonSet
...
spec:
  ...
    ...
      ...
      containers:
        ...
          command:
            - /usr/bin/kured
            - --prometheus-url=<PROMETHEUS SERVER URL>
            - --alert-filter-regexp=^(RebootRequired|AnotherBenignAlert|...$
----

[IMPORTANT]
====
The <PROMETHEUS SERVER URL> needs to contain the protocol (`http://` or `https://`)
====

Alternatively you can adapt the `kured` DaemonSet also later during runtime (after bootstrap) by editing `my-cluster/addons/kured/kured.yaml` and executing:
----
kubectl apply -f my-cluster/addons/kured/kured.yaml
----

This will restart all `kured` pods with the additional configuration flags.

==== Cluster bootstrap
. Switch to the new directory.
. Now bootstrap a master node.
For `--target` enter the IP address of your first master node.
Replace `<NODE NAME>` with a unique identifier for example "master-one".
+
.Secure configuration files access
[WARNING]
====
The directory created during this step contains configuration files that allow full administrator access to your cluster. Apply best practices for access control to this folder.
====
+
----
cd my-cluster
skuba node bootstrap --user sles --sudo --target <IP/FQDN> <NODE NAME>
----
This will bootstrap the specified node as the first master in the cluster.
The process will generate authentication certificates and the `admin.conf` file that is used for authentication against the cluster.
The files will be stored in the `my-cluster` directory specified in step one.
. Add additional master nodes to the cluster.
+
Replace the `<IP/FQDN>` with the IP for the machine.
Replace `<NODE NAME>` with a unique identifier for example "master-two".
+
----
skuba node join --role master --user sles --sudo --target <IP/FQDN> <NODE NAME>
----
. Add a worker to the cluster.
+
Replace the `<IP/FQDN>` with the IP for the machine.
Replace `<NODE NAME>` with a unique identifier for example "worker-one".
+
----
skuba node join --role worker --user sles --sudo --target <IP/FQDN> <NODE NAME>
----
. Verify the nodes that you added
+
----
skuba cluster status
----
+
The output should look like this:
+
----
NAME         OS-IMAGE                              KERNEL-VERSION        CONTAINER-RUNTIME   HAS-UPDATES   HAS-DISRUPTIVE-UPDATES
master-one   SUSE Linux Enterprise Server 15 SP1   4.12.14-110-default   cri-o://1.13.3      <none>        <none>
worker-one   SUSE Linux Enterprise Server 15 SP1   4.12.14-110-default   cri-o://1.13.3      <none>        <none>
----

[IMPORTANT]
====
The IP/FQDN must be reachable by every node of the cluster and therefore 127.0.0.1/localhost can't be used.
====

=== Using kubectl

You can install and use kubectl by installing the kubernetes-client package from the {productname} extension.

----
sudo zypper in kubernetes-client
----

[TIP]
====
Alternatively you can install from upstream: https://kubernetes.io/docs/tasks/tools/install-kubectl/.
====

To talk to your cluster, simply symlink the generated configuration file to `~/.kube/config`.

[source,bash]
----
ln -s ~/clusters/my-cluster/admin.conf ~/.kube/config
----

Then you can perform all cluster operations as usual. For example checking cluster status with either:

* `skuba cluster status`
+
or
* `kubectl get nodes -o wide`
+
or
* `kubectl get pods --all-namespaces`
+
[source,bash]
----
# kubectl get pods --all-namespaces

NAMESPACE     NAME                                READY     STATUS    RESTARTS   AGE
kube-system   coredns-86c58d9df4-5zftb            1/1       Running   0          2m
kube-system   coredns-86c58d9df4-fct4m            1/1       Running   0          2m
kube-system   etcd-my-master                      1/1       Running   0          1m
kube-system   kube-apiserver-my-master            1/1       Running   0          1m
kube-system   kube-controller-manager-my-master   1/1       Running   0          1m
kube-system   kube-flannel-ds-amd64-b6krs         1/1       Running   0          53s
kube-system   kube-flannel-ds-amd64-v7kt7         1/1       Running   0          2m
kube-system   kube-proxy-5qxnt                    1/1       Running   0          2m
kube-system   kube-proxy-746ws                    1/1       Running   0          53s
kube-system   kube-scheduler-my-master            1/1       Running   0          1m
----
