[[software-installation]]
= Software Installation

Software can be installed in three basic layers

Base OS layer::
Linux RPM packages, Kernel etc.. Installation via {ay},{tf} or {zypper}

{kube} Stack::
Software that helps/controls execution of workloads in {kube}

Container image::
Here it entirely depends on the actual makeup of the container what can be installed and how.
Please refer to your respecitve container image documentation for further details.
[NOTE]
Installation of software in container images is beyond the scope of this document.

== Base OS

Applications that will be deployed to {kube} will typically contain all the required software to be executed.
In some cases, especially when it comes to the hardware layer abstraction (storage backends, GPU), additional packages
must be installed on the underlying operating system outside of {kube}.

[NOTE]
====
The following examples show installation of required packages for `Ceph`, please adjust the list of
packages and repositories to whichever software you need to install.

While you can install any software package from the {sles} ecosystem this falls outside of the support scope for {productname}.
====

=== Initial Rollout

During the rollout of nodes you can use either {ay} or {tf} (depending on your chosen deployment type)
to automatically install packages to all nodes.

For example, to install additional packages required by the `Ceph` storage backend you can modify
your `autoyast.xml` or `tfvars.yml` files to include the additional repositories and instructions to
install `xfsprogs` and `ceph-common`.

. `tfvars.yml`
+
[source,yaml]
----
# EXAMPLE:
# repositories = {
#   repository1 = "http://example.my.repo.com/repository1/"
#   repository2 = "http://example.my.repo.com/repository2/"
# }
repositories = {
        ....
}

# Minimum required packages. Do not remove them.
# Feel free to add more packages
packages = [
  "kernel-default",
  "-kernel-default-base",
  "ca-certificates-suse",
  "xfsprogs",
  "ceph-common"
]
----
. `autoyast.xml`
+
[source,xml]
----
<!-- install required packages -->
<software>
  <image/>
  <products config:type="list">
    <product>SLES</product>
  </products>
  <instsource/>
  <patterns config:type="list">
    <pattern>base</pattern>
    <pattern>enhanced_base</pattern>
    <pattern>minimal_base</pattern>
    <pattern>basesystem</pattern>
  </patterns>
  <packages config:type="list">
    <package>ceph-common</package>
    <package>xfsprogs</package>
  </packages>
</software>
----

=== Existing Cluster

To install software on existing cluster nodes, you must use `zypper` on each node individually.
Simply log in to a node via SSH and run:

----
sudo zypper in ceph-common xfsprogs
----

== {kube} stack

[[helm_tiller_install]]
=== Installing Helm and Tiller

- As of {productname} {productversion},
Helm is part of the {productname} package repository, so to use Centralized Logging,
you only need to run the following command from the location where you normally run `skuba` commands:

[source,bash]
----
sudo zypper install helm
----

- As of {productname} {productversion}, Tiller is not part of the {productname} package repository,
so to install the Tiller container image, run:

[source,bash,subs="attributes"]
----
kubectl create serviceaccount --namespace kube-system tiller
kubectl create clusterrolebinding tiller --clusterrole=cluster-admin --serviceaccount=kube-system:tiller

helm init --tiller-image registry.suse.com/caasp/v4/helm-tiller:{helm_tiller_version} --service-account tiller
----

////
Note: When Helm is included in v4, Tiller server will be automatically installed after CaaS Platform setup.
So we probably  just need to mention that we use it and that it's installed automatically.
////
