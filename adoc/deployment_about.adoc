[[_deployment.about]]
= About {productname}
:doctype: book
:sectnums:
:toc: left
:icons: font
:experimental:
:sourcedir: .
:imagesdir: ./images
= About SUSE CaaS Platform
:doctype: book
:sectnums:
:toc: left
:icons: font
:experimental:
:imagesdir: ./images

{suse}{reg}
CaaS (Container as a Service) Platform is a platform designed for fast deployment of container-based applications.
You can deploy {productname}
onto physical servers or use it on virtual machines.
After deployment, it is ready to run out of the box and provides a highly-scalable cluster. 

{productname}
was designed to simplify the transformation of applications into __application containers__.
Creating clouds with containers is much easier than installing those applications directly onto the underlying operating system, as it eliminates problems with inter-application  compatibility and dependencies.
To enable the most rapid application deployment possible, a container orchestration framework, e.g. {kube}
, helps considerably. 

While {productname}
inherits benefits of {sle}
and uses tools and technologies well-known to system administrators{mdash}
such as ``cloud-init``, Kubernetes, and SaltStack{mdash}
the main innovation (compared to {sls}
) comes with **transactional
  updates**.
A transactional update is an update that can be installed when the system is running without any down-time.
A transaction update can be rolled back, so if the upgdate fails or is not compatible with your infrastructure, you can restore the previous system state. 

{productname}
uses the {btrfs}
file system with the following characteristics: 

* The root filesystem and its snapshots are read-only. 
* Sub-volumes for data sharing are read-write. 
* {productname} introduces overlays of the `/etc` directories used by `cloud-init` and {salt} . 


[[_caasp.architecture]]
== {productname} Architecture


A typical {productname}
cluster consists of several types of nodes: 

* The _{admin_node}_ is a {smaster} which assigns roles to {sminion} s. This node runs the Velum Web-based dashboard for managing the whole cluster. For details, refer to <<_administration_dashboard>>. 
* Each {cluster_node} is a {sminion} which can have one of the following roles: 
** {kube} master: _{master_node} s_ manage the worker nodes. 
** {kube} worker: _{worker_node} s_ run the application containers which are the main workload of the cluster.


In large-scale clusters, there are other nodes that can help you to manage and run the cluster: 

* a local _{smt} server_ that manages subscriptions for workers and so decreases the traffic to the {scc} . 
* a _log server_ that stores the cluster nodes' logs. 


The following figure illustrates the interactions of the nodes. 

.{productname}Nodes Architecture [[_caasp.architecture.cluster]]

image::caasp_generic_scheme.png[scaledwidth=100%]


To run the whole cluster, {productname}
uses various technologies such as {salt}
Stack, Flannel networking, the `etcd` distributed key- store database, a controller, a scheduler, ``kubelet``, a choice of two container runtime engines, and a {kube}
 API Server. 

{salt}
is used to manage deployment and administration of the cluster.
The `{salt}
-api` is used to distribute commands from {dashboard}
 to the `salt-master` daemon.
The `salt-master` daemon stores events in _MariaDB_ (the database is also used to store {dashboard}
 data). The `salt-minion` daemon on the {admin_node}
 is used to generate required certificates, and on the {sminion}
s the daemons communicate with the {admin_node}
. 

As there can be several containers running on each host machine, each container is assigned an IP address that is used for communication with other containers on the same host machine.
Containers might need to have a unique IP address exposed for network communication, thus flannel networking is used.
Flannel gives each host an IP sub net from which the container engine can allocate IP addresses for containers.
The mapping of IP addresses is stored by using ``etcd``.
The `flanneld` is used to manage routing of packets and mapping of IP addresses. 

Within the cluster there are several instances of ``etcd``, each with a different purpose.
The `etcd discovery` running on the {admin_node}
 is used to bootstrap instances of `etcd` running on other nodes and is not part of the `etcd` cluster on other nodes.
The `etcd` instance on the {master_node}
 stores events from the API Server.
The `etcd` instance on {worker_node}
s runs as a proxy that forwards clients to the `etcd` on the {master_node}
. 

{kube}
is used to manage containers orchestration.
The following services and daemons are used by {kube}
: 
 kubelet::
An agent that runs on each node to monitor and control all the containers in a pod, ensuring that they are running and healthy. 
 kube-apiserver ::
This daemon exposes a REST API used to manage pods.
The API server performs authentication and authorization. 
 scheduler::
The scheduler assigns pods onto nodes.
It does not run them itself; that is ``kubelet``'s job. 
 controllers::
These monitor the shared state of the cluster through the `apiserver` and handle pod replication, deployment, etc. 
 kube-proxy::
This runs on each node and is used to distribute loads and reach services. 


Now let's focus on a more detailed view of the cluster that involves also services running on each node type. 

.Services on nodes [[_caasp.architecture.services]]

image::caasp_nodes_architecture.png[scaledwidth=90%]


[[_administration_dashboard]]
=== The Administration Node


The {admin_node}
manages the cluster and runs several applications required for proper functioning of the cluster.
Because it is integral to the  operation of {productname}
,the {admin_node}
must have a fully-qualified  domain name (FQDN) which can be resolved from outside the cluster. 

The {admin_node}
runs {dashboard}
, the administration dashboard; the MariaDB database; the `etcd discovery` server, __salt-api__, `salt-master` and ``salt-minion``.
The dashboard, database, and daemons all run in separate containers. 

{dashboard}
is a Web application that enables you to deploy, manage, and monitor the cluster.
The dashboard manages the cluster using _salt-api_ to interact with the underlying {salt}
 technology. 

The containers on the {admin_node}
are managed by `kubelet` as a static pod.
Bear in mind that this `kubelet` does not manage the cluster nodes.
Each cluster node has its own running instance of ``kubelet``. 

[[_master_node]]
=== Master Nodes

{productname}{master_node}
s monitor and control the {worker_node}
s.
They make global decisions about the cluster, such as starting and scheduling  pods of containers on the {worker_node}
s.
They run _kube-apiserver_ but do not host application containers. 

Each cluster must have at least one {master_node}
.
For larger clusters, more {master_node}
s can be added, but there must always be an odd number. 

Like the {admin_node}
, the {master_node}
must have a resolvable FQDN.
For {dashboard}
to function correctly, it must always be able to resolve the  IP address of a {master_node}
, so if there are multiple {master_node}
s, they must all share the same FQDN, meaning that some form of DNS load-balancing, such as a round-robin DNS server, should be configured. 

[[_worker_node]]
=== Worker Nodes


The {worker_node}
s are the machines in the cluster which host application containers.
Each runs its own instance of _kubelet_ which controls the pods on that machine.
Earlier versions of {kube}
 referred to {worker_node}
s as "minions". 

Each {worker_node}
runs a container runtime engine (either `Docker` or ``cri-o``) and an instance of ``kube-proxy``. 

The {worker_node}
s do not require individual FQDNs, although it may help in troubleshooting network problems. 