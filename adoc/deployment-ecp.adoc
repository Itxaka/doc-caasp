== Deployment on SUSE OpenStack Cloud

.Preparation Required
[NOTE]
You must have completed <<deployment.preparations>> to proceed.

You will use {tf} to deploy the required master and worker cluster nodes (plus a load balancer) to {soc} and then use the
`skuba` tool to bootstrap the {kube} cluster on top of those.

. Download the SUSE OpenStack Cloud RC file.
.. Log in to SUSE OpenStack Cloud.
.. Click on your username in the upper right hand corner to reveal the dropdown menu.
.. Click on menu:Download OpenStack RC File v3[].
.. Save the file to your workstation.
.. Load the file into your shell environment.
+
----
source container-openrc.sh
----
.. Enter the password for the RC file. This should be same credentials that you use to log in to {soc}.
. Get the SLES15-SP1 image.
.. Download the latest SUSE SLES15-SP1 for {soc} from {download_url}.
.. Upload the image to your {soc}.

.The default user is 'sles'
[NOTE]
The SUSE SLES15-SP1 images for {soc} come with pre-defined user `sles` that you use to log in into the cluster nodes. This user has been configured for password-less 'sudo' and is the one recommended to be used by {tf} and `skuba`.

=== Deploying the cluster nodes

. Find the {tf} template files for {soc} in `/usr/share/caasp/terraform/openstack`.
Copy this folder to a location of your choice as the files need adjustment.
+
----
mkdir -p ~/caasp/deployment/
cp -r /usr/share/caasp/terraform/openstack/ ~/caasp/deployment/
cd ~/caasp/deployment/openstack/
----
. Once the files are copied, rename the `terraform.tfvars.example` file to
`terraform.tfvars`:
+
----
mv terraform.tfvars.example terraform.tfvars
----
. Edit the `terraform.tfvars` file and add modify the following variables:
+
include::deployment-terraform-example.adoc[tags=tf_openstack]
+
[TIP]
====
You can set timezone in before deploying the nodes by modifying the file:

* `~/my-deployment/cloud-init/common.tpl`
====
. Optional: If you absolutely need to be able to ssh into your cluster nodes using password instead of key-based authentication, this is the best time to do it massively for all of your nodes. Alternatively you can do it also later, but manually. To do that you need to modify the cloud-init configuration and comment-out the related SSH configuration:
* `~/my-deployment/cloud-init/common.tpl`
+
----
# Workaround for bsc#1138557 . Disable root and password SSH login
# - sed -i -e '/^PermitRootLogin/s/^.*$/PermitRootLogin no/' /etc/ssh/sshd_config
# - sed -i -e '/^#ChallengeResponseAuthentication/s/^.*$/ChallengeResponseAuthentication no/' /etc/ssh/sshd_config
# - sed -i -e '/^#PasswordAuthentication/s/^.*$/PasswordAuthentication no/' /etc/ssh/sshd_config
# - systemctl restart sshd
----
+
. Enter the registration code for your nodes in `~/my-deployment/registration.auto.tfvars`:
+
Substitute `REGISTRATION_CODE` for the code from <<registration_code>>.
+
[source,json]
----
# SUSE CaaSP Product Registration Code
caasp_registry_code = "REGISTRATION_CODE"
----
+
This is required so all the deployed nodes can automatically register to {scc} and retrieve packages.
. Now you can deploy the nodes by running:
+
----
terraform init
terraform plan
terraform apply
----
+
Check the output for the actions to be taken. Type "yes" and confirm with Enter when ready.
Terraform will now provision all the machines and network infrastructure for the cluster.
+
.Note down IP/FQDN for nodes
[IMPORTANT]
====
The IP addresses of the generated machines will be displayed in the terraform
output during the cluster node deployment. You need these IP addresses to
deploy {productname} to the cluster.

If you need to find an IP addresses later on you can run `terraform output` within the directory you performed the deployment from `~/my-deployment` directory or perform the following steps:

. Log in to {soc} and click on menu:Network[Load Balancers]. Find the one with the string you entered in the terraform configuration above e.g. "testing-lb".
. Note down the "Floating IP". If you have configured a FQDN for this IP, use the hostname instead.
+
image::deploy-loadbalancer-ip.png[]
. Now click on menu:Compute[Instances].
. Switch the filter dropdown to `Instance Name` and enter the string you specified for `stack_name` in the `terraform.tfvars` file.
. Find the Floating IPs on each of the nodes of your cluster.
====

=== Log into the cluster notes

. Connecting into the cluster nodes can be accomplished only via SSH key-based authentication thanks to the ssh-public key injection done earlier via {tf}. You can log in as `sles` user.
+
----
# With ssh-agent running in the background
ssh sles@<node-ip-address>

# Without ssh-agent running
ssh sles@<node-ip-address> -i <path-to-your-ssh-private-key>
----
. Once you are connected you can execute commands using password-less `sudo`. In addition to that, you can also set a password if you prefer to.
+
----
# Set root password
sudo passwd

# Set sles user's password
sudo passwd sles
----
+
.Password authentication has been disabled
[IMPORTANT]
====
Even after setting a password for either `root` or `sles` user, you will be unable to log in via SSH using their passwords respectively. Most likely you will receive a `Permission denied (publickey)` error. This mechanism has been disabled on purpose because of security best practices. However, if this environment does not fit your workflows you can change it on your own risk by modifying the SSH configuration on your own.

* `sudo vi /etc/ssh/sshd_config`


----
# Allow password ssh authentication
+ PasswordAuthentication yes

# Allow login as root via ssh
+ PermitRootLogin yes
----

For the changes to take effect you need to restart the ssh service by running `sudo systemctl restart sshd.service`.
====
