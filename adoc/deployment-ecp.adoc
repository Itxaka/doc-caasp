== Deployment on SUSE OpenStack Cloud

You will use `terrafrom` to deploy the required master and worker cluster nodes (plus a load balancer) to {soc} and then use the
`caaspctl` tool to bootstrap the kubernetes cluster on top of those.

=== Preparation

Both `caaspctl` and `terraform` (along with its configuration examples) are provided by `SUSE-CaaSP-Management` pattern.

. Login and register
+
Activate your SUSE SLES15-SP1 and SUSE CaaSP v4.0 subscription, activating the containers module as well:
+
----
sudo SUSEConnect -r <code>
sudo SUSEConnect -p sle-module-containers/15.1/x86_64
sudo SUSEConnect -p caasp/4.0/x86_64 -r <code>
----
. Install SUSE-CaaSP-Management pattern
+
----
sudo zypper in -t pattern SUSE-CaaSP-Management
----
. Download the SUSE OpenStack Cloud RC file.
.. Log in to SUSE OpenStack Cloud.
.. Click on your username in the upper right hand corner to reveal the dropdown menu.
.. Click on menu:Download OpenStack RC File v3[].
.. Save the file to your workstation.
.. Load the file into your shell environment.
+
----
source container-openrc.sh
----
.. Enter the password for the RC file. This should be same credentials that you use to log in to {soc}.
. Get the SLES15-SP1 image
.. Download the latest SUSE SLES15-SP1 for Openstack from https://download.suse.com
.. Upload the image to your {soc}

=== Deploying the cluster nodes

. Copy the configuration files into a directory:
----
mkdir ~/my-deployment
cp -r /usr/share/caasp/terraform/openstack/* ~/my-deployment
cd my-deployment
----
. Create a file `terraform.tfvars` or just copy the existing `terraform.tfvars.example` to it and modify the configuration: 
+
----
# Name of the image to use
image_name = "SLE-15-SP1-JeOS-GMC"

# Name of the internal network to be created
internal_net = "testing"

# Name of the external network to be used, the one used to allocate floating IPs
external_net = "floating"

# Identifier to make all your resources unique and avoid clashes with other users of this terraform project
stack_name = "testing"

# CIDR of the subnet for the internal network
subnet_cidr = "172.28.0.0/24"

# DNS servers for the nodes
"dns_nameservers" = [
    "172.28.0.2",
    "8.8.8.8",
    "8.8.8.4"
]

# Number of master nodes
masters = 1

# Number of worker nodes
workers = 2

# Size of the master nodes
master_size = "m1.medium"

# Size of the worker nodes
worker_size = "m1.medium"

# Attach persistent volumes to workers
workers_vol_enabled = 0

# Size of the worker volumes in GB
workers_vol_size = 5

# Name of DNS domain
dnsdomain = "testing.qa.caasp.suse.net"

# Set DNS Entry (0 is false, 1 is true)
dnsentry = 0

# Username for the cluster nodes
username = "sles"

# Password for the cluster nodes
password = "linux"

# Optional: Define the repositories to use
# repositories = [
#     { repository1 = "http://example.my.repo.com/repository1/" },
#     { repository2 = "http://example.my.repo.com/repository2/" }
# ]
repositories = []

# Define required packages to be installed/removed. Do not change those.
packages = [
  "kernel-default",
  "-kernel-default-base",
  "kubernetes-kubeadm",
  "kubernetes-kubelet",
  "kubernetes-client"
]

# ssh keys to inject into all the nodes
authorized_keys = [
  ""
]

# IMPORTANT: Replace these ntp servers with ones from your infrastructure
ntp_servers = ["0.novell.pool.ntp.org", "1.novell.pool.ntp.org", "2.novell.pool.ntp.org", "3.novell.pool.ntp.org"]
----
. Replace `internal_net` and `stack_name` with your desired name for the cluster. This string will be used to generate the human readable IDs in {soc}.
If you use a generic term it is very likely to fail deployment because the term is already in use by someone else. It's a good idea to use your username or other unique identifier.
. You can adjust the size of the deployed cluster by changing the "masters" and "workers" variables.
. Insert your public SSH key into the "authorized keys" variable. This key will be distributed to all machines and is your login credentials for all cluster nodes.
. Make sure to set the correct NTP time servers for your infrastructure.
. You can set timezone in before deploying the nodes by modifying the files:
+
* `~/my-deployment/cloud-init/master.tpl`
* `/my-deployment/cloud-init/worker.tpl`
. Enter the registration code for your nodes into `~/my-deployment/registration.auto.tfvars`:
+
----
# SUSE CaaSP Product Registration Code
caasp_registry_code = ""
----
. Now you can deploy the nodes by running:
----
terraform init
terraform plan
terraform apply
----

Check the output for the actions to be taken. Type "yes" and confirm with Enter when ready.
Terraform will now provision all the machines and network infrastructure for the cluster.

.Note down IP/FQDN for nodes
[IMPORTANT]
====
The IP addresses of the generated machines will be displayed in the terraform
output during the cluster node deployment. You need these IP addresses to
deploy {productname} to the cluster.

If you need to find an IP addresses later on you can run `terraform output` within the directory you performed the deployment from `~/my-deployment` directory or perform the following steps:

. Log in to {soc} and click on menu:Network[Load Balancers]. Find the one with the string you entered in the terraform configuration above e.g. "testing-lb".
. Note down the "Floating IP". If you have configured a FQDN for this IP, use the hostname instead.
+
image::deploy-loadbalancer-ip.png[]
. Now click on menu:Compute[Instances].
. Switch the filter dropdown to `Instance Name` and enter the string you specified for `stack_name` in the `terraform.tfvars` file.
. Find the Floating IPs on each of the nodes of your cluster.
====
