== Handling updates

=== Kubernetes components

The update of Kubernetes components is handled via `skuba`.

==== An overview

In order to get an overview of the updates available, you can run:

----
skuba cluster upgrade plan
----

If the cluster is already running the latest available versions, the output
should look like this:

----
skuba version: 0.6.1 (development) 0a6f78f 20190703 go1.12.4
Current Kubernetes cluster version: 1.14.1
Latest Kubernetes version: 1.14.1
Congratulations! You are already at the latest version available
----

If the cluster has a new patch-level and minor Kubernetes version available, the
output should look like this:

----
skuba version: 0.6.1 (development) 0a6f78f 20190703 go1.12.4
Current Kubernetes cluster version: 1.14.1
Latest Kubernetes version: 1.15.1
Upgrade path to update from 1.14.1 to 1.15.1:
 - 1.14.1 -> 1.14.2
 - 1.14.2 -> 1.15.1
----

Similarly, you can also fetch this information on a per-node basis with the following command:

----
skuba node upgrade plan <node>
----

For example, if the cluster has a node named `worker0` which is running the latest available versions, the output should look like this:

----
skuba version: 0.6.1 (development) b6a653225c98 20190704 go1.12.4
Current Kubernetes cluster version: 1.14.1
Latest Kubernetes version: 1.14.1

Node worker0 is up to date
----

On the other hand, if this same node has a new patch-level or minor Kubernetes version available, the output should look like this:

----
skuba version: 0.6.1 (development) b6a653225c98 20190704 go1.12.4
Current Kubernetes cluster version: 1.14.1
Latest Kubernetes version: 1.15.0

Component versions in worker0
  - kubelet: 1.13.7 -> 1.14.1
  - cri-o: 1.13.7 -> 1.14.1
----

You will get a similar output if there is a version available on a master node
(named `master0` in this example):

----
skuba version: 0.6.1 (development) b6a653225c98 20190704 go1.12.4
Current Kubernetes cluster version: 1.15.0
Latest Kubernetes version: 1.14.1

Component versions in master0
  - apiserver: 1.14.1 -> 1.15.0
  - controller-manager: 1.14.1 -> 1.15.0
  - scheduler: 1.14.1 -> 1.15.0
  - etcd: 3.3.11 -> 3.3.11
  - kubelet: 1.14.1 -> 1.15.0
  - cri-o: 1.14.1 -> 1.15.0
----

Finally, it may happen that the control plane is out of date. In this case, you would get something like this:

----
skuba version: 0.6.1 (development) b6a653225c98 20190704 devel +5f509148b13f Wed Jun 5 00:53:25 2019 +0000
Current Kubernetes cluster version: 1.15.1
Latest Kubernetes version: 1.15.1

Unable to plan node upgrade: at least one control plane does not tolerate the current cluster version
----

[TIP]
=====
The control plane consists of these components:

* apiserver
* controller-manager
* scheduler
* etcd
* kubelet
* cri-o
=====

=== Applying an update

[NOTE]
====
The following steps are temporary and will be integrated into the official product soon
====

Updates have to be applied on a per node base, starting from the controlplane, down to the worker nodes.

. To prepare the update, `ssh` to each of the nodes and perform the following steps:
.. disable the `skuba-update.timer`
+
----
sudo systemctl disable --now skuba-update.timer
----
.. create a temporary lock on the kubernetes component system packages
+
----
sudo zypper addlock kubernetes-client kubernetes-common kubernetes-kubelet cri-o
----
.. update kubeadm to the newer version
+
----
sudo zypper update kubernetes-kubeadm
----
.. choose the `break kubernetes-kubeadm-[VERSION] by ignoring some of its dependencies` solution
.. remove the locks again
+
----
sudo zypper removelock kubernetes-client kubernetes-common kubernetes-kubelet cri-o
----
. Now that all nodes are prepared, we can use `skuba` to:
.. upgrade the master nodes
+
----
skuba node upgrade apply --target <master-node-ip> --user <user> --sudo
----
.. when all master nodes are upgraded, upgrade the worker nodes
+
----
skuba node upgrade apply --target <worker-node-ip> --user <user> --sudo
----

Now all of your cluster nodes are upgraded. You can verify this by running:
----
kubectl get nodes -o wide
----

To enable the automatic OS updates again, `ssh` to each of the nodes and enable the `skuba-update.timer` again:

----
sudo systemctl enbable --now skuba-update.timer
----

[TIP]
====
The upgrade via `skuba node upgrade apply` will

* upgrade the containerized control plane
* upgrade the rest of the kubernetes system stack (kubelet, cri-o)
* restart services
====

=== Base OS

Base Operating System updates are handled by `skuba-update`, which works together
with `kured`, and they are configurable.

==== Disable automatic updates

Nodes added to a cluster have the service `skuba-update.timer`, which is responsible for running automatic updates, activated by default. This service is calling `skuba-update` utility and it can be configured with the `/etc/sysconfig/skuba-update` file. To disable the automatic updates on a node simply `ssh` to it and then configure the skuba-update service by editing `/etc/sysconfig/skuba-update` file with the following runtime options:

----
## Path           : System/Management
## Description    : Extra switches for skuba-update
## Type           : string
## Default        : ""
## ServiceRestart : skuba-update
#
SKUBA_UPDATE_OPTIONS="--annotate-only"
----

It is not required to reload or restart `skuba-update.timer`.

The `--annotate-only` flag makes `skuba-update` utility to only check if updates are available and annotate the node accordingly. When this flag is activated no updates are installed at all.

==== Completely disable reboots

If you would like to take care of reboots manually, either as a temporary measure or permanently, you can disable them by creating a lock:

----
kubectl -n kube-system annotate ds kured weave.works/kured-node-lock='{"nodeID":"manual"}'
----

This command modifies an annotation (`annotate`) on the daemonset (`ds`) named `kured`.
You must replace `"nodeID"` with the ID of the cluster node that you wish to lock out of automatic reboots.
Retrieve the ID by running `kubectl get nodes` and copying the value from the first column.

==== Manual unlock

In exceptional circumstances, such as a node experiencing a permanent failure whilst rebooting, manual intervention may be required to remove the cluster lock:

----
kubectl -n kube-system annotate ds kured weave.works/kured-node-lock-
----

This command modifies an annotation (`annotate`) on the daemonset (`ds`) named `kured`.
It explicitly performs an "unset" (`-`) for the value for the annotation named `weave.works/kured-node-lock`.
