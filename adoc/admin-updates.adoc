== Handling updates

=== Kubernetes components

The update of {kube} components is handled via `skuba`.

==== An overview

In order to get an overview of the updates available, you can run:

----
skuba cluster upgrade plan
----

This will show you a list of updates (if available) for different components
installed on the cluster. If the cluster is already running the latest available
versions, the output should look like this:

----
skuba version: 0.7.1 (release)  20190712 go1.12.6
Current Kubernetes cluster version: 1.15.0
Latest Kubernetes version: 1.15.0

Congratulations! You are already at the latest version available
----

If the cluster has a new patch-level and minor {kube} version available, the
output should look like this:

----
skuba version: 0.7.1 (release)  20190712 go1.12.6
Current Kubernetes cluster version: 1.14.1
Latest Kubernetes version: 1.15.0
Upgrade path to update from 1.14.1 to 1.15.0:
 - 1.14.1 -> 1.14.2
 - 1.14.2 -> 1.15.0
----

Similarly, you can also fetch this information on a per-node basis with the following command:

----
skuba node upgrade plan <node>
----

For example, if the cluster has a node named `worker0` which is running the latest available versions, the output should look like this:

----
skuba version: 0.7.1 (release)  20190712 go1.12.6
Current Kubernetes cluster version: 1.15.0
Latest Kubernetes version: 1.15.0

Node worker0 is up to date
----

On the other hand, if this same node has a new patch-level or minor {kube} version available, the output should look like this:

----
skuba version: 0.7.1 (release)  20190712 go1.12.6
Current Kubernetes cluster version: 1.14.1
Latest Kubernetes version: 1.15.0

Component versions in worker0
  - kubelet: 1.13.7 -> 1.14.1
  - cri-o: 1.13.7 -> 1.14.1
----

You will get a similar output if there is a version available on a master node
(named `master0` in this example):

----
skuba version: 0.7.1 (release)  20190712 go1.12.6
Current Kubernetes cluster version: 1.15.0
Latest Kubernetes version: 1.14.1

Component versions in master0
  - apiserver: 1.14.1 -> 1.15.0
  - controller-manager: 1.14.1 -> 1.15.0
  - scheduler: 1.14.1 -> 1.15.0
  - etcd: 3.3.11 -> 3.3.11
  - kubelet: 1.14.1 -> 1.15.0
  - cri-o: 1.14.1 -> 1.15.0
----

It may happen that the {kube} version on the control plane is too outdated
for the update to progress. 
In this case, you would get output similar to the following:

----
skuba version: 0.7.1 (release)  20190712 go1.12.6
Current Kubernetes cluster version: 1.15.0
Latest Kubernetes version: 1.15.0

Unable to plan node upgrade: at least one control plane does not tolerate the current cluster version
----


[TIP]
=====
The control plane consists of these components:

* apiserver
* controller-manager
* scheduler
* etcd
* kubelet
* cri-o
=====

=== Applying Updates

[NOTE]
====
The following steps are temporary, the procedure is prone to change with the next update.
====

[IMPORTANT]
====
Updates have to be applied separately to each node, starting with the control plane all the way down to the worker nodes.
====

==== Preparation

Before proceeding with update itself, some preparation is needed. SSH to each of the nodes and perform the following steps.

. Disable the `skuba-update.timer`:
+
----
sudo systemctl disable --now skuba-update.timer
----
. Update kubeadm to the newer version:
+
----
sudo zypper install --force kubernetes-kubeadm=1.15.0 kubernetes-client=1.14.1 kubernetes-common=1.14.1 kubernetes-kubelet=1.14.1
----
Select the solution 3: `break kubernetes-kubeadm-[VERSION] by ignoring some of its dependencies`.

==== Applying The Update

Now that you have done the required preparations, we can use `skuba` to:

. Upgrade the master nodes:
+
During the upgrade to a newer version, the apiserver will be unavailable so it is recommended to remove the server from the load balancer pool before the upgrade to avoid any connection issues.
+
----
skuba node upgrade apply --target <master-node-ip> --user <user> --sudo
----
. When all master nodes are upgraded, upgrade the worker nodes as well:
+
----
skuba node upgrade apply --target <worker-node-ip> --user <user> --sudo
----

* Verify that your cluster nodes are upgraded by running:
+
----
kubectl get nodes -o wide
----

* To enable the automatic OS updates again, SSH to each of the nodes and enable the `skuba-update.timer`:
+
----
sudo systemctl enable --now skuba-update.timer
----

[TIP]
====
The upgrade via `skuba node upgrade apply` will

* Upgrade the containerized control plane
* Upgrade the rest of the {kube} system stack (`kubelet`, `cri-o`)
* Restart services
====

=== Base OS

Base Operating System updates are handled by `skuba-update`, which works together
with the `kured` reboot daemon.

==== Disable automatic updates

Nodes added to a cluster have the service `skuba-update.timer`, which is responsible for running automatic updates, activated by default.
This service is calling `skuba-update` utility and it can be configured with the `/etc/sysconfig/skuba-update` file.
To disable the automatic updates on a node simply `ssh` to it and then configure the skuba-update service by editing `/etc/sysconfig/skuba-update` file with the following runtime options:

----
## Path           : System/Management
## Description    : Extra switches for skuba-update
## Type           : string
## Default        : ""
## ServiceRestart : skuba-update
#
SKUBA_UPDATE_OPTIONS="--annotate-only"
----

[TIP]
It is not required to reload or restart `skuba-update.timer`.

The `--annotate-only` flag makes `skuba-update` utility to only check if updates are available and annotate the node accordingly.
When this flag is activated no updates are installed at all.

==== Completely disable reboots

If you would like to take care of reboots manually, either as a temporary measure or permanently, you can disable them by creating a lock:

----
kubectl -n kube-system annotate ds kured weave.works/kured-node-lock='{"nodeID":"manual"}'
----

This command modifies an annotation (`annotate`) on the daemonset (`ds`) named `kured`.
You must replace `"nodeID"` with the ID of the cluster node that you wish to lock out of automatic reboots.
Retrieve the ID by running `kubectl get nodes` and copying the value from the first column.

==== Manual unlock

In exceptional circumstances, such as a node experiencing a permanent failure whilst rebooting, manual intervention may be required to remove the cluster lock:

----
kubectl -n kube-system annotate ds kured weave.works/kured-node-lock-
----

This command modifies an annotation (`annotate`) on the daemonset (`ds`) named `kured`.
It explicitly performs an "unset" (`-`) for the value for the annotation named `weave.works/kured-node-lock`.
