include::entities.adoc[]
:isofile: SLE-15-SP1-Installer-DVD-x86_64-Build225.1-Media1.iso
:isolink: http://download.suse.de/install/SLE-15-SP1-Installer-GMC1/

== Deployment on VMWare

=== Environment description

[NOTE]
====
These instructions are based on `VMware ESXi {vmware_version}`.
====

[IMPORTANT]
====
These instructions currently do not describe how to set up a load balancer.
This will be added in future versions. You must provide your own load balancing
solution that directs access to the master nodes.
====

=== VM preparation for creating a template

. Temporarily enable SSH access to one of the ESXi nodes in the vSphere WebUI
as described at link:https://kb.vmware.com/s/article/2004746[]
. Log in to the node.
Replace `vmware.esxi.node` with the hostname of your SSH enabled node.
+
----
ssh root@vmware.esxi.node
----
. Change to a data volume and create a new directory to hold the installation media
+
----
cd /vmfs/volumes/my-directory/
----
. Download the installation media from
{isolink}{isofile}
+
[subs=attributes]
----
wget {isolink}{isofile}
----

Now you can create a new base VM for {productname} within the designated resource
pool through the vSphere WebUI:

. Create a "New Virtual Machine"
. Define a name for the virtual machine (VM)
+
image::vmware_step1.png[width=80%,pdfwidth=80%]
. Select the folder where the VM will be stored
. Select a `Compute Resource` that will run the VM
+
image::vmware_step2.png[width=80%,pdfwidth=80%]
. Select the storage used by the VM
+
image::vmware_step3.png[width=80%,pdfwidth=80%]
. Select `ESXi 6.7 and later` from compatibility
+
image::vmware_step4.png[width=80%,pdfwidth=80%]
. Select menu:Guest OS Family[Linux] and menu:Guest OS Version[SUSE Linux Enterprise 15 (64 Bit)].
+
*Note*: You will manually select the correct installation media in the next step.
+
image::vmware_step5.png[width=80%,pdfwidth=80%]
. Now customize the hardware settings
+
image::vmware_step6.png[width=80%,pdfwidth=80%]
.. Select menu:CPU[2]
.. Select menu:Memory[4096 MB]
.. Select menu:New Hard disk[40GB]
.. Select menu:New SCSI Controller[LSI Logic Parallel SCSI controller (default)] and change it to "VMware Paravirtualized"
.. Select menu:New Network[VM Network], menu:New Network[Adapter Type > VMXNET3]
+
("VM Network" sets up a bridged network which provides a public IP address reachable within a company)
.. Select menu:New CD/DVD[Datastore ISO File]
.. Tick the box menu:New CD/DVD[Connect At Power On] to be able boot from ISO/DVD
.. The click on "Browse" next to the `CD/DVD Media` field to select the downloaded ISO image on desired datastore (here: `/vmfs/volumes/my-directory/{isofile}`),

+
image::vmware_step6b.png[width=80%,pdfwidth=80%]
.. Go to tab VM Options
.. Select menu:Boot Options[]
.. Select menu:Firmware[Bios]

==== {sls} installation

[NOTE]
====
Use AutoYaST and ensure to use a staged frozen patchlevel via SMT/SUSE Manager to ensure a 100% reproducible setup.
For the moment (until an AutoYAsT template is available) follow the procedure below.
====

Power on the newly created VM and install the system over graphical remote console:

. Enter registration code for SLES in YaST.
. Confirm the update repositories prompt with "Yes".
. Remove the checkmark in the "Hide Development Versions" box
.
. Make sure the following modules are selected on the "Extension and Module Selection" screen:
+
image::vmware_extension.png[width=80%,pdfwidth=80%]
** SUSE CaaS Platform 4.0 x86_64 (ALPHA)
** Basesystem Module
** Containers Module (this will automatically be ticked when you select {productname})
** Public Cloud Module
. Enter the registration code to unlock the {productname} extension
. Select menu:System Role[Minimal] on the "System Role" screen
. Click on "Expert Partitioner" to redesign the default partition layout
. Select "Start with current proposal"
+
image::vmware_step8.png[width=80%,pdfwidth=80%]
.. Keep `sda1` as BIOS partition
.. Remove the root `/` partition.
+
Select the device in "System View" on the left (Default: `/dev/sda2`) and click "Delete". Confirm with "Yes".
+
image::vmware_step9.png[width=80%,pdfwidth=80%]
.. Remove the `/home` partition.
.. Remove the `swap` partition.
. Select the `/dev/sda/` device in "System View" and then click menu:Partitions[Add Partition]
+
image::vmware_step10.png[width=80%,pdfwidth=80%]
. Accept the default maximum size (remaining size of the hard disk defined earlier without the boot partition)
+
image::vmware_step11.png[width=80%,pdfwidth=80%]
.. Confirm with "Next"
.. Select menu:Role[Operating System]
+
image::vmware_step12.png[width=80%,pdfwidth=80%]
.. Confirm with "Next"
.. Accept the default settings
+
image::vmware_step13.png[width=80%,pdfwidth=80%]
*** Filesystem: BtrFS
*** Enable Snapshots
*** Mount Device
*** Mount Point `/`
. You should be left with 2 partitions. Now click "Accept".
+
image::vmware_step7.png[width=80%,pdfwidth=80%]
. Confirm the partitioning changes.
+
image::vmware_step14.png[width=80%,pdfwidth=80%]
. Click "Next".
. Configure your timezone and click "Next".
. Create a user with the Username `sles` and specify a password
.. Tick the box menu:Local User[Use this password for system administrator]
+
image::vmware_step15.png[width=80%,pdfwidth=80%]
. Click "Next".
. On the "Installation Settings" screen:
.. In the "Network Configuration" section:
... Disable the Firewall (click on `(disable)`)
... Enable the SSH service (clock on `(enable)`)
.. Scroll to the `kdump` section of the software description and click on the title.
. In the "Kdump Start-Up" screen select menu:Enable/Disable Kdump[Disable Kdump]
.. Confirm with "OK".
+
image::vmware_step16.png[width=80%,pdfwidth=80%]
. Click "Install". Confirm the installation by clicking "Install" in the popup dialog.
. Finish the installation and confirm system reboot with "OK".
+
image::vmware_step17.png[width=80%,pdfwidth=80%]

==== Preparation of the VM as a template

In order to run {productname} on the created VMs, you must configure and install some additional details
like sudo, vmware tools and adding the {kube} dependencies.

[TIP]
Steps 1-3 may be skipped, if they were already performed in YaST during the {sle} installation.

. Register SLES15-SP1 system. Substitute `SLES_REGCODE` for the {sle} registration code.
+
----
SUSEConnect -r SLES_REGCODE
----
. Register Container Module (free of charge)
+
----
SUSEConnect -p sle-module-containers/15.1/x86_64
----
. Register the {productname} Module. Substitute `CAASP_REGCODE` for the {productname} registration code.
+
----
SUSEConnect -p caasp/4.0/x86_64 -r CAASP_REGCODE
----
// . Install `open-vm-tools`
// +
// ----
// zypper in open-vm-tools # in my case it was installed and enabled automatically during sles installation
// systemctl status vmtoolsd; systemctl status vgauthd.service
// ----
. Copy your public SSH key for the user used for caaspctl bootstrap (normally 'sles') which will be used for {productname} deployment:
+
----
ssh-copy-id -i ~/.ssh/id_rsa.pub sles@vm-template.host
----
. Install required packages:
+
----
zypper in cloud-init sudo kubernetes-kubelet kubernetes-kubeadm kubernetes-client cni-plugins btrfsprogs e2fsprogs cloud-init-vmware-guestinfo
----
. Configure `sudo` for the `{username}` user to be able authenticate without password. Replace `{username}` with the user you created during installation. As root run:
+
----
echo "{username} ALL=(ALL) NOPASSWD: ALL" >> /etc/sudoers
----

. Enable the installed `cloud-init` services:
+
----
systemctl enable cloud-init cloud-init-local cloud-config cloud-final
----

. Deregister from `scc`:
+
----
SUSEConnect -d
----

. Do a cleanup of the SLE image for converting into a VMWare template:
+
----
rm /etc/machine-id /var/lib/zypp/AnonymousUniqueId /var/lib/systemd/random-seed /var/lib/dbus/machine-id /var/lib/wicked/*
----
. Cleanup btrfs snapshots and create one with initial state:
+
----
snapper list
snapper delete <list_of_nums_of_unneeded_snapshots>
snapper create -d "Initial snapshot for caasp template" -t single
----
. Power down the VM as root:
+
----
shutdown -h now
----

==== Creating the VMWare Template

Now you can convert the VM into a template in VMware (or repeat this action block for each vm).

. In the vSphere WebUI right-click on the VM and select menu:Template[Convert to Template].
Name it reasonably so you can later identify the template. The template will be created.

=== Deploying VMs from the Template

==== Terraform

To deploy the required virtual machines from the newly created template, it is
possible to use `terraform`.

To obtain the required packages you can use the {productname} product repositories to
install the packages: `terraform` and `caaspctl`.

Once these packages are installed, you will find terraform template files from
VMWare in `/usr/share/caasp/terraform/vmware`. Copy this folder to a location
of your choice, as the files need adjustment.

----
mkdir -p ~/caasp/deployment/
cp -r /usr/share/caasp/terraform/vmware/ ~/caasp/deployment/
cd ~/caasp/deployment/vmware/
----

Once the files are copied, rename the `terraform.tfvars.example` file to
`terraform.tfvars`:

----
mv terraform.tfvars.example terraform.tfvars
----

Edit the `terraform.tfvars` file and adjust (at least) the following variables:

. `vsphere_datastore`: The datastore to use.
. `vsphere_datacenter`: The datacenter to use.
. `vsphere_network`: The network to use.
. `vsphere_resource_pool`: The resource pool to use.
. `template_name`: The name of the template created according to instructions
. `stack_name`: Prefix for all machines of the cluster spawned by terraform.
. `authorized_keys`: List of ssh-public-keys that will be able to login to the
deployed machines.
. `ntp_servers`: A list of `ntp` servers you would like to use with `chrony`.
. `repositories`: A list of additional repositories to be added on each
machines - leave empty if no additional packages need to be installed.

Also adjust the file `registration.auto.tfvars` and add the {productname}
registration code, so each machine is correctly registered after initialization.

Once the files are adjusted, `terraform` needs to know about the `vSphere` server
and the login details for it; these can be exported as environment variables or
entered every time `terraform` is invoked.

Additionally, the `ssh-key` that is specified in the `tfvars` file must be added
to the keyring, so the machine running `caaspctl` can `ssh` into the machines:

----
export VSPHERE_SERVER="<server_address"
export VSPHERE_USER="<username>"
export VSPHERE_PASSWORD="<password>"

ssh-add -i <path_to_ssh_key_from_tfvars>
----

Run terraform to create the required machines for use with `caaspctl`:

----
terraform init
terraform apply
----

===== Example
[source,json]
----
# SUSE CaaSP Product Registration Code
caasp_registry_code = "..."

# Name of the internal network to be created
internal_net = "caasp"

# identifier to make all your resources unique and avoid clashes with other users of this terraform project
stack_name = "caasp"

# instance user name
username = "sles"

# define which image to use
image_name = "SLE-15-SP1-JeOS-GMC"

# Number of master nodes
masters = 1

# Number of worker nodes
workers = 2

repositories = []

packages = [
  "kubernetes-kubeadm",
  "kubernetes-client",
  "kernel-default",
  "-kernel-default-base",
]

# ssh keys to inject into all the nodes
authorized_keys = [
  "ssh-rsa ..."
]

# IMPORTANT: Replace these ntp servers with ones from your infrastructure
ntp_servers = ["0.novell.pool.ntp.org", "1.novell.pool.ntp.org", "2.novell.pool.ntp.org", "3.novell.pool.ntp.org"]
----

==== Setup by hand

. Right-click on the template and select "New VM from This Template..." and name it something like `caasp4-master-0`.
.. Select storage `datastore` and the designated resource pool.

Repeat these steps until you have created the desired number of machines comprising your cluster.

[IMPORTANT]
====
Make sure to give each VM a clear name that shows it's purpose in the cluster e.g.

* `caasp-worker-0`
* `caasp-worker-1`
* `caasp-master-0`
* `caasp-master-1`

You will need these names during bootstrapping of the cluster.
====
. Power on the newly created VMs.
. You need to know the FQDN/IP for each of the created VMs during the bootstrap process.
. Once the VMs booted up, log in via SSH and run the following command to regenerate the Machine ID `/etc/machine-id`:
+
----
sudo dbus-uuidgen --ensure
sudo systemd-machine-id-setup
----

=== Installing {productname} packages

Install {productname} related packages to each node. You can alternatively pre-install them during the creation of the template but
then you are limited to installing old versions of the packages with the template and updating manually afterwards.

[TIP]
====
These steps have to be performed on each VM that you just created; thus it can be time consuming.
Use a terminal with multiplex support or something like link:https://parallel-ssh.org/[Parallel SSH (pssh)]
to perform commands or control SSH sessions across multiple machines simultaneously.
====

Install the following packages on each node:

* kubernetes-kubeadm
* kubernetes-client

----
sudo zypper in kubernetes-kubeadm kubernetes-kubelet kubernetes-client cri-o cni-plugins
----
