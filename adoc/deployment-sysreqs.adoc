== Requirements

=== Registration Code

[NOTE]
The registration code for {productname} {productnumber} also contains the activation
permissions for the underlying {sle} operating system. You can use your {productname}
registration code to activate the {sle} 15 SP1 subscription.

[IMPORTANT]
====
Because of a bug (gh#avant-garde#284) it is possible that the SLE 15 activation
will not work with the registration codes for SUSE-CaaSP. In this case please
download a registration code for SLE15 SP1 and use this to activate instead.

Nonetheless, you must use the registration code downloaded for SUSE-CaaSP when
enabling the extension.
====

You need a subscription registration code to use {productname}. You can retrieve your
registration code from {scc}.

* Login to https://scc.suse.com
* Navigate to menu:Internal tools[Products]
* Search for "caasp"
* Select menu:SUSE CaaS Platform[4.0 >  x86_64 alpha]
* Copy the menu:Registration Code[]

=== Platform

Currently we support:

* SUSE OpenStack Cloud 8.
* VMWare ESXi 6.0.0 Update 3

== Nodes

You will need at least two machines:

* 1 Master Node
* 1 Worker Node

{productname} {productversion} supports deployments with a single or multiple master nodes.
Production environments must be deployed with multiple master nodes for resilience.

=== Hardware

==== Master

* Storage: 50 GB+
* (v)CPU: 2
* RAM: 2 GB
* Network: Minimum 1GB/s, faster preferred

==== Worker

* Storage: Depending on workloads, minimum 20-30GB to hold base OS and required packages
** Mount additional storage volumes as needed
* (v)CPU: 2
* RAM: 2 GB
* Network: Minimum 1GB/s, faster preferred

==== Storage Performance

For Master and Worker nodes you must ensure storage performance of at least 500 sequential IOPS with disk bandwidth depending on your cluster size.

    "Typically 50 sequential IOPS (e.g., a 7200 RPM disk) is required.
    For heavily loaded clusters, 500 sequential IOPS (e.g., a typical local SSD
    or a high performance virtualized block device) is recommended."

    "Typically 10MB/s will recover 100MB data within 15 seconds.
    For large clusters, 100MB/s or higher is suggested for recovering 1GB data
    within 15 seconds."

link:https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md#disks[]

=== Networking

==== Ports

[cols="2*.^,.^,.>"",options="header,autowidth"]
|===
|Node |Port |Accessibility |Description

.2+|All nodes
|22
|Internal
|SSH (required in public clouds)

|10010
|Internal
|CRI-O stream port

.6+|Masters
|2379 - 2380
|Internal
|etcd (peer-to-peer traffic)

|6443 - 6444
|Internal / External
|Kubernetes API server

|8471 - 8472
|Internal
|VXLAN traffic (used by Cilium)

|10250, 20255
|Internal
|Kubelet

|10256
|Internal
|kube-proxy

|32000
|External
|Dex (OIDC Connect)

.6+|Workers
|2379 - 2380
|Internal
|etcd (peer-to-peer traffic)

|4149
|Internal
|Kubelet

|8471 - 8472
|Internal
|VXLAN traffic (used by Cilium)

|10250
|Internal
|Kubelet

|10256
|Internal
|kube-proxy

|32000
|External
|Dex (OIDC Connect)
|===

==== IP Addresses

All nodes must be assigned static IP addresses that must not be changed manually afterwards.
Communication

Please make sure that all your Kubernetes components can communicate with each other.
This might require the configuration of routing when using multiple network adapters per node.

Refer to: https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-network-adapters.

Configure firewall and other network security to allow communication on the default ports required by Kubernetes: https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports

==== Performance

All master nodes of the cluster must have a minimum 1GB/s network connection to fulfill the requirements for etcd.

    "1GbE is sufficient for common etcd deployments. For large etcd clusters,
    a 10GbE network will reduce mean time to recovery."

link:https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md#network[]

==== Security

Do not grant access to the kubeconfig file or any workstation configured with this configuration to unauthorized personnel.
In the current state, full administrative access is granted to the cluster.

Authentication is done via the kubeconfig file generated during deployment. This file will grant full access to the cluster and all workloads.
Apply best practices for access control to workstations configured to administer the {productname} cluster.
