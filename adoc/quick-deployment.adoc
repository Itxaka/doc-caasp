include::entities.adoc[]

= {productname} {productversion} QuickStart Guide
:doctype: book
:sectnums:
:toc: left
:toclevels: 3
:icons: font
:experimental: true
:revdate: 2019-03-29
:imagesdir: images/


[WARNING]
====
This is an internal release and MUST NOT be distributed outside SUSE
====

[IMPORTANT]
====
This is a very early pre-release of the software. You will encounter bugs
and incomplete features. Please do not use for any productive deployments.
====

[NOTE]
====
This guide assumes a configured SUSE SLE 15 SP1 workstation environment.
====

== Purpose of this document

This guide describes the deployment for {productname} {productversion}.

== Requirements

=== Platform

Currently the only tested platform is SUSE OpenStack Cloud.
Nodes

You will need at least two machines:

* 1 Master Node
* 1 Worker Node

CaaS Platform vNext supports deployments with a single or multiple master nodes.
Production environments must be deployed with multiple master nodes for resilience.

=== Hardware

==== Master

* Storage: 50 GB+
* (v)CPU: 2
* RAM: 2 GB
* Network: Minimum 1GB/s, faster preferred

==== Worker

* Storage: Depending on workloads, minimum 20-30GB to hold base OS and required packages
** Mount additional storage volumes as needed
* (v)CPU: 2
* RAM: 2 GB
* Network: Minimum 1GB/s, faster preferred

==== Storage Performance

For Master and Worker nodes you must ensure storage performance of at least 500 sequential IOPS with disk bandwidth depending on your cluster size.

    "Typically 50 sequential IOPS (e.g., a 7200 RPM disk) is required.
    For heavily loaded clusters, 500 sequential IOPS (e.g., a typical local SSD
    or a high performance virtualized block device) is recommended."

    "Typically 10MB/s will recover 100MB data within 15 seconds.
    For large clusters, 100MB/s or higher is suggested for recovering 1GB data
    within 15 seconds."

link:https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md#disks[]

=== Networking Requirements

==== IP Addresses

All nodes must be assigned static IP addresses that must not be changed manually afterwards.
Communication

Please make sure that all your Kubernetes components can communicate with each other.
This might require the configuration of routing when using multiple network adapters per node.

Refer to: https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-network-adapters.

Configure firewall and other network security to allow communication on the default ports required by Kubernetes: https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports

==== Performance

All master nodes of the cluster must have a minimum 1GB/s network connection to fullfill the requirements for etcd.

    "1GbE is sufficient for common etcd deployments. For large etcd clusters,
    a 10GbE network will reduce mean time to recovery."

link:https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md#network[]

==== Security

Do not grant access to the kubeconfig file or any workstation configured with this configuration to unauthorized personnel.
In the current state, full administrative access is granted to the cluster.

Authentication is done via the kubeconfig file generated during deployment. This file will grant full access to the cluster and all workloads.
Apply best practices for access control to workstations configured to administer the SUSE CaaS Platform cluster.

== Setup

You will use terrafrom to deploy the cluster nodes to SUSE OpenStack Cloud and then use the caaspctl tool to bootstrap the cluster.
Node deployment

=== Preparation

You need terraform installed on your workstation and configure access to the SUSE OpenStack Cloud API.

. Install terraform on your local machine.
+
Select the appropriate repository from here: https://build.opensuse.org/repositories/systemsmanagement:terraform
Add the repository to your workstation (here Tumbleweed):
+
----
sudo zypper ar https://download.opensuse.org/repositories/systemsmanagement:/terraform/openSUSE_Tumbleweed/ terraform
----
. Install terraform
+
----
zypper in terraform
----
. Download the SUSE OpenStack Cloud RC file.
.. Log in to SUSE OpenStack Cloud.
.. Click on your username in the upper right hand corner to reveal the dropdown menu.
.. Click on menu:Download OpenStack RC File v3[].
.. Save the file to your workstation.
.. Load the file into your shell environment.
+
----
source container-openrc.sh
----
.. Enter the password for the RC file. This should be same credentials that you use to log in to {soc}.

=== Deploying the cluster nodes

. Clone the repository https://github.com/SUSE/caaspctl/tree/master/ci/infra/openstack
+
----
git clone git@github.com:SUSE/caaspctl.git
----
. Switch to the directory "caaspctl/ci/infra/openstack".
. Create a file terraform.tfvars
+
----
# Name of the internal network to be created
internal_net = "testing"

# identifier to make all your resources unique and avoid clashes with other users of this terraform project
stack_name = "testing"

# instance user name
username = "sles"

# define which image to use
image_name = "SLES15-SP1-JeOS.x86_64-15.1-OpenStack-Cloud-RC1"

# define the repositories to use
repositories = [
  {
    caasp = "http://download.suse.de/ibs/SUSE:/SLE-15-SP1:/Update:/Products:/CASP40/standard"
  },
  {
    sle15sp1_pool = "http://download.suse.de/ibs/SUSE:/SLE-15-SP1:/GA/standard/"
  },
  {
    sle15sp1_update = "http://download.suse.de/ibs/SUSE:/SLE-15-SP1:/Update/standard/"
  },
  {
    sle15_pool = "http://download.suse.de/ibs/SUSE:/SLE-15:/GA/standard/"
  },
  {
    sle15_update = "http://download.suse.de/ibs/SUSE:/SLE-15:/Update/standard/"
  },
  {
    suse_ca = "http://download.suse.de/ibs/SUSE:/CA/SLE_15_SP1/"
  }
]

packages = [
  "ca-certificates-suse",
  "kubernetes-kubeadm",
  "kubernetes-kubelet",
  "kubernetes-client"
]

# ssh keys to inject into all the nodes
authorized_keys = [
  ""
]

# Number of master and worker nodes to be created
masters = 1
workers = 1
----

. Replace `internal_net` and `stack_name` with your desired name for the cluster. This string will be used to generate the human readable IDs in {soc}.
If you use a generic term it is very likely to fail deployment because the term is already in use by someone else. It's a good idea to use your username or other unique identifier.
. Insert your public SSH key into the "authorized keys" variable. This key will be distributed to all machines and is your login credentials for all cluster nodes.
. You can adjust the size of the deployed cluster by changing the "masters" and "workers" variables.
. You can set timezone in before deploying the nodes by modifying the files:
+
* caaspctl/ci/infra/openstack/cloud-init/master.tpl
* caaspctl/ci/infra/openstack/cloud-init/worker.tpl
. Now you can deploy the nodes by running:
----
terraform init
terraform apply
----

Check the output for the actions to be taken. Type "yes" and confirm with Enter when ready.
Terraform will now provision all the machines and network infrastructure for the cluster.

.Note down IP/FQDN for nodes
[IMPORTANT]
====
The IP addresses of the generated machines will be displayed in the terraform
output during the cluster node deployment. You need these IP addresses to
deploy {productname} to the cluster.

If you need to find an IP addresses later on you can perform the following steps:

. Log in to {soc} and click on menu:Network[Load Balancers]. Find the one with the string you entered in the terraform configuration above e.g. "testing-lb".
. Note down the "Floating IP". If you have configured a FQDN for this IP, use the hostname instead.
+
image::deploy-loadbalancer-ip.png[]
. Now click on menu:Compute[Instances].
. Switch the filter dropdown to `Instance Name` and enter the string you specified for `stack_name` in the `terraform.tfvars` file.
. Find the Floating IPs on each of the nodes of your cluster.
====

[NOTE]
====
Please note that we provide some internal repositories with the terraform configuration that are not usually available to customers.
In future versions the customer must enable two modules:

* sle-module-server-applications/15.1/x86_64
and
* sle-module-containers/15.1/x86_64

You must log in to every node via SSH and perform:
----
SUSEConnect -p sle-module-server-applications/15.1/x86_64
SUSEConnect -p sle-module-containers/15.1/x86_64
----
====

=== Bootstrapping the cluster

==== Preparation

===== Install caaspctl

First you need to install caaspctl on your local workstation:

. Add the SLE15 SP1 repository containing caaspctl.
+
----
sudo zypper ar http://download.suse.de/ibs/SUSE:/SLE-15-SP1:/Update:/Products:/CASP40/standard/SUSE:SLE-15-SP1:Update:Products:CASP40 caasp
----
. Install caaspctl:
+
----
sudo zypper in caaspctl
----

==== Cluster Deployment

Make sure you have added the SSH identity (corresponding to the SSH key distributed above) to the ssh-agent on your workstation. This is a requirement for caaspctl (https://github.com/SUSE/caaspctl#prerequisites)

. Now you can initialize the cluster on the deployed machines.
As `--control-plane` enter the IP/FQDN of your loadbalancer.
+
----
caaspctl cluster init --control-plane <LB IP/FQDN> my-cluster
----
`cluster init` generates the folder named `my-cluster` and initializes the directory that will hold the configuration for the cluster.
. Switch to the new directory.
. Now bootstrap a master node.
For `--target` enter the IP address of your first master node.
Replace `<NODE NAME>` with a unique identifier for example "master-one".
+
[WARNING]
====
The directory created during this step contains configuration files that allow full administrator access to your cluster. Apply best practices for access control to this folder.
====
+
----
cd my-cluster
caaspctl node bootstrap  --user sles --sudo --target <IP/FQDN> <NODE NAME>
----
This will bootstrap the specified node as the first master in the cluster.
The process will generate authentication certificates and the `admin.conf` file that is used for authentication against the cluster.
The files will be stored in the `my-cluster` directory specified in step one.
. Add additional master nodes to the cluster.
+
Replace the `<IP/FQDN>` with the IP for the machine.
Replace `<NODE NAME>` with a unique identifier for example "master-two".
+
----
caaspctl node join --role master --user sles --sudo --target <IP/FQDN> <NODE NAME>
----
. Add a worker to the cluster.
+
Replace the `<IP/FQDN>` with the IP for the machine.
Replace `<NODE NAME>` with a unique identifier for example "worker-one".
+
----
caaspctl node join --role master --user sles --sudo --target <IP/FQDN> <NODE NAME>
----
. Verify the nodes that you added
+
----
caaspctl cluster status
----
+
The output should look like this:
+
----
NAME         OS-IMAGE                              KERNEL-VERSION        CONTAINER-RUNTIME   HAS-UPDATES   HAS-DISRUPTIVE-UPDATES
master-one   SUSE Linux Enterprise Server 15 SP1   4.12.14-110-default   cri-o://1.13.3      <none>        <none>
worker-one   SUSE Linux Enterprise Server 15 SP1   4.12.14-110-default   cri-o://1.13.3      <none>        <none>
----

==== Using kubectl

You can install and use kubectl by installing the kubernetes-client package from  http://download.suse.de/ibs/SUSE:/SLE-15-SP1:/Update:/Products:/CASP40/standard/.

[TIP]
====
Alternatively you can install from upstream: https://kubernetes.io/docs/tasks/tools/install-kubectl/.
====

To talk to your cluster, simply symlink the generated configuration file to `~/.kube/config`.

[source,bash]
----
ln -s ~/clusters/my-cluster/admin.conf ~/.kube/config
----

Then you can perform all cluster operations as usual. For example checking cluster status with either:

* `kubectl get nodes -o wide`
+
or
* `kubectl caasp cluster status`
+
or
* `kubectl get pods --all-namespaces`
+
[source,bash]
----
# kubectl get pods --all-namespaces

NAMESPACE     NAME                                READY     STATUS    RESTARTS   AGE
kube-system   coredns-86c58d9df4-5zftb            1/1       Running   0          2m
kube-system   coredns-86c58d9df4-fct4m            1/1       Running   0          2m
kube-system   etcd-my-master                      1/1       Running   0          1m
kube-system   kube-apiserver-my-master            1/1       Running   0          1m
kube-system   kube-controller-manager-my-master   1/1       Running   0          1m
kube-system   kube-flannel-ds-amd64-b6krs         1/1       Running   0          53s
kube-system   kube-flannel-ds-amd64-v7kt7         1/1       Running   0          2m
kube-system   kube-proxy-5qxnt                    1/1       Running   0          2m
kube-system   kube-proxy-746ws                    1/1       Running   0          53s
kube-system   kube-scheduler-my-master            1/1       Running   0          1m
----
