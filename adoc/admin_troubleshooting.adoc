[[_cha.admin.troubleshooting]]
= Troubleshooting
:doctype: book
:sectnums:
:toc: left
:icons: font
:experimental:
:sourcedir: .
:imagesdir: ./images

This chapter summarizes frequent problems that can occur while using {productname}
and their solutions.

[[_sec.admin.troubleshooting.overview]]
== Overview


This chapter is a collection of frequent problems that are reported for {productname}
.
Additionally, {suse}
support collects problems and their solutions online at https://www.suse.com/support/kb/?id=SUSE_CaaS_Platform.

In case of problems, a detailed system report can be created with the [command]``supportconfig`` command line tool.
It will collect information about the system such as: current kernel version, hardware, installed packages, partition setup, and much more.
The result is a TAR archive of files.
After opening a Service Request (SR), you can upload the TAR archive to Global Technical Support.
It will help to locate the issue you reported and to assist you in solving the problem.
For details, see https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_adm_support.html.

While resolving a technical difficulty with SUSE Support, you may receive a so-called Program Temporary Fix (PTF). PTFs may be issued for various packages as RPMs.
For details about handling PTFs, see <<_sec.admin.software.patch>>.

For details about how to contact {suse}
support, refer to https://www.suse.com/media/flyer/suse_customer_support_quick_reference_guide_flyer.pdf and https://www.suse.com/support/.

[[_sec.admin.troubleshooting.node_acceptance]]
== New Nodes Not Showing In {dashboard}


All {productname}
nodes must have a [path]``/etc/salt/minion.d/master.conf``
 configuration file that contains the IP address or the FQDN of (one of) the Salt master(s) in the cluster.
During the installation you must provide the IP/FQDN of the Salt master (typically the admin node).

To become part of the cluster, the node must be (manually) accepted. {dashboard}
shows a list of Salt minions that are known to the Salt master and can be accepted.

=== Nodes Not Showing In "Pending Nodes"


You have added a new machine to the cluster but there is no new node key displayed in the menu:Pending Nodes[]
 list.

* Check that the configured value in the [path]``master.conf`` of the machine is correct and it is communicating with the admin node.
* Check if the {admin_node} has enough free memory. In some situations {kube} will intermittently shut down containers to conserve resources. If any of ``salt-master``, `salt-api` or other critical services are affected, this could lead to this kind of issues.


=== Accepted Nodes Not Showing In "Nodes" List


If the accepted node does not show up in the menu:Nodes[]
list, you can log in to the {admin_node}
 via SSH and perform the following checks:

* Check that the minion ID has been actually accepted:
+

----
{prompt.user}``docker exec -it $(docker ps --filter=name=salt-master -q) salt-key`` Accepted Keys:
admin
ca
231bb41553ac4102bb8fda8b2fd9d60d
Denied Keys:
Unaccepted Keys:
bbaf54654d5649f68a0af5f1914ecedf
db79f30700974971b5e32d62ba0d5d76
Rejected Keys:
----
+
The minion ID of the node should appear on the "Accepted Keys" list.
+
If the node does not appear on this "Accepted Keys" list, the acceptance request didn't reach the backend and/or the Salt API.
Check with [command]``docker ps`` that containers on the {admin_node}
have approximately the same age.
If there is a large age differences, it's likely that the {admin_node}
is under memory pressure and is shutting down containers.
This can lead to multiple problems respective of which container is not available.
For example: MariaDB could have been down when Salt tried to insert events into the database etc..
+
Adjust workloads accordingly to restore available memory and retry accepting the node from {dashboard}
.
* Check that the event processor is running on the {admin_node} :
+

----
{prompt.user}``docker ps -q -f name=event-processor``
----
+
If it's not running or has a significantly different age from the other containers, make sure the admin node is not under memory pressure.
* Check if the event processor is lagging behind processing events:
+
If there was a previous problem with the event processor, it's possible that salt continued creating events while it wasn't running for a long time, causing it to lag behind processing events.
Check the number of events in the processing queue.
+

----
{prompt.user}``docker exec -it $(docker ps -q -f name=dashboard) \
entrypoint.sh bundle exec rails runner \
'puts "Number of events to process: #{SaltEvent.not_processed.count}"'``
----
* Check if Salt couldn't introduce an important event on the database
+
Make sure there's enough free space on the {admin_node}
.


[[_sec.admin.troubleshooting.failed_bootstrap]]
== Debugging Failed Bootstrap


If the bootstrapping as described in <<_sec.deploy.install.bootstrap>> fails, there are several places to look for errors.
The following procedure outlines where to start debugging.


. Check the logs from Velum dashboard. Execute on the {admin_node} :
+

----
{prompt.root.admin}``docker logs $(docker ps -q -f name="velum-dashboard")``
----
. Convert the log into a human readable format and open it with [command]``less``.
+

----
{prompt.root.admin}``/var/lib/supportutils-plugin-suse-caasp/debug-salt --json_output=salt.json \
    --summary_output=summary.txt --text-status --no-color`` {prompt.root.admin}``less summary.txt``
----
. Retry bootstrapping from {admin_node} console.
+

----
{prompt.root.admin}``docker exec -it $(docker ps -q -f name="salt-master") \
    salt-run -l debug state.orchestrate orch.kubernetes > salt-orchestration.logs``
----
. If the orchestration failed in late state, you can check if [command]``etcd`` and the {kube} cluster is up and running. Log in on a {master_node} and check if [command]``etcd`` cluster is up and running.
+

----
{prompt.root.master}``set -a`` {prompt.root.master}``source /etc/sysconfig/etcdctl`` {prompt.root.master}``export ETCDCTL_API=3`` {prompt.root.master}``etcdctl member list`` {prompt.root.master}``etcdctl endpoint health`` {prompt.root.master}``etcdctl endpoint status`` {prompt.root.master}``journalctl -u etcd``
----
+
Check if {kube}
cluster is up and running.
+

----
{prompt.root.master}``kubectl get nodes`` {prompt.root.master}``kubectl cluster-info`` {prompt.root.master}``journalctl -u kubelet`` {prompt.root.master}``journalctl -u kube-apiserver`` {prompt.root.master}``journalctl -u kube-scheduler`` {prompt.root.master}``journalctl -u kube-controller-manager``
----
. Collect all relevant logs with [command]``supportconfig`` on the {master_node} and {admin_node} .
+

----
{prompt.root.admin}``supportconfig`` {prompt.root.admin}``tar -xvjf /var/log/nts_*.tbz`` {prompt.root.admin}``cd nts*`` {prompt.root.admin}``cat etcd.txt kubernetes.txt salt-minion.txt``
----
+

----
{prompt.root.master}``supportconfig`` {prompt.root.master}``tar -xvjf /var/log/nts_*.tbz`` {prompt.root.master}``cd nts*`` {prompt.root.master}``cat etcd.txt kubernetes.txt salt-minion.txt``
----


[[_sec.admin.troubleshooting.failed_update]]
== Recovering From Failed Update


See <<_sec.admin.software.transactional_updates.recovering>>.

[[_sec.admin.troubleshooting.etcd]]
== Checking `etcd` Health


To work with [command]``etcdctl``, you have to pass parameters with paths to required certificates.

Use SSH to log into one of the cluster nodes, for example your first master.
The following command provides an example for using [command]``etcdctl``.

----
{prompt.root}``set -a`` {prompt.root}``source /etc/sysconfig/etcdctl`` {prompt.root}``export ETCDCTL_API=3`` {prompt.root}``etcdctl cluster-health``
----

[[_sec.admin.troubleshooting.velum_ptf]]
== Locking Installed Program Temporary Fixes


It can happen that [command]``zypper`` removes an installed _Program Temporary Fix_ (__PTF__) when updates are started with Velum.
To prevent this, execute the following procedure.

.Locks Disable Updates for Package
[IMPORTANT]
====
If a package is locked, it will no longer receive any updates until the lock is released with [command]``zypper rl``.
====


. Install the PTF manually.
+

----
{prompt.root}``transactional-update reboot pkg install PTF_FILE.rpm``
----
. As soon as the node has restarted, verify that the RPM is installed.
+

----
{prompt.root}``rpm -qa | grep PTF``
----
. Create a [command]``zypper`` lock for this RPM.
+

----
{prompt.root}``zypper al PTF_PACKAGE_NAME``
----
. Verify that the lock is created.
+

----
{prompt.root}``zypper ll``
----


[[_sec.admin.troubleshooting.network_planning]]
== Network Planning And Reconfiguration


It is not possible to reconfigure the used IP ranges of {productname}
once the deployment is complete.
Therefore, carefully plan for required IP ranges and future scenarios.

Additionally, keep the network requirements in mind, as stated in <<_sec.deploy.requirements.network>>.

[[_sec.admin.troubleshooting.proxy]]
== Using A Proxy Server With Authentication


If you need to register the {admin_node}
with {scc}
over a proxy server that requires authentication, it is not possible to specify the configuration in [path]``/etc/sysconfig/proxy``
.

For information about setting authentication credentials and connecting to {scc}
with [command]``SUSEConnect``, see <<_sec.configuration.suseconnect.proxy>>.

[[_sec.admin.troubleshooting.replace_certificates]]
== Replacing TLS/SSL Certificates


Sometimes certificates are not updated properly because they are outdated.
To replace outdated certificates, execute the following procedure.


. Use SSH  and log in on the {admin_node} .
. Move the expired certs out of the way.
+

----
{prompt.root.admin}``mv /etc/pki/{velum,ldap,salt-api}.crt /root``
----
. Generate new certificates.
+

----
{prompt.root.admin}``cd /etc/pki`` {prompt.root.admin}``/usr/share/caasp-container-manifests/gen-certs.sh``
----
+
.Generating Additional Certificates
TIP: To regenerate additional certificates, for example [path]``/etc/pki/kubectl-client-cert.crt``
, add an additional line at the end of the [command]``gen-certs.sh`` script:

----
{prompt.root.admin}``transactional-update shell``
transactional update # ``echo "gencert \"kubectl-client-cert\" \"kubectl-client-cert\" \
    \"\$all_hostnames\" \"\$(ip_addresses)\"" >>/usr/share/caasp-container-manifests/gen-certs.sh``
transactional update # ``/usr/share/caasp-container-manifests/gen-certs.sh``
transactional update # ``exit``
----
+

. Use SSH and log in on a {master_node} .
. Backup and delete the dex-tls secret.
+

----
{prompt.root.master}``kubectl -n kube-system get secret dex-tls -o yaml > /root/dex-tls`` {prompt.root.master}``kubectl -n kube-system delete secret dex-tls``
----
. On a master node, find and delete the Dex pods.
+
.This Step Breaks Authentication
IMPORTANT: Executing this step prevents new authentications requests from succeeding.
However, the static credentials located on the master nodes will continue to function.

The Dex pods will not restart by themselves until the dex-tls secret is recreated.
+


+

----
{prompt.root.master}``kubectl -n kube-system get pods | grep dex`` {prompt.root.master}``kubectl -n kube-system delete pods DEX_POD1 DEX_POD2 DEX_POD3``
----
. Manually run the salt orchestration on the {admin_node} . This may take some time.
+

----
{prompt.root.admin}``docker exec -it $(docker ps -q -f name="salt-master") \
    bash -c "salt-run state.orchestrate orch.kubernetes" 2&>1 > salt-run.log``
----
. Check the tail of [path]``salt-run.log`` to see if the orchestration succeeded.
+

----
{prompt.root.admin}``tail -n 50 salt-run.log``
----
. On a master node, validate the dex pods are running.
+

----
{prompt.root.master}``kubectl -n kube-system get pods | grep dex``
----
. If you are not able to log in into Velum, reboot the {admin_node} . Then test and validate that the cluster is still functional.


[[_sec.admin.troubleshooting.add_ca_cert]]
== Fixing "`x509: certificate signed by unknown authority`"


When interacting with {kube}
you can run into the situation where your existing configuration for the authentication has changed (cluster has been rebuild, certificates have been switched.)

In such a case you might see an error message in the output of your CLI tool.

----
x509: certificate signed by unknown authority
----


This message indicates that your current system does not know the Certificate Authority (CA) that signed the SSL certificates used for encrypting the communication to the cluster.
You then need to add or update the Root CA certificate in your local trust store.

For details about installing the root CA certificate, see <<_sec.admin.security.certs.installing_rootca>>.

[[_sec.admin.troubleshooting.lost_node]]
== Replacing a Lost Node


If your cluster loses a node, for example due to failed hardware, remove the node as explained in <<_sec.admin.nodes.remove>>.
Then add a new node as described in <<_sec.admin.nodes.add>>.
