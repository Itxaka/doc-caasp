[[_cha.admin.integration]]
= {ses} Integration
:doctype: book
:sectnums:
:toc: left
:icons: font
:experimental:
:sourcedir: .
:imagesdir: ./images
= SUSE Enterprise Storage Integration
:doctype: book
:sectnums:
:toc: left
:icons: font
:experimental:
:imagesdir: ./images

{productname}
can use another {suse}
product as storage for containers{mdash}{ses}
(henceforth called SES). This chapter gives details on several ways to integrate these two products. 

[[_sec.admin.integration.prerequisites]]
== Prerequisites


Before you start the integration process, you need to ensure the following: 

* The {productname} cluster can communicate with the SES nodes{mdash} master, monitoring nodes, OSD nodes and the metadata server, in case you need a shared file system. For more details regarding SES refer to the SES documentation, here: https://www.suse.com/documentation/suse-enterprise-storage/. 
* The SES cluster has a pool with RADOS Block Device (RBD) enabled. 


[[_sec.admin.integration.mounting_named_object_storage]]
== Using RBD in a Pod


The procedure below describes steps to take when you need to use a RADOS Block Device in a Pod. 

.Procedure: Using RBD In A Pod
. Retrieve the Ceph admin secret. Get the key value from the file [path]``/etc/ceph/ceph.client.admin.keyring`` . 
. On the {master_node} apply the configuration that includes the Ceph secret by using `kubectl apply`. Replace `CEPH_SECRET` with your Ceph secret. 
+

----
{prompt.user}``kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
type: "kubernetes.io/rbd"
data:
  key: "$(echo CEPH_SECRET | base64)"
*EOF*`` 
----
. Create an image in the SES cluster. On the {master_node} , run the following command: 
+

----
{prompt.root}``rbd create -s SIZE YOUR_VOLUME`` 
----
+
Replace `SIZE` with the size of the image, for example ``1G``, and `YOUR_VOLUME` is the name of the image. 
. Create a pod that uses the image by executing the following command on the {master_node} . In this example it is a minimal configuration for using a `RADOS` Block Device. Fill in the IP addresses and ports of your monitor nodes. The port number usually is ``6789``. 
+

----
{prompt.user}``kubectl apply -f - << *EOF*
apiVersion: v1
kind: Pod
metadata:
  name: POD_NAME
spec:
  containers:
  - name: CONTAINER_NAME
    image: IMAGE_NAME
    volumeMounts:
    - mountPath: /mnt/rbdvol
      name: rbdvol
  volumes:
  - name: rbdvol
    rbd:
      monitors:
      - 'MONITOR1_IP:MONITOR1_PORT'
      - 'MONITOR2_IP:MONITOR2_PORT'
      - 'MONITOR3_IP:MONITOR3_PORT'
      pool: rbd
      image: YOUR_VOLUME
      user: admin
      secretRef:
        name: ceph-secret
      fsType: ext4
      readOnly: false
*EOF*`` 
----
. Verify that the pod exists and its status: 
+

----
{prompt.user}``kubectl get po`` 
----
. Once the pod is running, check the mounted volume: 
+

----
{prompt.user}``kubectl exec -it CONTAINER_NAME -- df -k`` ...
/dev/rbd1               999320      1284    929224   0% /mnt/rbdvol
...
----


In case you need to delete the pod, run the following command on the {master_node}
: 

----
{prompt.user}``kubectl delete pod POD_NAME`` 
----

[[_sec.admin.integration.persistent_volumes]]
== Using RBD in Persistent Volume


The following procedure describes how to attach a pod to a persistent SES volume. 

.Procedure: Creating a Pod with RBD in Persistent Volume
. Retrieve the Ceph admin secret. Get the key value from the file [path]``/etc/ceph/ceph.client.admin.keyring`` . 
. On the {master_node} , apply the configuration that includes the Ceph secret by using `kubectl apply`. Replace `CEPH_SECRET` with your Ceph secret. 
+

----
{prompt.user}``kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
type: "kubernetes.io/rbd"
data:
  key: "$(echo CEPH_SECRET | base64)"
*EOF*`` 
----
. Create a volume on the SES cluster: 
+

----
{prompt.root}``rbd create -s SIZE YOUR_VOLUME`` 
----
+
Replace `SIZE` with the size of the image, for example `1G` (1 Gigabyte), and `YOUR_VOLUME` is the name of the image. 
. Create the persistent volume on the {master_node} : 
+

----
{prompt.user}``kubectl apply -f - << *EOF*
apiVersion: v1
kind: PersistentVolume
metadata:
  name: PV_NAME
spec:
  capacity:
    storage: SIZE
  accessModes:
    - ReadWriteOnce
  rbd:
    monitors:
    - 'MONITOR1_IP:MONITOR1_PORT'
    - 'MONITOR2_IP:MONITOR2_PORT'
    - 'MONITOR3_IP:MONITOR3_PORT'
    pool: rbd
    image: YOUR_VOLUME
    user: admin
    secretRef:
      name: ceph-secret
    fsType: ext4
    readOnly: false
*EOF*`` 
----
+
Replace `SIZE` with the desired size of the volume.
Use the _gibibit_ notation, for example ``1Gi``. 
. Create a persistent volume claim on the {master_node} : 
+

----
{prompt.user}``kubectl apply -f - << *EOF*
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: PV_NAME
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: SIZE
*EOF*`` 
----
+
Replace `SIZE` with the desired size of the volume.
Use the _gibibit_ notation, for example ``1Gi``. 
+
.Listing Volumes
NOTE: This persistent volume claim does not explicitly list the volume.
Persistent volume claims work by picking any volume that meets the criteria from a pool.
In this case we specified any volume with a size of 1G or larger.
When the claim is removed the recycling policy will be followed. 
+

. Create a pod that uses the persistent volume claim. On the {master_node} run the following: 
+

----
{prompt.user}``kubectl apply -f - <<*EOF*
apiVersion: v1
kind: Pod
metadata:
  name: POD_NAME
spec:
  containers:
  - name: CONTAINER_NAME
    image: IMAGE_NAME
    volumeMounts:
    - mountPath: /mnt/rbdvol
      name: rbdvol
  volumes:
  - name: rbdvol
    persistentVolumeClaim:
      claimName: PV_NAME
*EOF*`` 
----
. Verify that the pod exists and its status. On the {master_node} run: 
+

----
{prompt.user}``kubectl get po`` 
----
. Once pod is running, check the volume by running on the {master_node} : 
+

----
{prompt.user}``kubectl exec -it CONTAINER_NAME -- df -k`` ...
/dev/rbd3               999320      1284    929224   0% /mnt/rbdvol
...
----


In case you need to delete the pod, run the following command on the {master_node}
: 

----
{prompt.user}``kubectl delete pod CONTAINER_NAME`` 
----


And when the command finishes, run 

----
{prompt.user}``kubectl delete persistentvolume PV_NAME`` 
----

.Deleting A Pod
[NOTE]
====
When you delete the pod, the persistent volume claim is deleted as well.
The RBD is not deleted. 
====