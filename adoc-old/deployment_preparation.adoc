[[_cha.deployment.preparation]]
= Preparing The Installation
:doctype: book
:sectnums:
:toc: left
:icons: font
:experimental:
:sourcedir: .
:imagesdir: ./images
= Preparing The Installation
:doctype: book
:sectnums:
:toc: left
:icons: font
:experimental:
:imagesdir: ./images


This chapter prepares the installation of {productname}
on physical machines, manually configured virtual machines or in private cloud environments. 

There are several ways to install {productname}
: 

<<_sec.deploy.preparation.dvd>>::
This method requires lots of manual interaction with the physical or virtual machines.
This method is suitable for small to medium cluster sizes. 

<<_sec.deploy.preparation.disk_images>>::
You can use prepared disk images ready to deploy on virtual machines.
This increases the deployment speed. 

<<_sec.deploy.preparation.pxe>>::
With this method the boot into the installation is done with a network installation server.
This reduces the need to interact with every single node.
This method is suitable for medium cluster sizes. 

<<_sec.deploy.preparation.openstack>>::
You can deploy {productname}
on {soc}
.
This method is suitable for large cluster sizes. 

<<_sec.deploy.preparation.public_cloud>>::
This section is about installing {productname}
in a public cloud, for example __Microsoft Azure__*, __Amazon AWS__* and __Google Compute
Engine__*. 


To customize the setup of the nodes, use `cloud-init`.
For details, see <<_sec.deploy.cloud_init>>. 

[[_sec.deploy.preparation.dvd]]
== Installing From USB, DVD Or ISO Images


This procedure provides an overview of the steps for the cluster deployment with classical boot devices like DVD drives.
This method is suitable for small to medium cluster sizes. 


. Choose an installation medium. You can install from DVD or USB sticks. On virtual machines, you can install from ISO images. 
. Boot the machine designated for the {admin_node} from your selected medium. Then follow the installation and configuration instructions detailed in <<_sec.deploy.nodes.admin_install>> and <<_sec.deploy.nodes.admin_configuration>>. 
. Boot the machines designated to be the Master and Worker Nodes from your selected medium. Then follow the installation and configuration instructions detailed in <<_sec.deploy.nodes.worker_install>>. To automate the installation, you can use an {ay} file provided by the {admin_node} . We recommend to use {ay} to speed up the node deployment. For details, see <<_sec.deploy.nodes.worker_install.manual.autoyast>>. 
. Finish the installation by bootstrapping the cluster. For details, see <<_sec.deploy.install.bootstrap>>. 
. For {vmware} ESX/ESXi: install the [package]#open-vm-tools# package. See <<_sec.deploy.install.vmware_tools>>. 


[[_sec.deploy.preparation.disk_images]]
== Installing From Virtual Disk Images


For building clusters from virtual machines on supported hypervisors, it is not necessary to individually install each node. {suse}
offers pre-installed VM disk images in the following formats: 

{kvm} and {xen}::
In QCOW2 format, for {kvm}
and for {xen}
using full virtualization. 

{xen}::
In QCOW2 format, for {xen}
using paravirtualization. 

{vmware}::
In VMDK format, for {vmware}
ESXi. 
 VHD::
For Microsoft Hyper-V.


[[_sec.deploy.preparation.disk_images.overview]]
=== Overview


When deploying a cluster node from pre-installed disk images, the setup program never runs.
Therefore, the configuration must happen while the node is starting up.
For this purpose, {productname}
includes `cloud-init`.
The described process is very similar for all hypervisors.
The examples in this manual use {kvm}
 running on {sle}
.
The following procedure outlines the overall process. 


. For {vmware} ESX/ESXi: Convert VMDK images. See <<_sec.deploy.preparation.disk_images.vmware>>. 
. Write `cloud-init` configuration files. See <<_sec.deploy.preparation.disk_images.configuration>>. 
. Prepare ISO images containing the `cloud-init` configuration files. See <<_sec.deploy.preparation.disk_images.iso_image>>. 
. Create a copy of the downloaded disk image for each virtual machine, naming them appropriately: for example, ``caas-admin``, ``caas-master``, ``caas-worker1``, `caas-worker2` and so on. 
. Boot and configure the {admin_node} . See <<_sec.deploy.preparation.qcow2.admin>>. 
. Boot and configure the worker nodes. See <<_sec.deploy.preparation.disk_images.worker>>. 


[[_sec.deploy.preparation.disk_images.vmware]]
=== Converting Images For {vmware} ESX and ESXi


Downloaded disk images need to be converted for usage with {vmware}
ESX and ESXi.
On the ESX/ESXi host, run the following command on the downloaded disk images: 

----
{prompt.root}``vmkfstools -i DOWNLOADED_IMAGE.vmdk CONVERTED_IMAGE.vmdk`` 
----

[[_sec.deploy.preparation.disk_images.configuration]]
=== Configuration Files


There are two separate configuration files: [path]``user-data``
 and [path]``meta-data``
.
Each node needs both.
Thus, you need to prepare at a minimum one pair of files for the {admin_node}
, and another pair of files for all the {worker_node}
s. 

Place the files into subdirectories named [path]``cc-admin``
 for the {admin_node}
 and [path]``cc-worker``
 for the {worker_node}
s. 

So, for instance, if your working directory is [path]``~/cloud-config``
, then for the admin node, you need these two files: 

----
~/cloud-config/cc-admin/user-data
~/cloud-config/cc-admin/meta-data
----


For a {worker_node}
, you need: 

----
~/cloud-config/cc-worker/user-data
~/cloud-config/cc-worker/meta-data
----


The same [path]``meta-data``
 file can be used for both node types.
Here is an sample [path]``meta-data``
 file: 

----
#cloud-config
instance-id:`iid-CAAS01`network-interfaces: |
   auto`eth0`iface`eth0`inet dhcp
----


The [path]``user-data``
 file contains settings such as time servers, the {rootuser}
 password, and the node type. 

Here is an example [path]``cc-admin/user-data``
 file for an {admin_node}
: 

----
#cloud-config
debug: True
disable_root: False
ssh_deletekeys: False
ssh_pwauth: True
chpasswd:
   list: |
     root:`MY_PASSWORD`expire: False
ntp:
   servers:
     -`ntp1.example.com`-`ntp2.example.com`runcmd:
   - /usr/bin/systemctl enable --now ntpd
suse_caasp:
  role:`admin`
----


Here is an example [path]``cc-worker/user-data``
  for a {worker_node}
.
Rather than providing the {rootuser}
 password in clear text, you can use a hash instead; this example is hashed with SHA-256. 

----
#cloud-config
debug: True
disable_root: False
ssh_deletekeys: False
ssh_pwauth: True
chpasswd:
   list: |
   root:`$5$eriogqzq$Dg7PxHsKGzziuEGkZgkLvacjuEFeljJ.rLf.hZqKQLA`expire: False
suse_caasp:
    role:`cluster`admin_node:`caas-admin.example.com`
----


For more informatin, also refer to <<_sec.deploy.cloud_init>>. 

[[_sec.deploy.preparation.disk_images.iso_image]]
=== Preparing An ISO image


First, edit the configuration files as described in <<_sec.deploy.preparation.disk_images.configuration>>.
Then create an ISO image with the volume label `cidata` containing only the subdirectory for that node type. 

On {sle}
12 or {opensuse}
Leap 42, use `genisoimage` to do this.
On {sle}
 or {opensuse}
 Leap 15, use `mkisofs`.
The parameters are the same for both commands. 

For example, to create the ISO image for an admin node on a computer running {opensuse}
Leap 42: 

----
{prompt.user}``sudo genisoimage -output cc-admin.iso -volid cidata -joliet -rock cc-admin`` 
----


To create the ISO image for a worker node on a computer running {opensuse}
15, substituting the name of the folder containing the configuration files for a {worker_node}
and titling the volume ``cc-worker``: 

----
{prompt.user}``sudo mkisofs -output cc-worker.iso -volid cidata -joliet -rock cc-worker`` 
----

[[_sec.deploy.preparation.qcow2.admin]]
=== Bringing Up An {Admin_Node}


. Create a new VM for the {admin_node} . 
. Attach a copy of the downloaded disk image as its main hard disk. 
. Attach the [path]``cc-admin.iso`` image as a virtual DVD unit. For details about preparing the image, see <<_sec.deploy.preparation.disk_images.iso_image>>
. Start the VM. 
. Configure the new {admin_node} as in step <<_sec.deploy.nodes.admin_configuration>>. 


[[_sec.deploy.preparation.disk_images.worker]]
=== Bringing Up A {Worker_Node}


Then, repeat the following steps for each {worker_node}
: 


. Create a new VM for the {worker_node} . 
. Attach a copy of the downloaded disk image as its main hard disk. 
. Attach the [path]``cc-worker.iso`` disk image as a virtual DVD unit. For details about preparing the image, see <<_sec.deploy.preparation.disk_images.iso_image>>. The ISO image can be reused for multiple {worker_node} s. 
. Start the VM. 


Once you have brought up as many {worker_node}
s as you need, proceed to bootstap the cluster using the {dashboard}
dashboard. 

[[_sec.deploy.preparation.pxe]]
== Installing From Network Source


This procedure provides an overview of the steps for the cluster deployment from an network installation server.
A PXE environment sis used to provide the nodes with the data required for installation. 


. Install an installation server that provides a DHCP, PXE and TFTP service. Additionally, you can provide the installation data on an HTTP or FTP server. For details, refer to the {sle} 12 Deployment Guide: https://www.suse.com/documentation/sles-12/singlehtml/book_sle_deployment/book_sle_deployment.html#cha.deployment.prep_boot. 
+ 
You can directly use the [path]``initrd``
and [path]``linux``
files from your installation media, or install the package [package]#tftpboot-installation-CAASP-3.0#
onto your TFTP server.
The package provides the required [path]``initrd``
and [path]``linux``
files in the [path]``/srv/tftpboot/``
directory.
You will need to modify the paths used in the {sle}
12 Deployment Guide to correctly point to the files provided by the package. 
. PXE boot the machine designated for the {admin_node} . Then follow the installation and configuration instructions detailed in <<_sec.deploy.nodes.admin_install>> and <<_sec.deploy.nodes.admin_configuration>>. 
. Pxe boot the machines designated to be the Master and Worker Nodes. Then follow the installation and configuration instructions detailed in <<_sec.deploy.nodes.worker_install>>. To automate the installation, you can use an {ay} file provided by the {admin_node} . For details, see <<_sec.deploy.nodes.worker_install.manual.autoyast>>. 
. Finish the installation by bootstrapping the cluster. For details, see <<_sec.deploy.install.bootstrap>>. 
. For {vmware} ESX/ESXi: install the [package]#open-vm-tools# package. See <<_sec.deploy.install.vmware_tools>>. 


[[_sec.deploy.preparation.openstack]]
== Installing In {soc}


You can deploy a {productname}
on {soc}
using {ostack}
.
You will need a {productname}
machine image and {ostack}
Heat templates.
Once you have created a stack, you will continue with the {productname}
setup. 

.{productname}Machine Image For {soc}
[NOTE]
====
Download the latest {productname}
for {ostack}
image from https://download.suse.com (for example, [path]``SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2``
). 
====

.{ostack}Heat Templates Repository 
[NOTE]
====
{productname}
Heat templates are available from https://github.com/SUSE/caasp-openstack-heat-templates[GitHub]. 
====

[[_sec.deploy.preparation.openstack.horizon]]
=== Using The Horizon Dashboard


. Go to menu:Project → Compute → Images[] and click on menu:Create Image[] . 
+ 
Give your image a name (for example: ``CaaSP-3``); you will need to use this later to find the image. 
. Go to menu:Project → Orchestration[] and click on menu:Stacks[] . 
+


image::horizon_stacks.png[scaledwidth=100%]
. Click on menu:Launch Stack[] and provide the stack templates. Either upload the files, provide the URL to the raw files directly (only applies to stack template), or copy and paste the contents into the menu:Direct Input[] fields. 
+
.Replace The Default `root_password`
WARNING: Do not use the [path]``caasp-environment.yaml``
 directly from the GitHub repository. 

You must make sure to replace the value for `root_password` with a secure password.
This will become the password for the {rootuser}
 account on all nodes in the stack. 
+


+


image::horizon_launch_stack.png[scaledwidth=100%]
. Click menu:Next[] . 
. Now you need to define more information about the stack. 
+

`Stack Name`:::
Give your stack a name 

`Password`:::
Your {soc}
password 

`Image`:::
Select the image your machines will be created from 

`root_password`:::
Set the root password for your cluster machines 

``admin``/master/worker_flavor:::
Select the machine flavor for your nodes 

`worker_count`:::
Number of worker nodes to be launched 

`external_net`:::
Select an external network that your cluster will be reachable from 

`internal_net_cidr`:::
The internal network range to be used inside the cluster 

`dns_nameserver`:::
Internal name server for the cluster 

+


image::horizon_stack_options.png[scaledwidth=100%]
. Click menu:Launch[] . 
. After the cluster has been started and the cluster overview shows menu:Create Complete[] , you need to find the external IP address for the admin node of your cluster (here: ``192.168.1.72``). Now visit that IP address in your browser. You should see the {dashboard} login page and can continue with <<_sec.deploy.nodes.admin_configuration>>. 
+


image::horizon_stack_resources.png[scaledwidth=100%]


[[_sec.deploy.preparation.openstack.cli]]
=== Using The {ostack} CLI

[NOTE]
====
You need to have access to the {ostack}
command-line tools.
You can either access those via `ssh` on your {soc}
 admin server or https://docs.openstack.org/newton/user-guide/common/cli-install-openstack-command-line-clients.html[
     install a local openstack client]. 

To use the local client, you need to access menu:Project → Compute → Access & Security[]
 in the Horizon Dashboard and click on the menu:Download OpenStack RC File v3[]
. 

The downloaded file is a script that you then need to load using the `source` command.
The script will ask you for your {soc}
 password. 

----
{prompt.user}``source container-openrc.sh`` 
----
====


. Upload the container image to {ostack} Glance (Image service). This example uses the name `CaaSP-3` as the name of the image that is created in {soc} . 
+

----
{prompt.user}``openstack image create --public --disk-format qcow2 \
--container-format bare \
--file SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2 \
CaaSP-3`` 
----
. {empty}
+
.Replace The Default `root_password`
WARNING: Do not use the [path]``caasp-environment.yaml``
 directly from the GitHub repository. 

You must make sure to replace the value for `root_password` with a secure password.
This will become the password for the {rootuser}
 account on all nodes in the stack. 
+


+
Download the [path]``caasp-stack.yaml``
and [path]``caasp-environment.yaml``
Heat templates to your workstation and then run the `openstack stack create` command. 
+

----
{prompt.user}``openstack stack create \
-t caasp-stack.yaml \
-e caasp-environment.yaml \
--parameter image=CaaSP-3 caasp3-stack`` 
----
. Find out which (external) IP address was assigned to the admin node of your {productname} cluster (here: ``192.168.1.51``). 
+

----
{prompt.user}``openstack server list --name "admin" | awk 'FNR > 3 {print $4 $5 $9}'`` caasp3-stack-admin|192.168.1.51
----
. Visit the external IP address in your browser. You should see the {dashboard} login page and can continue with <<_sec.deploy.nodes.admin_configuration>>. 


[[_sec.deploy.preparation.public_cloud]]
== Installing In Public Cloud

[[_sec.deploy.preparation.public_cloud.overview]]
=== Overview


The {productname}
images published by {suse}
in selected Public Cloud environments are provided as `Bring Your Own Subscription
    (BYOS)` images. {productname}
 instances need to be registered with the {scc}
 in order to receive bugfix and security updates.
Images labeled with the `cluster` designation in the name are not intended to be started directly; they are deployed by the Administrative node.
Administrative node images contain the `admin` designation in the image name. 

The following procedure outlines the deployment process: 


. Read the special system requirements for public cloud installations in <<_sec.deploy.requirements.public_cloud>>. 
. Provision the cluster nodes. For details, see <<_sec.deploy.preparation.public_cloud.provisioning>>. 
. Deploy the admin node with `caasp-admin-setup`. For details, see <<_sec.deploy.nodes.admin_install_cli>>. 
. Finish bootstrapping your cluster. The provisioned worker nodes are ready to be consumed into the cluster. For details, see <<_sec.deploy.install.bootstrap>>. 


[[_sec.deploy.preparation.public_cloud.provisioning]]
=== Provisioning Cluster Nodes

[[_sec.deploy.preparation.public_cloud.provisioning.aws]]
==== Amazon Web Services EC2


You may select from one of the predefined instance types, hand selected for general container workloads, or choose menu:Other types...[]
 and enter any menu:instance type[]
, as defined at https://aws.amazon.com/ec2/instance-types/[
     https://aws.amazon.com/ec2/instance-types/]

Two configuration options are required in EC2: 

Subnet ID::
The `subnet` within which cluster nodes will be attached to the network, in the form ``subnet-xxxxxxxx``. 

Security Group ID::
The `security group` defining network access rules for the cluster nodes, in the form ``sg-xxxxxxxx``. 


The defaults used for those two options are preset to the subnet ID of the administration host and the security group ID that was automatically created by `caasp-admin-setup`.
You may choose to place the cluster nodes in a different subnet and you can also use a custom security group, but please bear in mind that traffic must be allowed between the individual cluster nodes and also between the admininstration node and the cluster nodes. 

See the https://aws.amazon.com/documentation/vpc/[
     Amazon Virtual Private Cloud Documentation]for more information. 

[[_sec.deploy.preparation.public_cloud.provisioning.azure]]
==== Microsoft Azure


You need to configure credentials for access to the Azure framework so instances can be created, as well as parameters for the cluster node instances themselves.
The credentials refer to authentication via a service principal.
See https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal[
     https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal] for more information on how you can create a service principal. 

Subscription ID::
The subscription ID of your Azure account. 

Tenant ID::
The tenant ID of your service principal, also known as the Active Directory ID. 

Application ID::
The application ID of your service principal. 

Client Secret::
The key value or password of your service principal. 


Below the menu:Service Principal Authentication[]
 box you will find the menu:Instance Type[]
 configuration.
You may select from one of the predefined instance types, hand selected for general container workloads, or choose menu:Other types...[]
 and enter any ``size``, as defined at https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes/[
     https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes/]Set the menu:Cluster size[]
 using the slider. 

The parameters in menu:Resource Scopes[]
 define attributes of the cluster instances, as required for Azure Resource Manager: 

Resource Group::
The Resource Group in which all cluster nodes will be created. 

Storage Account::
The Storage Account that will be used for storing the cluster node OS disks.
See https://docs.microsoft.com/en-us/azure/storage/common/storage-create-storage-account[
https://docs.microsoft.com/en-us/azure/storage/common/storage-create-storage-account]for more information about Azure Storage Accounts. 

Network::
The virtual network the cluster nodes will be connected to. 

Subnet::
A subnet in the previously defined virtual network.
See https://docs.microsoft.com/en-us/azure/virtual-network/[
https://docs.microsoft.com/en-us/azure/virtual-network/] for more information about Azure Virtual Networks. 


[[_sec.deploy.preparation.public_cloud.provisioning.google]]
==== Google Compute Engine


You may select from one of the predefined instance types, hand selected for general container workloads, or choose menu:Other types...[]
 and enter any ``machine type``, as defined at https://cloud.google.com/compute/docs/machine-types[
     https://cloud.google.com/compute/docs/machine-types]

Two configuration options are required in GCE: 

Network::
The name of the virtual network the cluster nodes will run within. 

Subnet::
If you created a custom network, you must specify the name of the subnet within which the cluster nodes will run. 


See the https://cloud.google.com/vpc/docs/vpc[ GCE
     Network Documentation] for more information. 